#!/usr/bin/env python3
"""
Duplicate Detection System Fix Script
AynÄ± haberlerin birden Ã§ok tweet olarak paylaÅŸÄ±lmasÄ± sorununu Ã§Ã¶zer
"""

import json
import hashlib
from datetime import datetime, timedelta
import os

def load_json(filename, default=None):
    """JSON dosyasÄ±nÄ± yÃ¼kle"""
    try:
        if os.path.exists(filename):
            with open(filename, 'r', encoding='utf-8') as f:
                return json.load(f)
    except Exception as e:
        print(f"âŒ {filename} yÃ¼kleme hatasÄ±: {e}")
    return default if default is not None else []

def save_json(filename, data):
    """JSON dosyasÄ±nÄ± kaydet"""
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        return True
    except Exception as e:
        print(f"âŒ {filename} kaydetme hatasÄ±: {e}")
        return False

def generate_hash(title):
    """BaÅŸlÄ±ktan hash oluÅŸtur"""
    if not title or not title.strip():
        return ""
    return hashlib.md5(title.strip().encode('utf-8')).hexdigest()

def create_backup(filename):
    """Dosya yedeÄŸi oluÅŸtur"""
    if os.path.exists(filename):
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"{filename}.backup_{timestamp}"
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                data = f.read()
            with open(backup_name, 'w', encoding='utf-8') as f:
                f.write(data)
            print(f"âœ… Yedek oluÅŸturuldu: {backup_name}")
            return True
        except Exception as e:
            print(f"âŒ Yedek oluÅŸturma hatasÄ±: {e}")
    return False

def fix_posted_articles():
    """Posted articles dosyasÄ±ndaki duplicate sorunlarÄ±nÄ± Ã§Ã¶z"""
    print("\nğŸ” Posted Articles dÃ¼zeltiliyor...")
    
    # Yedek oluÅŸtur
    create_backup("posted_articles.json")
    
    articles = load_json("posted_articles.json", [])
    if not articles:
        print("âš ï¸ Posted articles dosyasÄ± boÅŸ")
        return
    
    print(f"ğŸ“Š {len(articles)} makale bulundu")
    
    # Hash'leri dÃ¼zelt ve duplicate'larÄ± temizle
    seen_hashes = set()
    seen_urls = set()
    fixed_articles = []
    hash_fixed = 0
    duplicates_removed = 0
    
    for article in articles:
        title = article.get('title', '').strip()
        url = article.get('url', '').strip()
        hash_value = article.get('hash', '').strip()
        
        # BoÅŸ baÅŸlÄ±k kontrolÃ¼
        if not title:
            print(f"âš ï¸ BoÅŸ baÅŸlÄ±k atlandÄ±: {url[:50]}...")
            continue
        
        # Hash yoksa oluÅŸtur
        if not hash_value:
            hash_value = generate_hash(title)
            article['hash'] = hash_value
            hash_fixed += 1
            print(f"ğŸ”§ Hash oluÅŸturuldu: {title[:50]}...")
        
        # Duplicate kontrolÃ¼
        is_duplicate = False
        
        if hash_value in seen_hashes:
            print(f"ğŸ”„ Hash duplicate kaldÄ±rÄ±ldÄ±: {title[:50]}...")
            is_duplicate = True
            duplicates_removed += 1
        elif url in seen_urls:
            print(f"ğŸ”„ URL duplicate kaldÄ±rÄ±ldÄ±: {title[:50]}...")
            is_duplicate = True
            duplicates_removed += 1
        
        if not is_duplicate:
            fixed_articles.append(article)
            seen_hashes.add(hash_value)
            seen_urls.add(url)
    
    # DÃ¼zeltilmiÅŸ veriyi kaydet
    if save_json("posted_articles.json", fixed_articles):
        print(f"âœ… Posted articles dÃ¼zeltildi:")
        print(f"   - Hash dÃ¼zeltilen: {hash_fixed}")
        print(f"   - Duplicate kaldÄ±rÄ±lan: {duplicates_removed}")
        print(f"   - Kalan makale: {len(fixed_articles)}")
    else:
        print("âŒ Posted articles kaydetme hatasÄ±")

def fix_pending_tweets():
    """Pending tweets dosyasÄ±ndaki duplicate sorunlarÄ±nÄ± Ã§Ã¶z"""
    print("\nğŸ” Pending Tweets dÃ¼zeltiliyor...")
    
    # Yedek oluÅŸtur
    create_backup("pending_tweets.json")
    
    tweets = load_json("pending_tweets.json", [])
    if not tweets:
        print("âš ï¸ Pending tweets dosyasÄ± boÅŸ")
        return
    
    print(f"ğŸ“Š {len(tweets)} pending tweet bulundu")
    
    # Hash'leri dÃ¼zelt ve duplicate'larÄ± temizle
    seen_hashes = set()
    seen_urls = set()
    fixed_tweets = []
    hash_fixed = 0
    duplicates_removed = 0
    
    for tweet in tweets:
        article = tweet.get('article', {})
        title = article.get('title', '').strip()
        url = article.get('url', '').strip()
        hash_value = article.get('hash', '').strip()
        
        # BoÅŸ baÅŸlÄ±k kontrolÃ¼
        if not title:
            print(f"âš ï¸ BoÅŸ baÅŸlÄ±k atlandÄ±: {url[:50]}...")
            continue
        
        # Hash yoksa oluÅŸtur
        if not hash_value:
            hash_value = generate_hash(title)
            article['hash'] = hash_value
            tweet['article'] = article
            hash_fixed += 1
            print(f"ğŸ”§ Hash oluÅŸturuldu: {title[:50]}...")
        
        # Duplicate kontrolÃ¼
        is_duplicate = False
        
        if hash_value in seen_hashes:
            print(f"ğŸ”„ Hash duplicate kaldÄ±rÄ±ldÄ±: {title[:50]}...")
            is_duplicate = True
            duplicates_removed += 1
        elif url in seen_urls:
            print(f"ğŸ”„ URL duplicate kaldÄ±rÄ±ldÄ±: {title[:50]}...")
            is_duplicate = True
            duplicates_removed += 1
        
        if not is_duplicate:
            fixed_tweets.append(tweet)
            seen_hashes.add(hash_value)
            seen_urls.add(url)
    
    # DÃ¼zeltilmiÅŸ veriyi kaydet
    if save_json("pending_tweets.json", fixed_tweets):
        print(f"âœ… Pending tweets dÃ¼zeltildi:")
        print(f"   - Hash dÃ¼zeltilen: {hash_fixed}")
        print(f"   - Duplicate kaldÄ±rÄ±lan: {duplicates_removed}")
        print(f"   - Kalan tweet: {len(fixed_tweets)}")
    else:
        print("âŒ Pending tweets kaydetme hatasÄ±")

def remove_cross_duplicates():
    """Posted articles ve pending tweets arasÄ±ndaki cross-duplicate'larÄ± temizle"""
    print("\nğŸ” Cross-duplicate'lar temizleniyor...")
    
    posted_articles = load_json("posted_articles.json", [])
    pending_tweets = load_json("pending_tweets.json", [])
    
    if not posted_articles or not pending_tweets:
        print("âš ï¸ Cross-duplicate kontrolÃ¼ iÃ§in yeterli veri yok")
        return
    
    # Posted articles'dan hash ve URL'leri topla
    posted_hashes = set()
    posted_urls = set()
    
    for article in posted_articles:
        hash_value = article.get('hash', '').strip()
        url = article.get('url', '').strip()
        if hash_value:
            posted_hashes.add(hash_value)
        if url:
            posted_urls.add(url)
    
    # Pending tweets'den cross-duplicate'larÄ± kaldÄ±r
    fixed_pending = []
    cross_duplicates_removed = 0
    
    for tweet in pending_tweets:
        article = tweet.get('article', {})
        hash_value = article.get('hash', '').strip()
        url = article.get('url', '').strip()
        title = article.get('title', '').strip()
        
        is_cross_duplicate = False
        
        if hash_value in posted_hashes:
            print(f"ğŸ”„ Cross-duplicate (hash) kaldÄ±rÄ±ldÄ±: {title[:50]}...")
            is_cross_duplicate = True
            cross_duplicates_removed += 1
        elif url in posted_urls:
            print(f"ğŸ”„ Cross-duplicate (URL) kaldÄ±rÄ±ldÄ±: {title[:50]}...")
            is_cross_duplicate = True
            cross_duplicates_removed += 1
        
        if not is_cross_duplicate:
            fixed_pending.append(tweet)
    
    # DÃ¼zeltilmiÅŸ pending tweets'i kaydet
    if save_json("pending_tweets.json", fixed_pending):
        print(f"âœ… Cross-duplicate'lar temizlendi: {cross_duplicates_removed} kaldÄ±rÄ±ldÄ±")
    else:
        print("âŒ Cross-duplicate temizleme hatasÄ±")

def enhance_duplicate_detection():
    """Duplicate detection sistemini geliÅŸtir"""
    print("\nğŸ”§ Duplicate detection sistemi geliÅŸtiriliyor...")
    
    # Automation settings'i gÃ¼ncelle
    settings = load_json("automation_settings.json", {})
    
    # Duplicate detection ayarlarÄ±nÄ± optimize et
    settings.update({
        "enable_duplicate_detection": True,
        "title_similarity_threshold": 0.85,
        "content_similarity_threshold": 0.75,
        "hash_based_detection": True,
        "url_based_detection": True,
        "title_based_detection": True,
        "cross_check_enabled": True
    })
    
    if save_json("automation_settings.json", settings):
        print("âœ… Automation settings gÃ¼ncellendi")
    else:
        print("âŒ Automation settings gÃ¼ncelleme hatasÄ±")

def test_duplicate_detection():
    """Duplicate detection sistemini test et"""
    print("\nğŸ§ª Duplicate detection sistemi test ediliyor...")
    
    posted_articles = load_json("posted_articles.json", [])
    pending_tweets = load_json("pending_tweets.json", [])
    
    # Hash kontrolÃ¼
    posted_hashes = [article.get('hash', '') for article in posted_articles]
    pending_hashes = [tweet.get('article', {}).get('hash', '') for tweet in pending_tweets]
    
    empty_hashes_posted = sum(1 for h in posted_hashes if not h)
    empty_hashes_pending = sum(1 for h in pending_hashes if not h)
    
    print(f"ğŸ“Š Hash durumu:")
    print(f"   - Posted articles: {len(posted_articles)} makale, {empty_hashes_posted} boÅŸ hash")
    print(f"   - Pending tweets: {len(pending_tweets)} tweet, {empty_hashes_pending} boÅŸ hash")
    
    # Duplicate kontrolÃ¼
    posted_urls = [article.get('url', '') for article in posted_articles]
    pending_urls = [tweet.get('article', {}).get('url', '') for tweet in pending_tweets]
    
    posted_duplicates = len(posted_urls) - len(set(posted_urls))
    pending_duplicates = len(pending_urls) - len(set(pending_urls))
    
    print(f"ğŸ“Š Duplicate durumu:")
    print(f"   - Posted articles: {posted_duplicates} duplicate")
    print(f"   - Pending tweets: {pending_duplicates} duplicate")
    
    # Cross-duplicate kontrolÃ¼
    posted_url_set = set(posted_urls)
    pending_url_set = set(pending_urls)
    cross_duplicates = len(posted_url_set.intersection(pending_url_set))
    
    print(f"ğŸ“Š Cross-duplicate durumu:")
    print(f"   - Cross-duplicate: {cross_duplicates} adet")
    
    # Ã–neriler
    print(f"\nğŸ’¡ Ã–neriler:")
    if empty_hashes_posted > 0 or empty_hashes_pending > 0:
        print(f"   - BoÅŸ hash'ler var, hash oluÅŸturma gerekli")
    if posted_duplicates > 0 or pending_duplicates > 0:
        print(f"   - Duplicate'lar var, temizlik gerekli")
    if cross_duplicates > 0:
        print(f"   - Cross-duplicate'lar var, temizlik gerekli")
    if empty_hashes_posted == 0 and empty_hashes_pending == 0 and posted_duplicates == 0 and pending_duplicates == 0 and cross_duplicates == 0:
        print(f"   - âœ… Sistem saÄŸlÄ±klÄ± gÃ¶rÃ¼nÃ¼yor!")

def main():
    """Ana fonksiyon"""
    print("ğŸš€ Duplicate Detection System Fix")
    print("=" * 50)
    
    # 1. Posted articles dÃ¼zelt
    fix_posted_articles()
    
    # 2. Pending tweets dÃ¼zelt
    fix_pending_tweets()
    
    # 3. Cross-duplicate'larÄ± temizle
    remove_cross_duplicates()
    
    # 4. Duplicate detection sistemini geliÅŸtir
    enhance_duplicate_detection()
    
    # 5. Test et
    test_duplicate_detection()
    
    print("\nâœ… Duplicate detection sistemi dÃ¼zeltildi!")
    print("\nğŸ“‹ YapÄ±lan iÅŸlemler:")
    print("   1. BoÅŸ hash'ler oluÅŸturuldu")
    print("   2. Duplicate makaleler temizlendi")
    print("   3. Cross-duplicate'lar kaldÄ±rÄ±ldÄ±")
    print("   4. Duplicate detection ayarlarÄ± optimize edildi")
    print("   5. Sistem test edildi")
    
    print("\nğŸ”§ Ã–nerilen sonraki adÄ±mlar:")
    print("   - Sistemi yeniden baÅŸlatÄ±n")
    print("   - Yeni makalelerin duplicate kontrolÃ¼nÃ¼ izleyin")
    print("   - HaftalÄ±k olarak test_duplicate_detection.py Ã§alÄ±ÅŸtÄ±rÄ±n")

if __name__ == "__main__":
    main()
