import os
import json
import requests
from bs4 import BeautifulSoup
from fpdf import FPDF
import tweepy
from datetime import datetime, timedelta
import hashlib
from dotenv import load_dotenv
from PIL import Image
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import time
import re
from difflib import SequenceMatcher

# .env dosyasÄ±nÄ± yÃ¼kle
load_dotenv()

# Web scraping iÃ§in alternatif kÃ¼tÃ¼phaneler (opsiyonel)
try:
    import requests_html  # type: ignore
    REQUESTS_HTML_AVAILABLE = True
except ImportError:
    REQUESTS_HTML_AVAILABLE = False
    requests_html = None  # type: ignore

try:
    from selenium import webdriver  # type: ignore
    from selenium.webdriver.chrome.options import Options  # type: ignore
    from selenium.webdriver.common.by import By  # type: ignore
    from selenium.webdriver.support.ui import WebDriverWait  # type: ignore
    from selenium.webdriver.support import expected_conditions as EC  # type: ignore
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False
    webdriver = None  # type: ignore
    Options = None  # type: ignore
    By = None  # type: ignore
    WebDriverWait = None  # type: ignore
    EC = None  # type: ignore

# .env dosyasÄ±nÄ± yÃ¼kle
load_dotenv()

# Firecrawl MCP fonksiyonlarÄ± iÃ§in placeholder
def mcp_firecrawl_scrape(params):
    """Firecrawl MCP scrape fonksiyonu - GeliÅŸmiÅŸ alternatif sistemli"""
    try:
        print(f"[MCP] Firecrawl scrape Ã§aÄŸrÄ±sÄ±: {params.get('url', 'unknown')}")
        
        url = params.get('url', '')
        if not url:
            return {"success": False, "error": "URL gerekli"}
        
        # JavaScript gerekli mi kontrol et
        use_js = any(domain in url.lower() for domain in [
            'techcrunch.com', 'theverge.com', 'wired.com', 
            'arstechnica.com', 'venturebeat.com'
        ])
        
        # Ã–nce geliÅŸmiÅŸ scraper dene
        try:
            print(f"[MCP] GeliÅŸmiÅŸ scraper deneniyor (JS: {use_js})...")
            result = advanced_web_scraper(url, wait_time=3, use_js=use_js)
            
            if result.get("success") and result.get("content"):
                content = result.get("content", "")
                
                print(f"[MCP] GeliÅŸmiÅŸ scraper baÅŸarÄ±lÄ±: {len(content)} karakter ({result.get('method', 'unknown')})")
                
                return {
                    "success": True,
                    "content": content,
                    "markdown": content,
                    "source": f"advanced_scraper_{result.get('method', 'unknown')}",
                    "method": result.get('method', 'unknown')
                }
            else:
                print(f"[MCP] GeliÅŸmiÅŸ scraper baÅŸarÄ±sÄ±z: {result.get('error', 'Bilinmeyen hata')}")
                
        except Exception as advanced_error:
            print(f"[MCP] GeliÅŸmiÅŸ scraper hatasÄ±: {advanced_error}")
        
        # Fallback: Basit HTTP request
        try:
            print(f"[MCP] Basit fallback deneniyor...")
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive'
            }
            
            session = requests.Session()
            session.headers.update(headers)
            
            response = session.get(url, timeout=30, allow_redirects=True)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            content = extract_main_content(soup)
            
            if content and len(content) > 100:
                print(f"[MCP] Basit fallback baÅŸarÄ±lÄ±: {len(content)} karakter")
                
                return {
                    "success": True,
                    "content": content,
                    "markdown": content,
                    "source": "simple_fallback"
                }
            else:
                print(f"[MCP] Basit fallback yetersiz iÃ§erik: {len(content) if content else 0} karakter")
                
        except Exception as fallback_error:
            print(f"[MCP] Basit fallback hatasÄ±: {fallback_error}")
        
        # Son Ã§are: Sadece baÅŸlÄ±k Ã§ek
        try:
            print(f"[MCP] Son Ã§are: Sadece baÅŸlÄ±k Ã§ekiliyor...")
            
            response = requests.get(url, timeout=15)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            title = ""
            title_selectors = ['title', 'h1', 'h2', '.title', '.headline']
            
            for selector in title_selectors:
                elem = soup.select_one(selector)
                if elem:
                    title = elem.get_text(strip=True)
                    if len(title) > 10:
                        break
            
            if title:
                print(f"[MCP] Son Ã§are baÅŸarÄ±lÄ±: BaÅŸlÄ±k Ã§ekildi")
                return {
                    "success": True,
                    "content": title,
                    "markdown": title,
                    "source": "title_only"
                }
                
        except Exception as title_error:
            print(f"[MCP] Son Ã§are hatasÄ±: {title_error}")
        
        return {"success": False, "error": "TÃ¼m yÃ¶ntemler baÅŸarÄ±sÄ±z"}
        
    except Exception as e:
        print(f"[MCP] Genel hata: {e}")
        return {"success": False, "error": str(e)}

HISTORY_FILE = "posted_articles.json"
HASHTAG_FILE = "hashtags.json"
ACCOUNT_FILE = "accounts.json"
SUMMARY_FILE = "summaries.json"
MCP_CONFIG_FILE = "mcp_config.json"

def fetch_latest_ai_articles_with_firecrawl():
    """Firecrawl MCP ile geliÅŸmiÅŸ haber Ã§ekme - Sadece son 4 makale"""
    try:
        # Ã–nce mevcut yayÄ±nlanan makaleleri yÃ¼kle
        posted_articles = load_json(HISTORY_FILE)
        posted_urls = [article.get('url', '') for article in posted_articles]
        posted_hashes = [article.get('hash', '') for article in posted_articles]
        
        print("ğŸ” TechCrunch AI kategorisinden Firecrawl MCP ile makale Ã§ekiliyor...")
        
        # Firecrawl MCP ile ana sayfa Ã§ek
        try:
            # Firecrawl MCP scrape fonksiyonunu kullan
            scrape_result = mcp_firecrawl_scrape({
                "url": "https://techcrunch.com/category/artificial-intelligence/",
                "formats": ["markdown", "links"],
                "onlyMainContent": True,
                "waitFor": 2000
            })
            
            if not scrape_result.get("success", False):
                print(f"âš ï¸ Firecrawl MCP hatasÄ±, fallback yÃ¶nteme geÃ§iliyor...")
                return fetch_latest_ai_articles_fallback()
            
            # Markdown iÃ§eriÄŸinden makale linklerini Ã§Ä±kar
            markdown_content = scrape_result.get("markdown", "")
            
            # Basit regex ile TechCrunch makale URL'lerini bul
            import re
            current_year = datetime.now().year
            
            # TÃ¼m TechCrunch URL'lerini bul
            all_urls = re.findall(r'https://techcrunch\.com/[^\s\)\]]+', markdown_content)
            
            # GÃ¼ncel yÄ±l ve Ã¶nceki yÄ±lÄ±n makalelerini filtrele
            article_urls = []
            for url in all_urls:
                # URL'yi temizle
                url = url.rstrip(')')
                
                # YÄ±l kontrolÃ¼
                year_check = f'/{current_year}/' in url or f'/{current_year-1}/' in url
                
                # Makale URL'si kontrolÃ¼ (tarih formatÄ± iÃ§ermeli)
                date_pattern = r'/\d{4}/\d{2}/\d{2}/'
                is_article = re.search(date_pattern, url)
                
                if (year_check and is_article and 
                    url not in posted_urls and 
                    url not in article_urls and
                    len(article_urls) < 4):  # Sadece son 4 makale
                    article_urls.append(url)
            
            print(f"ğŸ”— {len(article_urls)} makale URL'si bulundu")
            
        except Exception as firecrawl_error:
            print(f"âš ï¸ Firecrawl MCP hatasÄ±: {firecrawl_error}")
            print("ğŸ”„ Fallback yÃ¶nteme geÃ§iliyor...")
            return fetch_latest_ai_articles_fallback()
        
        articles_data = []
        for url in article_urls:
            try:
                # Her makaleyi Firecrawl MCP ile Ã§ek
                article_content = fetch_article_content_with_firecrawl(url)
                
                if article_content and len(article_content.get("content", "")) > 100:
                    title = article_content.get("title", "")
                    content = article_content.get("content", "")
                    
                    # Makale hash'i oluÅŸtur
                    article_hash = hashlib.md5(title.encode()).hexdigest()
                    
                    # Tekrar kontrolÃ¼
                    if article_hash not in posted_hashes:
                        articles_data.append({
                            "title": title,
                            "url": url,
                            "content": content,
                            "hash": article_hash,
                            "fetch_date": datetime.now().isoformat(),
                            "is_new": True,
                            "already_posted": False,
                            "source": "firecrawl_mcp"
                        })
                        print(f"ğŸ†• Firecrawl ile yeni makale: {title[:50]}...")
                    else:
                        print(f"âœ… Makale zaten paylaÅŸÄ±lmÄ±ÅŸ: {title[:50]}...")
                else:
                    print(f"âš ï¸ Ä°Ã§erik yetersiz: {url}")
                    
            except Exception as article_error:
                print(f"âŒ Makale Ã§ekme hatasÄ± ({url}): {article_error}")
                continue
        
        print(f"ğŸ“Š Firecrawl MCP ile {len(articles_data)} yeni makale bulundu")
        
        # Duplikat filtreleme uygula
        if articles_data:
            articles_data = filter_duplicate_articles(articles_data)
        
        return articles_data
        
    except Exception as e:
        print(f"Firecrawl MCP haber Ã§ekme hatasÄ±: {e}")
        print("ğŸ”„ Fallback yÃ¶nteme geÃ§iliyor...")
        return fetch_latest_ai_articles_fallback()

def fetch_latest_ai_articles():
    """Ana haber Ã§ekme fonksiyonu - AkÄ±llÄ± sistem ile"""
    try:
        # Yeni akÄ±llÄ± haber Ã§ekme sistemini kullan
        return fetch_latest_ai_articles_smart()
        
    except Exception as e:
        print(f"âŒ Ana haber Ã§ekme hatasÄ±: {e}")
        print("ğŸ”„ Son Ã§are fallback deneniyor...")
        try:
            return fetch_latest_ai_articles_fallback()
        except Exception as fallback_error:
            print(f"âŒ Fallback da baÅŸarÄ±sÄ±z: {fallback_error}")
            return []

def fetch_latest_ai_articles_fallback():
    """Fallback haber Ã§ekme yÃ¶ntemi - BeautifulSoup ile"""
    try:
        # Ã–nce mevcut yayÄ±nlanan makaleleri yÃ¼kle
        posted_articles = load_json(HISTORY_FILE)
        posted_urls = [article.get('url', '') for article in posted_articles]
        posted_hashes = [article.get('hash', '') for article in posted_articles]
        
        headers = {'User-Agent': 'Mozilla/5.0'}
        html = requests.get("https://techcrunch.com/category/artificial-intelligence/", headers=headers).text
        soup = BeautifulSoup(html, "html.parser")
        article_links = soup.select("a.loop-card__title-link")[:4]  # Sadece son 4 makale
        
        print(f"ğŸ” Fallback: TechCrunch AI kategorisinden son {len(article_links)} makale kontrol ediliyor...")
        
        articles_data = []
        for link_tag in article_links:
            title = link_tag.text.strip()
            url = link_tag['href']
            
            # Makale hash'i oluÅŸtur (baÅŸlÄ±k bazlÄ±)
            article_hash = hashlib.md5(title.encode()).hexdigest()
            
            # Tekrar kontrolÃ¼ - URL ve hash bazlÄ±
            is_already_posted = url in posted_urls or article_hash in posted_hashes
            
            if is_already_posted:
                print(f"âœ… Makale zaten paylaÅŸÄ±lmÄ±ÅŸ, atlanÄ±yor: {title[:50]}...")
                continue
            
            # Makale iÃ§eriÄŸini geliÅŸmiÅŸ ÅŸekilde Ã§ek
            content = fetch_article_content_advanced(url, headers)
            
            if content and len(content) > 100:  # Minimum iÃ§erik kontrolÃ¼
                articles_data.append({
                    "title": title, 
                    "url": url, 
                    "content": content,
                    "hash": article_hash,
                    "fetch_date": datetime.now().isoformat(),
                    "is_new": True,  # Yeni makale iÅŸareti
                    "already_posted": False,
                    "source": "fallback"
                })
                print(f"ğŸ†• Fallback ile yeni makale bulundu: {title[:50]}...")
            else:
                print(f"âš ï¸ Ä°Ã§erik yetersiz, atlanÄ±yor: {title[:50]}...")
        
        print(f"ğŸ“Š Fallback ile toplam {len(articles_data)} yeni makale bulundu")
        
        # Duplikat filtreleme uygula
        if articles_data:
            articles_data = filter_duplicate_articles(articles_data)
        
        return articles_data
        
    except Exception as e:
        print(f"Fallback haber Ã§ekme hatasÄ±: {e}")
        return []

def fetch_article_content_with_firecrawl(url):
    """Firecrawl MCP ile makale iÃ§eriÄŸi Ã§ekme"""
    try:
        print(f"ğŸ” Firecrawl MCP ile makale Ã§ekiliyor: {url[:50]}...")
        
        # Firecrawl MCP scrape fonksiyonunu kullan
        scrape_result = mcp_firecrawl_scrape({
            "url": url,
            "formats": ["markdown"],
            "onlyMainContent": True,
            "waitFor": 3000,
            "removeBase64Images": True
        })
        
        if not scrape_result.get("success", False):
            print(f"âš ï¸ Firecrawl MCP baÅŸarÄ±sÄ±z, fallback deneniyor...")
            return fetch_article_content_advanced_fallback(url)
        
        # Markdown iÃ§eriÄŸini al
        markdown_content = scrape_result.get("markdown", "")
        
        if not markdown_content or len(markdown_content) < 100:
            print(f"âš ï¸ Firecrawl'dan yetersiz iÃ§erik, fallback deneniyor...")
            return fetch_article_content_advanced_fallback(url)
        
        # BaÅŸlÄ±ÄŸÄ± Ã§Ä±kar (genellikle ilk # ile baÅŸlar)
        lines = markdown_content.split('\n')
        title = ""
        content_lines = []
        
        for line in lines:
            line = line.strip()
            if line.startswith('# ') and not title:
                title = line[2:].strip()
            elif line and not line.startswith('#') and len(line) > 20:
                content_lines.append(line)
        
        # Ä°Ã§eriÄŸi birleÅŸtir ve temizle
        content = '\n'.join(content_lines)
        
        # Gereksiz karakterleri temizle
        content = content.replace('*', '').replace('**', '').replace('_', '')
        content = ' '.join(content.split())  # Ã‡oklu boÅŸluklarÄ± tek boÅŸluÄŸa Ã§evir
        
        # Ä°Ã§eriÄŸi sÄ±nÄ±rla
        content = content[:2500]
        
        print(f"âœ… Firecrawl ile iÃ§erik Ã§ekildi: {len(content)} karakter")
        
        return {
            "title": title or "BaÅŸlÄ±k bulunamadÄ±",
            "content": content,
            "source": "firecrawl_mcp"
        }
        
    except Exception as e:
        print(f"âŒ Firecrawl MCP hatasÄ± ({url}): {e}")
        print("ğŸ”„ Fallback yÃ¶nteme geÃ§iliyor...")
        return fetch_article_content_advanced_fallback(url)

def fetch_article_content_advanced_fallback(url):
    """Fallback makale iÃ§eriÄŸi Ã§ekme - BeautifulSoup ile"""
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        article_html = requests.get(url, headers=headers, timeout=10).text
        article_soup = BeautifulSoup(article_html, "html.parser")
        
        # BaÅŸlÄ±ÄŸÄ± bul
        title = ""
        title_selectors = ["h1", "h1.entry-title", "h1.post-title", ".article-title h1"]
        for selector in title_selectors:
            title_elem = article_soup.select_one(selector)
            if title_elem:
                title = title_elem.text.strip()
                break
        
        # Ã‡oklu selector deneme - daha kapsamlÄ± iÃ§erik Ã§ekme
        content_selectors = [
            "div.article-content p",
            "div.entry-content p", 
            "div.post-content p",
            "article p",
            "div.content p",
            ".article-body p"
        ]
        
        content = ""
        for selector in content_selectors:
            paragraphs = article_soup.select(selector)
            if paragraphs:
                content = "\n".join([p.text.strip() for p in paragraphs if p.text.strip()])
                if len(content) > 200:  # Yeterli iÃ§erik bulundu
                    break
        
        # EÄŸer hala iÃ§erik bulunamadÄ±ysa, tÃ¼m p etiketlerini dene
        if not content:
            all_paragraphs = article_soup.find_all('p')
            content = "\n".join([p.text.strip() for p in all_paragraphs if len(p.text.strip()) > 50])
        
        content = content[:2000]  # Ä°Ã§eriÄŸi sÄ±nÄ±rla
        
        return {
            "title": title or "BaÅŸlÄ±k bulunamadÄ±",
            "content": content,
            "source": "fallback"
        }
        
    except Exception as e:
        print(f"Fallback makale iÃ§eriÄŸi Ã§ekme hatasÄ± ({url}): {e}")
        return None

def fetch_article_content_advanced(url, headers):
    """Geriye dÃ¶nÃ¼k uyumluluk iÃ§in eski fonksiyon"""
    result = fetch_article_content_advanced_fallback(url)
    return result.get("content", "") if result else ""

def load_json(path):
    return json.load(open(path, 'r', encoding='utf-8')) if os.path.exists(path) else []

def save_json(path, data):
    with open(path, "w", encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)

def summarize_article(article_content, api_key):
    """LLM ile geliÅŸmiÅŸ makale Ã¶zetleme"""
    prompt = f"""AÅŸaÄŸÄ±daki AI/teknoloji haberini TÃ¼rkÃ§e olarak Ã¶zetle. Ã–zet tweet formatÄ±nda, ilgi Ã§ekici ve bilgilendirici olsun:

Haber Ä°Ã§eriÄŸi:
{article_content[:1500]}

LÃ¼tfen:
- Maksimum 200 karakter
- Ana konuyu vurgula
- Teknik detaylarÄ± basitleÅŸtir
- Ä°lgi Ã§ekici bir dil kullan

Ã–zet:"""
    return gemini_call(prompt, api_key, max_tokens=100)

def score_article(article_content, api_key):
    prompt = f"""Bu AI/teknoloji haberinin Ã¶nemini 1-10 arasÄ±nda deÄŸerlendir (sadece sayÄ±):

{article_content[:800]}

DeÄŸerlendirme kriterleri:
- Yenilik derecesi
- SektÃ¶rel etki
- GeliÅŸtiriciler iÃ§in Ã¶nem
- Genel ilgi

Puan:"""
    result = gemini_call(prompt, api_key, max_tokens=5)
    try:
        return int(result.strip().split()[0])
    except:
        return 5

def categorize_article(article_content, api_key):
    prompt = f"""Bu haberin hedef kitlesini belirle:

{article_content[:500]}

SeÃ§enekler: Developer, Investor, General
Cevap:"""
    return gemini_call(prompt, api_key, max_tokens=10).strip()

def gemini_call(prompt, api_key, max_tokens=100):
    """Google Gemini API Ã§aÄŸrÄ±sÄ±"""
    if not api_key:
        safe_log("Gemini API anahtarÄ± bulunamadÄ±", "WARNING")
        return "API anahtarÄ± eksik"
    
    try:
        import google.generativeai as genai
        
        # API anahtarÄ±nÄ± yapÄ±landÄ±r
        genai.configure(api_key=api_key)
        
        # Modeli oluÅŸtur
        model = genai.GenerativeModel('gemini-2.0-flash')
        
        safe_log("Gemini API Ã§aÄŸrÄ±sÄ± yapÄ±lÄ±yor... Model: gemini-2.0-flash", "DEBUG")
        
        # Generation config
        generation_config = genai.types.GenerationConfig(
            max_output_tokens=max_tokens,
            temperature=0.7,
        )
        
        # API Ã§aÄŸrÄ±sÄ±
        response = model.generate_content(
            prompt,
            generation_config=generation_config
        )
        
        safe_log("Gemini API YanÄ±tÄ± alÄ±ndÄ±", "DEBUG")
        
        if response.text:
            content = response.text.strip()
            safe_log(f"Ä°Ã§erik alÄ±ndÄ±: {len(content)} karakter", "DEBUG")
            return content
        else:
            safe_log("Gemini API yanÄ±tÄ±nda metin bulunamadÄ±", "DEBUG")
            return "API hatasÄ±"
            
    except Exception as e:
        safe_log(f"Gemini API Ã§aÄŸrÄ± hatasÄ±: {str(e)}", "ERROR")
        return "API hatasÄ±"

def generate_smart_hashtags(title, content):
    """Makale iÃ§eriÄŸine gÃ¶re akÄ±llÄ± hashtag oluÅŸturma - 5 popÃ¼ler hashtag"""
    combined_text = f"{title.lower()} {content.lower()}"
    hashtags = []
    
    # AI ve Machine Learning hashtag'leri
    if any(keyword in combined_text for keyword in ["artificial intelligence", "ai", "machine learning", "ml", "neural", "deep learning"]):
        hashtags.extend(["#ArtificialIntelligence", "#MachineLearning", "#DeepLearning", "#NeuralNetworks"])
    
    # Teknoloji ve yazÄ±lÄ±m hashtag'leri
    if any(keyword in combined_text for keyword in ["software", "programming", "code", "developer", "api"]):
        hashtags.extend(["#SoftwareDevelopment", "#Programming", "#Developer", "#API"])
    
    # Startup ve yatÄ±rÄ±m hashtag'leri
    if any(keyword in combined_text for keyword in ["startup", "funding", "investment", "venture", "billion", "million"]):
        hashtags.extend(["#Startup", "#Investment", "#VentureCapital", "#Funding", "#Business"])
    
    # Åirket Ã¶zel hashtag'leri
    if "openai" in combined_text:
        hashtags.extend(["#OpenAI", "#ChatGPT", "#GPT"])
    if "google" in combined_text:
        hashtags.extend(["#Google", "#Alphabet", "#GoogleAI"])
    if "microsoft" in combined_text:
        hashtags.extend(["#Microsoft", "#Azure", "#Copilot"])
    if "meta" in combined_text:
        hashtags.extend(["#Meta", "#Facebook", "#MetaAI"])
    if "apple" in combined_text:
        hashtags.extend(["#Apple", "#iOS", "#AppleAI"])
    if "tesla" in combined_text:
        hashtags.extend(["#Tesla", "#ElonMusk", "#Autopilot"])
    if "nvidia" in combined_text:
        hashtags.extend(["#NVIDIA", "#GPU", "#CUDA"])
    if "anthropic" in combined_text:
        hashtags.extend(["#Anthropic", "#Claude"])
    
    # Teknoloji alanlarÄ±
    if any(keyword in combined_text for keyword in ["blockchain", "crypto", "bitcoin", "ethereum"]):
        hashtags.extend(["#Blockchain", "#Cryptocurrency", "#Web3", "#DeFi"])
    if any(keyword in combined_text for keyword in ["cloud", "aws", "azure", "gcp"]):
        hashtags.extend(["#CloudComputing", "#AWS", "#Azure", "#CloudNative"])
    if any(keyword in combined_text for keyword in ["cybersecurity", "security", "privacy", "encryption"]):
        hashtags.extend(["#Cybersecurity", "#DataPrivacy", "#InfoSec"])
    if any(keyword in combined_text for keyword in ["quantum", "quantum computing"]):
        hashtags.extend(["#QuantumComputing", "#Quantum", "#QuantumTech"])
    if any(keyword in combined_text for keyword in ["robotics", "robot", "automation"]):
        hashtags.extend(["#Robotics", "#Automation", "#RoboticProcess"])
    if any(keyword in combined_text for keyword in ["iot", "internet of things", "smart home"]):
        hashtags.extend(["#IoT", "#SmartHome", "#ConnectedDevices"])
    if any(keyword in combined_text for keyword in ["5g", "6g", "network", "connectivity"]):
        hashtags.extend(["#5G", "#Connectivity", "#Telecommunications"])
    if any(keyword in combined_text for keyword in ["ar", "vr", "augmented reality", "virtual reality", "metaverse"]):
        hashtags.extend(["#AR", "#VR", "#Metaverse", "#XR"])
    
    # Genel teknoloji hashtag'leri
    general_hashtags = ["#Innovation", "#Technology", "#DigitalTransformation", "#FutureTech", "#TechNews"]
    hashtags.extend(general_hashtags)
    
    # TekrarlarÄ± kaldÄ±r ve 5 tane seÃ§
    unique_hashtags = list(dict.fromkeys(hashtags))  # SÄ±rayÄ± koruyarak tekrarlarÄ± kaldÄ±r
    
    # En alakalÄ± 5 hashtag seÃ§
    selected_hashtags = unique_hashtags[:5]
    
    # EÄŸer 5'ten az varsa, genel hashtag'lerle tamamla
    if len(selected_hashtags) < 5:
        remaining_general = [h for h in general_hashtags if h not in selected_hashtags]
        selected_hashtags.extend(remaining_general[:5-len(selected_hashtags)])
    
    return selected_hashtags[:5]

def generate_smart_emojis(title, content):
    """Makale iÃ§eriÄŸine gÃ¶re akÄ±llÄ± emoji seÃ§imi"""
    combined_text = f"{title.lower()} {content.lower()}"
    emojis = []
    
    # Konu bazlÄ± emojiler
    if any(keyword in combined_text for keyword in ["ai", "artificial intelligence", "robot", "machine learning"]):
        emojis.extend(["ğŸ¤–", "ğŸ§ ", "âš¡"])
    if any(keyword in combined_text for keyword in ["funding", "investment", "billion", "million", "money"]):
        emojis.extend(["ğŸ’°", "ğŸ’¸", "ğŸ“ˆ"])
    if any(keyword in combined_text for keyword in ["launch", "release", "unveil", "announce"]):
        emojis.extend(["ğŸš€", "ğŸ‰", "âœ¨"])
    if any(keyword in combined_text for keyword in ["research", "development", "breakthrough", "discovery"]):
        emojis.extend(["ğŸ”¬", "ğŸ’¡", "ğŸ§ª"])
    if any(keyword in combined_text for keyword in ["security", "privacy", "protection", "safe"]):
        emojis.extend(["ğŸ”’", "ğŸ›¡ï¸", "ğŸ”"])
    if any(keyword in combined_text for keyword in ["acquisition", "merger", "partnership"]):
        emojis.extend(["ğŸ¤", "ğŸ”—", "ğŸ’¼"])
    if any(keyword in combined_text for keyword in ["search", "query", "find", "discover"]):
        emojis.extend(["ğŸ”", "ğŸ”", "ğŸ“Š"])
    if any(keyword in combined_text for keyword in ["mobile", "phone", "app", "smartphone"]):
        emojis.extend(["ğŸ“±", "ğŸ“²", "ğŸ’»"])
    if any(keyword in combined_text for keyword in ["cloud", "server", "data", "storage"]):
        emojis.extend(["â˜ï¸", "ğŸ’¾", "ğŸ—„ï¸"])
    if any(keyword in combined_text for keyword in ["game", "gaming", "entertainment"]):
        emojis.extend(["ğŸ®", "ğŸ•¹ï¸", "ğŸ¯"])
    
    # EÄŸer emoji bulunamadÄ±ysa varsayÄ±lan emojiler
    if not emojis:
        emojis = ["ğŸš€", "ğŸ’»", "ğŸŒŸ", "âš¡", "ğŸ”¥"]
    
    # En fazla 3 emoji seÃ§
    return emojis[:3]

def generate_comprehensive_analysis(article_data, api_key):
    """Makale iÃ§in kapsamlÄ± AI analizi - AyrÄ± ayrÄ± Ã§aÄŸrÄ±lar ile gÃ¼venilir sonuÃ§ (Ä°ngilizce)"""
    title = article_data.get("title", "")
    content = article_data.get("content", "")
    
    print(f"ğŸ” KapsamlÄ± AI analizi baÅŸlatÄ±lÄ±yor...")
    
    # AI ile ilgili olmayan iÃ§erikleri kontrol et
    title_lower = title.lower()
    content_lower = content.lower()
    
    # AI anahtar kelimeleri
    ai_keywords = ['ai', 'artificial intelligence', 'machine learning', 'deep learning', 'neural', 'gpt', 'llm', 'openai', 'anthropic', 'claude', 'chatgpt', 'algorithm', 'automation', 'robot', 'tech', 'software', 'data', 'computer']
    
    # AI ile ilgili olmayan anahtar kelimeler
    non_ai_keywords = ['wood', 'dried', 'kiln', 'furniture', 'cooking', 'recipe', 'travel', 'music', 'art', 'painting', 'photography', 'sports', 'fashion', 'food', 'health', 'medicine', 'politics', 'economy', 'finance', 'real estate']
    
    # AI ile ilgili mi kontrol et
    has_ai_content = any(keyword in title_lower or keyword in content_lower for keyword in ai_keywords)
    has_non_ai_content = any(keyword in title_lower or keyword in content_lower for keyword in non_ai_keywords)
    
    # EÄŸer AI ile ilgili deÄŸilse veya AI olmayan iÃ§erik varsa uyarÄ± ver
    if not has_ai_content or has_non_ai_content:
        print(f"âš ï¸ Bu iÃ§erik AI/teknoloji ile ilgili gÃ¶rÃ¼nmÃ¼yor: {title[:50]}...")
        print(f"ğŸ” AI iÃ§erik: {has_ai_content}, AI olmayan iÃ§erik: {has_non_ai_content}")
    
    analysis_result = {
        "innovation": "",
        "companies": [],
        "impact_level": 5,
        "audience": "General",
        "hashtags": [],
        "emojis": [],
        "tweet_text": ""
    }
    
    try:
        # 1. Main innovation/insight analysis (ENGLISH)
        innovation_prompt = f"""Briefly explain the main innovation or breakthrough in this AI/tech news (max 50 words, in English):\n\nTitle: {title}\nContent: {content[:800]}\n\nMain innovation:"""
        innovation = gemini_call(innovation_prompt, api_key, max_tokens=80)
        analysis_result["innovation"] = innovation.strip() if innovation != "API hatasÄ±" else "Technology innovation"
        
        # 2. Company analysis (ENGLISH)
        company_prompt = f"""List the main companies mentioned in this news (max 3, comma separated, in English):\n\nTitle: {title}\nContent: {content[:600]}\n\nCompanies:"""
        companies_text = gemini_call(company_prompt, api_key, max_tokens=50)
        if companies_text != "API hatasÄ±":
            companies = [c.strip() for c in companies_text.split(",") if c.strip()]
            analysis_result["companies"] = companies[:3]
        
        # 3. Impact level analysis (ENGLISH)
        impact_prompt = f"""Rate the impact of this news on the tech sector from 1 to 10 (just a number):\n\nTitle: {title}\nContent: {content[:600]}\n\nImpact score (1-10):"""
        impact_text = gemini_call(impact_prompt, api_key, max_tokens=10)
        try:
            impact_level = int(impact_text.strip().split()[0])
            if 1 <= impact_level <= 10:
                analysis_result["impact_level"] = impact_level
        except:
            analysis_result["impact_level"] = 5
        
        # 4. Audience analysis (ENGLISH)
        audience_prompt = f"""Determine the target audience for this news (Developer/Investor/General):\n\nTitle: {title}\nContent: {content[:500]}\n\nAudience:"""
        audience = gemini_call(audience_prompt, api_key, max_tokens=15)
        if audience != "API hatasÄ±" and audience.strip() in ["Developer", "Investor", "General"]:
            analysis_result["audience"] = audience.strip()
        
        # 5. Hashtag analysis (ENGLISH)
        hashtag_prompt = f"""Suggest the 3 most relevant hashtags for this news (in English, only hashtags, comma separated):\n\nTitle: {title}\nContent: {content[:800]}\n\nExample: #AI, #Technology, #Innovation\n\nHashtags:"""
        ai_hashtags_text = gemini_call(hashtag_prompt, api_key, max_tokens=50)
        ai_hashtags = []
        if ai_hashtags_text != "API hatasÄ±":
            clean_text = ai_hashtags_text.replace("Hashtags:", "").replace("Hashtag'ler:", "").replace("Hashtag'ler", "").strip()
            import re
            hashtag_matches = re.findall(r'#\w+', clean_text)
            if not hashtag_matches:
                words = re.findall(r'\b[A-Za-z][A-Za-z0-9]*\b', clean_text)
                for word in words[:3]:
                    if len(word) > 2:
                        ai_hashtags.append(f"#{word}")
            else:
                ai_hashtags = hashtag_matches[:3]
        # Smart hashtag system (ENGLISH)
        smart_hashtags = generate_smart_hashtags(title, content)
        combined_hashtags = []
        for tag in ai_hashtags[:3]:
            if tag not in combined_hashtags:
                combined_hashtags.append(tag)
        for tag in smart_hashtags:
            if tag not in combined_hashtags and len(combined_hashtags) < 3:
                combined_hashtags.append(tag)
        analysis_result["hashtags"] = combined_hashtags[:3]
        # 6. Emoji analysis (unchanged, universal)
        emoji_prompt = f"""Suggest the 3 most suitable emojis for this news (just emojis, no spaces):\n\nTitle: {title}\nContent: {content[:500]}\n\nEmojis:"""
        ai_emojis_text = gemini_call(emoji_prompt, api_key, max_tokens=20)
        ai_emojis = []
        if ai_emojis_text != "API hatasÄ±":
            import re
            emoji_pattern = re.compile(r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF\U00002702-\U000027B0\U000024C2-\U0001F251]+')
            found_emojis = emoji_pattern.findall(ai_emojis_text)
            for emoji in found_emojis:
                for single_emoji in emoji:
                    if single_emoji not in ai_emojis and len(ai_emojis) < 3:
                        ai_emojis.append(single_emoji)
        smart_emojis = generate_smart_emojis(title, content)
        combined_emojis = ai_emojis[:2]
        for emoji in smart_emojis:
            if emoji not in combined_emojis and len(combined_emojis) < 3:
                combined_emojis.append(emoji)
        analysis_result["emojis"] = combined_emojis[:3]
        print(f"âœ… KapsamlÄ± analiz tamamlandÄ±:")
        print(f"ğŸ”¬ Innovation: {analysis_result['innovation'][:50]}...")
        print(f"ğŸ¢ Companies: {', '.join(analysis_result['companies'])}")
        print(f"ğŸ¯ Audience: {analysis_result['audience']}")
        print(f"ğŸ·ï¸ Hashtags: {' '.join(analysis_result['hashtags'])}")
        print(f"ğŸ˜Š Emojis: {''.join(analysis_result['emojis'])}")
        return analysis_result
    except Exception as e:
        print(f"âŒ KapsamlÄ± analiz hatasÄ±: {e}")
        return {
            "innovation": "AI/tech innovation",
            "companies": [],
            "impact_level": 5,
            "audience": "General",
            "hashtags": generate_smart_hashtags(title, content)[:3],
            "emojis": generate_smart_emojis(title, content)[:3],
            "tweet_text": ""
        }

def generate_ai_tweet_with_mcp_analysis(article_data, api_key):
    """MCP verisi ile geliÅŸmiÅŸ AI tweet oluÅŸturma - KapsamlÄ± analiz ile"""
    title = article_data.get("title", "")
    content = article_data.get("content", "")
    url = article_data.get("url", "")
    source = article_data.get("source", "unknown")
    
    # Twitter karakter limiti
    TWITTER_LIMIT = 280
    URL_LENGTH = 25  # "\n\nğŸ”— " + URL kÄ±saltmasÄ± iÃ§in
    
    print(f"ğŸ¤– AI ile tweet oluÅŸturuluyor (kaynak: {source})...")
    
    try:
        # KapsamlÄ± analiz yap
        analysis = generate_comprehensive_analysis(article_data, api_key)
        
        # Tweet metni oluÅŸtur
        companies_text = ', '.join(analysis['companies'][:2]) if analysis['companies'] else ""
        
        tweet_prompt = f"""Create a compelling English tweet about this AI/tech breakthrough:

Title: {title[:120]}
Key Innovation: {analysis['innovation'][:120]}
Companies: {companies_text}

Requirements:
- Write in perfect English only
- Maximum 200 characters
- Make it clear, engaging and newsworthy
- Focus on WHAT changed and WHY it matters
- Use active voice and strong action verbs
- Include specific details when possible (numbers, capabilities)
- Make it accessible to general audience
- Sound exciting but credible
- Do NOT include hashtags, emojis, URLs, or impact levels (added separately)
- Do NOT mention impact, effect level, or rating in the tweet

Examples of good style:
- "OpenAI's new model achieves 95% accuracy in medical diagnosis"
- "Tesla's robot now performs complex assembly tasks autonomously"
- "Google's AI reduces data center energy consumption by 40%"

Tweet text:"""
        
        tweet_text = gemini_call(tweet_prompt, api_key, max_tokens=80)
        
        if tweet_text == "API hatasÄ±" or not tweet_text.strip():
            # Fallback tweet metni - daha anlamlÄ±
            if analysis['companies'] and analysis['innovation']:
                company = analysis['companies'][0]
                innovation = analysis['innovation'][:100]
                # Daha anlamlÄ± fallback tweet oluÅŸtur
                if "launch" in innovation.lower():
                    tweet_text = f"{company} launches {innovation.lower().replace('launch', '').strip()}"
                elif "announce" in innovation.lower():
                    tweet_text = f"{company} announces {innovation.lower().replace('announce', '').strip()}"
                elif "develop" in innovation.lower():
                    tweet_text = f"{company} develops {innovation.lower().replace('develop', '').strip()}"
                else:
                    tweet_text = f"{company} unveils {innovation}"
            elif analysis['innovation']:
                tweet_text = f"Breaking: {analysis['innovation'][:150]}"
            else:
                tweet_text = f"AI breakthrough: {title[:120]}"
        
        # Tweet metnini temizle
        tweet_text = tweet_text.replace("Tweet:", "").replace("Tweet metni:", "").strip()
        
        # Gereksiz karakterleri temizle
        tweet_text = tweet_text.replace('"', '').replace("'", "'").strip()
        
        # Impact/etki bilgilerini temizle
        import re
        # "impact: orta", "etki: yÃ¼ksek", "effect: medium" gibi ifadeleri kaldÄ±r
        tweet_text = re.sub(r'\b(impact|etki|effect)\s*:\s*\w+\b', '', tweet_text, flags=re.IGNORECASE)
        # "(impact: medium)", "[etki: yÃ¼ksek]" gibi parantez iÃ§indeki ifadeleri kaldÄ±r
        tweet_text = re.sub(r'[\(\[\{]\s*(impact|etki|effect)\s*:\s*\w+\s*[\)\]\}]', '', tweet_text, flags=re.IGNORECASE)
        # Fazla boÅŸluklarÄ± temizle
        tweet_text = re.sub(r'\s+', ' ', tweet_text).strip()
        
        # Hashtag ve emoji metinlerini oluÅŸtur
        hashtag_text = " ".join(analysis['hashtags']).strip()
        emoji_text = "".join(analysis['emojis']).strip()
        url_part = f"\n\nğŸ”— {url}"
        
        # Sabit kÄ±sÄ±mlarÄ±n uzunluÄŸu
        fixed_parts_length = len(emoji_text) + len(hashtag_text) + len(url_part) + 2  # 2 boÅŸluk iÃ§in
        available_chars = TWITTER_LIMIT - fixed_parts_length
        
        # Tweet metnini temizle ve kÄ±salt
        tweet_text = tweet_text.strip()
        
        # EÄŸer tweet metni Ã§ok uzunsa kÄ±salt
        if len(tweet_text) > available_chars:
            # "..." iÃ§in 3 karakter ayÄ±r
            tweet_text = tweet_text[:available_chars-3] + "..."
        
        # Final tweet oluÅŸtur - boÅŸluklarÄ± optimize et
        if emoji_text and tweet_text:
            # Emoji varsa emoji ile tweet arasÄ±nda tek boÅŸluk
            main_content = f"{emoji_text} {tweet_text}"
        else:
            # Emoji yoksa direkt tweet
            main_content = tweet_text
        
        if hashtag_text:
            # Hashtag varsa tek boÅŸluk ile ekle
            final_tweet = f"{main_content} {hashtag_text}{url_part}"
        else:
            # Hashtag yoksa direkt URL ekle
            final_tweet = f"{main_content}{url_part}"
        
        # Son gÃ¼venlik kontrolÃ¼ - eÄŸer hala uzunsa daha agresif kÄ±salt
        if len(final_tweet) > TWITTER_LIMIT:
            excess = len(final_tweet) - TWITTER_LIMIT
            # Tweet metninden fazlalÄ±ÄŸÄ± Ã§Ä±kar
            new_tweet_length = len(tweet_text) - excess - 3  # 3 "..." iÃ§in
            if new_tweet_length > 10:  # Minimum 10 karakter bÄ±rak
                tweet_text = tweet_text[:new_tweet_length] + "..."
            else:
                # Ã‡ok kÄ±sa kalÄ±rsa hashtag'leri azalt
                hashtag_text = " ".join(analysis['hashtags'][:2])  # 2 hashtag
                fixed_parts_length = len(emoji_text) + len(hashtag_text) + len(url_part) + 1  # 1 boÅŸluk
                available_chars = TWITTER_LIMIT - fixed_parts_length
                tweet_text = tweet_text[:available_chars-3] + "..."
            
            # Yeniden oluÅŸtur
            if emoji_text and tweet_text:
                main_content = f"{emoji_text} {tweet_text}"
            else:
                main_content = tweet_text
            
            if hashtag_text:
                final_tweet = f"{main_content} {hashtag_text}{url_part}"
            else:
                final_tweet = f"{main_content}{url_part}"
        
        print(f"âœ… AI analizi ile tweet oluÅŸturuldu: {len(final_tweet)} karakter")
        print(f"ğŸ“ Tweet metni: {len(tweet_text)} karakter")
        print(f"ğŸ·ï¸ AI Hashtag'ler: {hashtag_text} ({len(hashtag_text)} karakter)")
        print(f"ğŸ˜Š AI Emojiler: {emoji_text} ({len(emoji_text)} karakter)")
        print(f"ğŸ”— URL kÄ±smÄ±: {len(url_part)} karakter")
        print(f"ğŸ¯ Hedef Kitle: {analysis['audience']}")
        print(f"ğŸ“Š Impact Score: 8 (varsayÄ±lan)")
        
        # Dictionary formatÄ±nda dÃ¶ndÃ¼r
        return {
            "tweet": final_tweet,
            "impact_score": 8,  # VarsayÄ±lan yÃ¼ksek skor
            "analysis": analysis,
            "source": "mcp_analysis"
        }
        
    except Exception as e:
        print(f"âŒ AI tweet oluÅŸturma hatasÄ±: {e}")
        print("ğŸ”„ Fallback yÃ¶nteme geÃ§iliyor...")
        fallback_tweet = generate_ai_tweet_with_content_fallback(article_data, api_key)
        return {
            "tweet": fallback_tweet,
            "impact_score": 6,  # Orta skor
            "analysis": {"audience": "General", "companies": [], "hashtags": [], "emojis": []},
            "source": "fallback"
        }

def generate_ai_tweet_with_content(article_data, api_key):
    """Ana tweet oluÅŸturma fonksiyonu - MCP analizi Ã¶ncelikli"""
    try:
        # Ã–nce MCP analizi ile dene
        tweet_data = generate_ai_tweet_with_mcp_analysis(article_data, api_key)
        
        # EÄŸer baÅŸarÄ±sÄ±zsa fallback kullan
        if not tweet_data or not tweet_data.get('tweet') or len(tweet_data.get('tweet', '')) < 50:
            print("ğŸ”„ MCP analizi yetersiz, fallback yÃ¶ntemi deneniyor...")
            fallback_tweet = generate_ai_tweet_with_content_fallback(article_data, api_key)
            return {
                "tweet": fallback_tweet,
                "impact_score": 6,
                "analysis": {"audience": "General", "companies": [], "hashtags": [], "emojis": []},
                "source": "fallback"
            }
        
        return tweet_data
        
    except Exception as e:
        print(f"Ana tweet oluÅŸturma hatasÄ±: {e}")
        fallback_tweet = generate_ai_tweet_with_content_fallback(article_data, api_key)
        return {
            "tweet": fallback_tweet,
            "impact_score": 6,
            "analysis": {"audience": "General", "companies": [], "hashtags": [], "emojis": []},
            "source": "fallback"
        }

def generate_ai_tweet_with_content_fallback(article_data, api_key):
    """Fallback tweet oluÅŸturma - Eski yÃ¶ntem"""
    title = article_data.get("title", "")
    content = article_data.get("content", "")
    url = article_data.get("url", "")
    
    # Twitter karakter limiti (URL iÃ§in 23 karakter ayrÄ±lÄ±r)
    TWITTER_LIMIT = 280
    URL_LENGTH = 25  # "\n\nğŸ”— " + URL kÄ±saltmasÄ± iÃ§in
    
    # AkÄ±llÄ± hashtag ve emoji oluÅŸtur
    smart_hashtags = generate_smart_hashtags(title, content)
    smart_emojis = generate_smart_emojis(title, content)
    
    hashtag_text = " ".join(smart_hashtags).strip()
    emoji_text = "".join(smart_emojis).strip()
    
    # Hashtag ve emoji iÃ§in yer ayÄ±r
    hashtag_emoji_length = len(hashtag_text) + len(emoji_text) + 2  # 2 boÅŸluk iÃ§in
    MAX_CONTENT_LENGTH = TWITTER_LIMIT - URL_LENGTH - hashtag_emoji_length
    
    # Ä°ngilizce tweet iÃ§in geliÅŸmiÅŸ prompt
    prompt = f"""Create a compelling English tweet about this AI/tech breakthrough:

Article Title: {title}
Article Content: {content[:1000]}

Requirements:
- Write in perfect English only
- Maximum {MAX_CONTENT_LENGTH} characters
- Make it clear, engaging and newsworthy
- Focus on WHAT changed and WHY it matters
- Use active voice and strong action verbs
- Include specific details when possible (numbers, capabilities, improvements)
- Make it accessible to general audience
- Sound exciting but credible
- Avoid jargon and technical terms
- Do NOT include hashtags, emojis, URLs, or impact levels (added separately)
- Do NOT mention impact, effect level, or rating in the tweet

Examples of good style:
- "OpenAI's new model achieves 95% accuracy in medical diagnosis"
- "Tesla's robot now performs complex assembly tasks autonomously"
- "Google's AI reduces data center energy consumption by 40%"
- "Meta's VR headset delivers 4K resolution at half the price"

Tweet text (max {MAX_CONTENT_LENGTH} chars):"""

    try:
        tweet_text = gemini_call(prompt, api_key, max_tokens=150)
        
        if tweet_text and len(tweet_text.strip()) > 10:
            # Tweet metnini temizle
            import re
            # Impact/etki bilgilerini temizle
            tweet_text = re.sub(r'\b(impact|etki|effect)\s*:\s*\w+\b', '', tweet_text, flags=re.IGNORECASE)
            tweet_text = re.sub(r'[\(\[\{]\s*(impact|etki|effect)\s*:\s*\w+\s*[\)\]\}]', '', tweet_text, flags=re.IGNORECASE)
            tweet_text = re.sub(r'\s+', ' ', tweet_text).strip()
            
            # Karakter limiti kontrolÃ¼
            if len(tweet_text.strip()) > MAX_CONTENT_LENGTH:
                tweet_text = tweet_text.strip()[:MAX_CONTENT_LENGTH-3] + "..."
            
            # Emoji, tweet metni, hashtag'ler ve URL'yi birleÅŸtir - boÅŸluklarÄ± optimize et
            parts = []
            if emoji_text:
                parts.append(emoji_text)
            if tweet_text.strip():
                parts.append(tweet_text.strip())
            if hashtag_text:
                parts.append(hashtag_text)
            
            main_content = " ".join(parts)
            final_tweet = f"{main_content}\n\nğŸ”— {url}"
            
            # Final karakter kontrolÃ¼
            if len(final_tweet) > TWITTER_LIMIT:
                # Tekrar kÄ±salt
                excess = len(final_tweet) - TWITTER_LIMIT
                tweet_text = tweet_text.strip()[:-(excess + 3)] + "..."
                
                # Yeniden birleÅŸtir - boÅŸluklarÄ± optimize et
                parts = []
                if emoji_text:
                    parts.append(emoji_text)
                if tweet_text:
                    parts.append(tweet_text)
                if hashtag_text:
                    parts.append(hashtag_text)
                
                main_content = " ".join(parts)
                final_tweet = f"{main_content}\n\nğŸ”— {url}"
            
            print(f"[FALLBACK] Tweet oluÅŸturuldu: {len(final_tweet)} karakter (limit: {TWITTER_LIMIT})")
            print(f"[FALLBACK] Hashtag'ler: {hashtag_text}")
            print(f"[FALLBACK] Emojiler: {emoji_text}")
            
            return final_tweet
        else:
            print("[FALLBACK] API yanÄ±tÄ± yetersiz, basit fallback tweet oluÅŸturuluyor...")
            return create_fallback_tweet(title, content, url)
            
    except Exception as e:
        print(f"Fallback tweet oluÅŸturma hatasÄ±: {e}")
        print("[FALLBACK] API hatasÄ±, basit fallback tweet oluÅŸturuluyor...")
        return create_fallback_tweet(title, content, url)

def create_fallback_tweet(title, content, url=""):
    """API hatasÄ± durumunda fallback tweet oluÅŸtur - AkÄ±llÄ± hashtag ve emoji ile"""
    try:
        # Twitter karakter limiti
        TWITTER_LIMIT = 280
        URL_LENGTH = 25  # "\n\nğŸ”— " + URL iÃ§in
        
        # AkÄ±llÄ± hashtag ve emoji oluÅŸtur
        smart_hashtags = generate_smart_hashtags(title, content)
        smart_emojis = generate_smart_emojis(title, content)
        
        hashtag_text = " ".join(smart_hashtags).strip()
        emoji_text = "".join(smart_emojis).strip()
        
        # Hashtag ve emoji iÃ§in yer ayÄ±r
        hashtag_emoji_length = len(hashtag_text) + len(emoji_text) + 2  # 2 boÅŸluk iÃ§in
        MAX_CONTENT_LENGTH = TWITTER_LIMIT - URL_LENGTH - hashtag_emoji_length
        
        # BaÅŸlÄ±ÄŸÄ± temizle
        clean_title = title.strip()
        
        # Ä°Ã§erikten anahtar kelimeler ve Ã¶nemli bilgiler Ã§Ä±kar
        content_lower = content.lower()
        title_lower = title.lower()
        combined_text = f"{title_lower} {content_lower}"
        
        # SayÄ±sal bilgileri Ã§Ä±kar
        import re
        numbers = re.findall(r'\$?(\d+(?:\.\d+)?)\s*(billion|million|%|percent)', combined_text, re.IGNORECASE)
        
        # Åirket isimlerini tespit et
        companies = []
        company_names = ["OpenAI", "Google", "Microsoft", "Meta", "Apple", "Amazon", "Tesla", "Nvidia", "Anthropic", "Perplexity", "Cursor", "DeviantArt", "AMD", "Intel"]
        for company in company_names:
            if company.lower() in combined_text:
                companies.append(company)
        
        # Ana tweet metni oluÅŸtur - daha anlamlÄ± Ä°ngilizce
        tweet_parts = []
        
        # Åirket ve eylem bazlÄ± tweet oluÅŸtur
        if companies:
            main_company = companies[0]
            
            # Eyleme gÃ¶re anlamlÄ± cÃ¼mle oluÅŸtur
            if "acquisition" in combined_text or "acquire" in combined_text:
                if "billion" in combined_text:
                    tweet_parts.append(f"{main_company} completes major acquisition")
                else:
                    tweet_parts.append(f"{main_company} acquires strategic company")
            elif "funding" in combined_text or "investment" in combined_text:
                if numbers:
                    largest_num = max(numbers, key=lambda x: float(x[0]))
                    if largest_num[1].lower() == 'billion':
                        tweet_parts.append(f"{main_company} raises ${largest_num[0]}B in funding")
                    elif largest_num[1].lower() == 'million':
                        tweet_parts.append(f"{main_company} secures ${largest_num[0]}M investment")
                    else:
                        tweet_parts.append(f"{main_company} secures major funding")
                else:
                    tweet_parts.append(f"{main_company} secures new funding round")
            elif "launch" in combined_text or "release" in combined_text:
                if "ai" in combined_text or "artificial intelligence" in combined_text:
                    tweet_parts.append(f"{main_company} launches new AI technology")
                elif "robot" in combined_text:
                    tweet_parts.append(f"{main_company} unveils advanced robotics")
                else:
                    tweet_parts.append(f"{main_company} releases breakthrough innovation")
            elif "partnership" in combined_text or "partner" in combined_text:
                tweet_parts.append(f"{main_company} forms strategic partnership")
            elif "breakthrough" in combined_text or "innovation" in combined_text:
                tweet_parts.append(f"{main_company} achieves major breakthrough")
            else:
                # BaÅŸlÄ±ÄŸÄ± kullan ama ÅŸirket adÄ±nÄ± Ã¶ne Ã§Ä±kar
                clean_title_short = clean_title.replace(main_company, "").strip()
                if clean_title_short:
                    tweet_parts.append(f"{main_company}: {clean_title_short[:80]}")
                else:
                    tweet_parts.append(f"{main_company} makes major announcement")
        else:
            # Åirket yoksa baÅŸlÄ±ÄŸÄ± kullan
            if "ai" in combined_text or "artificial intelligence" in combined_text:
                tweet_parts.append(f"AI breakthrough: {clean_title[:100]}")
            elif "robot" in combined_text:
                tweet_parts.append(f"Robotics advance: {clean_title[:100]}")
            else:
                tweet_parts.append(f"Tech news: {clean_title[:120]}")
        
        # SayÄ±sal bilgi ekle (eÄŸer henÃ¼z eklenmemiÅŸse)
        if numbers and not any("$" in part for part in tweet_parts):
            largest_num = max(numbers, key=lambda x: float(x[0]))
            if largest_num[1].lower() == 'billion':
                tweet_parts.append(f"(${largest_num[0]}B)")
            elif largest_num[1].lower() == 'million':
                tweet_parts.append(f"({largest_num[0]}M)")
            elif largest_num[1].lower() in ['%', 'percent']:
                tweet_parts.append(f"({largest_num[0]}% improvement)")
        
        # Tweet'i birleÅŸtir
        main_text = " ".join(tweet_parts)
        
        # Karakter limiti kontrolÃ¼
        if len(main_text) > MAX_CONTENT_LENGTH:
            # Ã‡ok uzunsa kÄ±salt
            main_text = main_text[:MAX_CONTENT_LENGTH-3] + "..."
        
        # Emoji, tweet metni, hashtag'ler ve URL'yi birleÅŸtir
        if url:
            fallback_tweet = f"{emoji_text} {main_text} {hashtag_text}\n\nğŸ”— {url}"
        else:
            fallback_tweet = f"{emoji_text} {main_text} {hashtag_text}"
        
        # Final karakter kontrolÃ¼
        if len(fallback_tweet) > TWITTER_LIMIT:
            # Tekrar kÄ±salt
            excess = len(fallback_tweet) - TWITTER_LIMIT
            main_text = main_text[:-(excess + 3)] + "..."
            if url:
                fallback_tweet = f"{emoji_text} {main_text} {hashtag_text}\n\nğŸ”— {url}"
            else:
                fallback_tweet = f"{emoji_text} {main_text} {hashtag_text}"
        
        print(f"[FALLBACK] Tweet oluÅŸturuldu: {len(fallback_tweet)} karakter (limit: {TWITTER_LIMIT})")
        print(f"[FALLBACK] Hashtag'ler: {hashtag_text}")
        print(f"[FALLBACK] Emojiler: {emoji_text}")
        
        return fallback_tweet
        
    except Exception as e:
        print(f"Fallback tweet oluÅŸturma hatasÄ±: {e}")
        # En basit fallback - akÄ±llÄ± hashtag ve emoji ile
        try:
            simple_hashtags = generate_smart_hashtags(title, "")[:3]  # 3 hashtag
            simple_emojis = generate_smart_emojis(title, "")[:2]  # 2 emoji
            
            hashtag_text = " ".join(simple_hashtags)
            emoji_text = "".join(simple_emojis)
            
            # Karakter hesaplama
            url_length = len(f"\n\nğŸ”— {url}") if url else 0
            available_chars = TWITTER_LIMIT - url_length - len(hashtag_text) - len(emoji_text) - 2
            
            # BaÅŸlÄ±ÄŸÄ± kÄ±salt
            if len(title) > available_chars:
                title_text = title[:available_chars-3] + "..."
            else:
                title_text = title
            
            simple_tweet = f"{emoji_text} {title_text} {hashtag_text}"
            if url:
                simple_tweet += f"\n\nğŸ”— {url}"
            
            return simple_tweet
            
        except:
            # En son Ã§are - basit tweet
            simple_text = f"ğŸ¤– {title[:200]}... #AI #Innovation #Technology"
            if url:
                simple_tweet = f"{simple_text}\n\nğŸ”— {url}"
            else:
                simple_tweet = simple_text
            
            # Karakter limiti kontrolÃ¼
            if len(simple_tweet) > TWITTER_LIMIT:
                available = TWITTER_LIMIT - len("\n\nğŸ”— ") - len(url) - len(" #AI #Innovation #Technology") - 3
                simple_text = f"ğŸ¤– {title[:available]}... #AI #Innovation #Technology"
                simple_tweet = f"{simple_text}\n\nğŸ”— {url}" if url else simple_text
            
            return simple_tweet

def setup_twitter_api():
    import tweepy
    import os
    # V1.1 API ile oturum aÃ§
    auth = tweepy.OAuth1UserHandler(
        os.environ['TWITTER_API_KEY'],
        os.environ['TWITTER_API_SECRET'],
        os.environ['TWITTER_ACCESS_TOKEN'],
        os.environ['TWITTER_ACCESS_TOKEN_SECRET']
    )
    api = tweepy.API(auth)
    return api

def post_tweet(tweet_text, article_title=""):
    """X platformunda tweet paylaÅŸma ve Gmail bildirimi - Twitter API v2 kullanarak"""
    try:
        # Twitter API v2 kullan
        tweet_result = post_text_tweet_v2(tweet_text)
        
        # BaÅŸarÄ±sÄ±z olursa hata dÃ¶ndÃ¼r
        if not tweet_result.get("success"):
            safe_log(f"Tweet paylaÅŸÄ±m hatasÄ±: {tweet_result.get('error', 'Bilinmeyen hata')}", "ERROR")
            return tweet_result
        
        tweet_id = tweet_result.get("tweet_id")
        tweet_url = tweet_result.get("url")
        
        # Gmail bildirimi gÃ¶nder (Telegram yerine)
        email_sent = False
        try:
            gmail_result = send_gmail_notification(
                message=tweet_text,
                tweet_url=tweet_url,
                article_title=article_title
            )
            if gmail_result.get("success"):
                safe_log(f"Gmail bildirimi gÃ¶nderildi: {gmail_result.get('email')}", "INFO")
                email_sent = True
            else:
                safe_log(f"Gmail bildirimi gÃ¶nderilemedi: {gmail_result.get('reason', 'unknown')}", "WARNING")
        except Exception as gmail_error:
            safe_log(f"Gmail bildirim hatasÄ±: {gmail_error}", "ERROR")
        
        # Fallback: Telegram bildirimi (eÄŸer Gmail baÅŸarÄ±sÄ±z olursa)
        telegram_sent = False
        if not email_sent:
            try:
                telegram_result = send_telegram_notification(
                    message=tweet_text,
                    tweet_url=tweet_url,
                    article_title=article_title
                )
                if telegram_result.get("success"):
                    safe_log("Fallback Telegram bildirimi gÃ¶nderildi", "INFO")
                    telegram_sent = True
                else:
                    safe_log(f"Fallback Telegram bildirimi de baÅŸarÄ±sÄ±z: {telegram_result.get('reason', 'unknown')}", "WARNING")
            except Exception as telegram_error:
                safe_log(f"Fallback Telegram bildirim hatasÄ±: {telegram_error}", "ERROR")
        
        return {
            "success": True,
            "tweet_id": tweet_id,
            "url": tweet_url,
            "email_sent": email_sent,
            "telegram_sent": telegram_sent
        }
        
    except Exception as e:
        safe_log(f"Tweet paylaÅŸÄ±m hatasÄ±: {str(e)}", "ERROR")
        return {"success": False, "error": f"Tweet paylaÅŸÄ±m hatasÄ±: {str(e)}"}

def mark_article_as_posted(article_data, tweet_result):
    """Makaleyi paylaÅŸÄ±ldÄ± olarak iÅŸaretle - API ve manuel paylaÅŸÄ±mlarÄ± destekler"""
    try:
        posted_articles = load_json(HISTORY_FILE)
        
        # Manuel paylaÅŸÄ±m kontrolÃ¼
        is_manual_post = tweet_result.get("manual_post", False)
        
        posted_article = {
            "title": article_data.get("title", ""),
            "url": article_data.get("url", ""),
            "hash": article_data.get("hash", ""),
            "posted_date": datetime.now().isoformat(),
            "tweet_id": tweet_result.get("tweet_id", ""),
            "tweet_url": tweet_result.get("url", ""),
            "manual_post": is_manual_post,
            "post_method": "manuel" if is_manual_post else "api"
        }
        
        # Manuel paylaÅŸÄ±m iÃ§in ek bilgiler
        if is_manual_post:
            posted_article["tweet_text"] = article_data.get("tweet_text", "")
            posted_article["manual_posted_at"] = tweet_result.get("posted_at", datetime.now().isoformat())
        
        posted_articles.append(posted_article)
        save_json(HISTORY_FILE, posted_articles)
        
        safe_log(f"Makale kaydedildi: {article_data.get('title', '')[:50]}... (YÃ¶ntem: {posted_article['post_method']})", "INFO")
        
        return True
    except Exception as e:
        safe_log(f"Makale kaydetme hatasÄ±: {e}", "ERROR")
        return False

def check_duplicate_articles():
    """Tekrarlanan makaleleri temizle"""
    try:
        posted_articles = load_json(HISTORY_FILE)
        
        # Son 30 gÃ¼nlÃ¼k makaleleri tut
        cutoff_date = datetime.now() - timedelta(days=30)
        
        filtered_articles = []
        seen_hashes = set()
        
        for article in posted_articles:
            try:
                posted_date = datetime.fromisoformat(article.get("posted_date", ""))
                article_hash = article.get("hash", "")
                
                if posted_date > cutoff_date and article_hash not in seen_hashes:
                    filtered_articles.append(article)
                    seen_hashes.add(article_hash)
            except:
                continue
        
        save_json(HISTORY_FILE, filtered_articles)
        return len(posted_articles) - len(filtered_articles)
        
    except Exception as e:
        print(f"Tekrar temizleme hatasÄ±: {e}")
        return 0

def generate_ai_digest(summaries_with_links, api_key):
    """Eski fonksiyon - geriye dÃ¶nÃ¼k uyumluluk iÃ§in"""
    if not summaries_with_links:
        return "Ã–zet bulunamadÄ±"
    
    # Ä°lk makaleyi kullanarak tweet oluÅŸtur
    first_summary = summaries_with_links[0]
    article_data = {
        "title": "AI Digest",
        "content": first_summary.get("summary", ""),
        "url": first_summary.get("url", "")
    }
    
    return generate_ai_tweet_with_content(article_data, api_key)

def create_pdf(summaries, filename="daily_digest.pdf"):
    pdf = FPDF()
    pdf.add_page()
    pdf.set_font("Arial", size=12)
    pdf.cell(200, 10, txt="AI Tweet Digest", ln=True, align='C')
    for s in summaries:
        pdf.multi_cell(0, 10, f"â€¢ {s}")
    pdf.output(filename)
    return filename

def get_posted_articles_summary():
    """PaylaÅŸÄ±lmÄ±ÅŸ makalelerin Ã¶zetini dÃ¶ndÃ¼r - geliÅŸmiÅŸ istatistiklerle"""
    try:
        from datetime import datetime, date, timedelta
        posted_articles = load_json(HISTORY_FILE)
        today = date.today()
        
        # Son 7 gÃ¼nlÃ¼k makaleleri al
        cutoff_date = datetime.now() - timedelta(days=7)
        recent_articles = []
        today_articles = []
        week_articles = []
        
        # En yÃ¼ksek skorlu makale
        highest_scored = None
        highest_score = 0
        
        # En son paylaÅŸÄ±lan makale
        latest_posted = None
        latest_date = None
        
        for article in posted_articles:
            try:
                posted_date_str = article.get("posted_date", "")
                if posted_date_str:
                    posted_date = datetime.fromisoformat(posted_date_str.replace('Z', '+00:00'))
                    
                    # BugÃ¼nkÃ¼ makaleler
                    if posted_date.date() == today:
                        today_articles.append(article)
                    
                    # Son 7 gÃ¼nlÃ¼k makaleler
                    if posted_date > cutoff_date:
                        recent_articles.append(article)
                        week_articles.append(article)
                    
                    # En son paylaÅŸÄ±lan
                    if latest_date is None or posted_date > latest_date:
                        latest_date = posted_date
                        latest_posted = article
                
                # En yÃ¼ksek skorlu makale (eÄŸer skor varsa)
                score = article.get("score", 0)
                if isinstance(score, (int, float)) and score > highest_score:
                    highest_score = score
                    highest_scored = article
                    
            except Exception as e:
                print(f"[DEBUG] Makale parse hatasÄ±: {e}")
                continue
        
        # BaÅŸarÄ± oranÄ± hesapla (basit: paylaÅŸÄ±lan / toplam)
        success_rate = (len(posted_articles) / max(len(posted_articles) + 1, 1)) * 100
        
        # Kategori daÄŸÄ±lÄ±mÄ± (eÄŸer varsa)
        category_distribution = {}
        for article in posted_articles:
            category = article.get("category", "Genel")
            category_distribution[category] = category_distribution.get(category, 0) + 1
        
        return {
            "total_posted": len(posted_articles),
            "recent_posted": len(recent_articles),
            "recent_articles": recent_articles[-5:],  # Son 5 makale
            "today_articles": len(today_articles),
            "week_articles": len(week_articles),
            "highest_scored": highest_scored,
            "latest_posted": latest_posted,
            "success_rate": success_rate,
            "category_distribution": category_distribution,
            "average_score": highest_score / max(len(posted_articles), 1) if highest_score > 0 else 0,
            "last_check_time": datetime.now().strftime("%H:%M") if posted_articles else "HenÃ¼z yok"
        }
        
    except Exception as e:
        print(f"PaylaÅŸÄ±lmÄ±ÅŸ makale Ã¶zeti hatasÄ±: {e}")
        return {
            "total_posted": 0, 
            "recent_posted": 0, 
            "recent_articles": [],
            "today_articles": 0,
            "week_articles": 0,
            "highest_scored": None,
            "latest_posted": None,
            "success_rate": 0,
            "category_distribution": {},
            "average_score": 0,
            "last_check_time": "HenÃ¼z yok"
        }

def create_automatic_backup():
    """GÃ¼nlÃ¼k otomatik yedekleme oluÅŸtur"""
    try:
        import shutil
        from datetime import datetime
        
        # BugÃ¼nÃ¼n tarihi
        today = datetime.now().strftime('%Y%m%d')
        backup_dir = f"daily_backup_{today}"
        
        # EÄŸer bugÃ¼nkÃ¼ yedekleme zaten varsa atla
        if os.path.exists(backup_dir):
            return {"success": True, "message": "BugÃ¼nkÃ¼ yedekleme zaten mevcut", "backup_dir": backup_dir}
        
        os.makedirs(backup_dir, exist_ok=True)
        
        files_to_backup = [
            "posted_articles.json",
            "pending_tweets.json", 
            "automation_settings.json",
            "news_sources.json"
        ]
        
        backup_count = 0
        for file_path in files_to_backup:
            if os.path.exists(file_path):
                backup_path = os.path.join(backup_dir, file_path)
                shutil.copy2(file_path, backup_path)
                backup_count += 1
        
        return {
            "success": True,
            "message": f"âœ… GÃ¼nlÃ¼k yedekleme oluÅŸturuldu: {backup_count} dosya",
            "backup_dir": backup_dir
        }
        
    except Exception as e:
        return {
            "success": False,
            "message": f"âŒ Yedekleme hatasÄ±: {str(e)}"
        }

def reset_all_data():
    """TÃ¼m uygulama verilerini sÄ±fÄ±rla - Otomatik yedekleme ile"""
    try:
        import shutil
        from datetime import datetime
        
        # Yedekleme klasÃ¶rÃ¼ oluÅŸtur
        backup_dir = f"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        os.makedirs(backup_dir, exist_ok=True)
        
        files_to_reset = [
            "posted_articles.json",
            "pending_tweets.json", 
            "summaries.json",
            "hashtags.json",
            "accounts.json",
            "automation_settings.json",
            "news_sources.json"
        ]
        
        # Ã–nce yedekle
        backup_count = 0
        for file_path in files_to_reset:
            if os.path.exists(file_path):
                backup_path = os.path.join(backup_dir, file_path)
                shutil.copy2(file_path, backup_path)
                backup_count += 1
                print(f"ğŸ“¦ {file_path} yedeklendi -> {backup_path}")
        
        # Sonra sÄ±fÄ±rla
        reset_count = 0
        for file_path in files_to_reset:
            if os.path.exists(file_path):
                save_json(file_path, [])
                reset_count += 1
                print(f"âœ… {file_path} sÄ±fÄ±rlandÄ±")
            else:
                # Dosya yoksa boÅŸ oluÅŸtur
                save_json(file_path, [])
                print(f"ğŸ†• {file_path} oluÅŸturuldu")
        
        return {
            "success": True,
            "message": f"âœ… {reset_count} dosya sÄ±fÄ±rlandÄ±, {backup_count} dosya yedeklendi ({backup_dir})",
            "reset_files": files_to_reset,
            "backup_dir": backup_dir
        }
        
    except Exception as e:
        return {
            "success": False,
            "message": f"âŒ SÄ±fÄ±rlama hatasÄ±: {str(e)}",
            "reset_files": []
        }

def clear_pending_tweets():
    """Sadece bekleyen tweet'leri temizle"""
    try:
        pending_tweets = load_json("pending_tweets.json")
        
        # Sadece posted olanlarÄ± tut, pending olanlarÄ± sil
        posted_tweets = [t for t in pending_tweets if t.get("status") == "posted"]
        
        save_json("pending_tweets.json", posted_tweets)
        
        cleared_count = len(pending_tweets) - len(posted_tweets)
        
        return {
            "success": True,
            "message": f"âœ… {cleared_count} bekleyen tweet temizlendi",
            "cleared_count": cleared_count
        }
        
    except Exception as e:
        return {
            "success": False,
            "message": f"âŒ Temizleme hatasÄ±: {str(e)}",
            "cleared_count": 0
        }

def get_data_statistics():
    """Veri istatistiklerini dÃ¶ndÃ¼r - bugÃ¼nkÃ¼ analizler dahil"""
    try:
        from datetime import datetime, date, timedelta
        today = date.today()
        stats = {}
        
        # PaylaÅŸÄ±lan makaleler
        posted_articles = load_json("posted_articles.json")
        stats["posted_articles"] = len(posted_articles)
        stats["total_articles"] = len(posted_articles)  # Template uyumluluÄŸu iÃ§in
        
        # BugÃ¼nkÃ¼ paylaÅŸÄ±lan makaleler (silinmemiÅŸ olanlar)
        today_articles = []
        active_articles = []
        deleted_articles = []
        
        for article in posted_articles:
            # SilinmiÅŸ/aktif ayrÄ±mÄ±
            if article.get("deleted", False):
                deleted_articles.append(article)
            else:
                active_articles.append(article)
            
            # BugÃ¼nkÃ¼ makaleler (sadece aktif olanlar)
            if article.get("posted_date") and not article.get("deleted", False):
                try:
                    # ISO format tarihini parse et
                    posted_date_str = article["posted_date"]
                    # Z suffix'ini kaldÄ±r ve parse et
                    if posted_date_str.endswith('Z'):
                        posted_date_str = posted_date_str[:-1] + '+00:00'
                    
                    posted_date = datetime.fromisoformat(posted_date_str)
                    article_date = posted_date.date()
                    
                    if article_date == today:
                        today_articles.append(article)
                        
                except (ValueError, TypeError) as e:
                    print(f"[DEBUG] Tarih parse hatasÄ±: {e} - {article.get('posted_date')}")
                    continue
        
        # DÃ¼nkÃ¼ makaleler de hesapla
        yesterday = today - timedelta(days=1)
        yesterday_articles = []
        
        for article in active_articles:
            if article.get("posted_date"):
                try:
                    posted_date_str = article["posted_date"]
                    if posted_date_str.endswith('Z'):
                        posted_date_str = posted_date_str[:-1] + '+00:00'
                    
                    posted_date = datetime.fromisoformat(posted_date_str)
                    article_date = posted_date.date()
                    
                    if article_date == yesterday:
                        yesterday_articles.append(article)
                        
                except (ValueError, TypeError):
                    continue
        
        stats["today_articles"] = len(today_articles)
        stats["yesterday_articles"] = len(yesterday_articles)
        stats["active_articles"] = len(active_articles)
        stats["deleted_articles"] = len(deleted_articles)
        
        # Bekleyen tweet'ler
        pending_tweets = load_json("pending_tweets.json")
        pending_count = len([t for t in pending_tweets if t.get("status") == "pending"])
        posted_count = len([t for t in pending_tweets if t.get("status") == "posted"])
        stats["pending_tweets"] = pending_count
        stats["posted_tweets_in_pending"] = posted_count
        
        # BugÃ¼nkÃ¼ bekleyen tweet'ler
        today_pending = []
        for tweet in pending_tweets:
            # created_date veya created_at alanÄ±nÄ± kontrol et
            date_field = tweet.get("created_date") or tweet.get("created_at")
            if date_field:
                try:
                    created_date = datetime.fromisoformat(date_field.replace('Z', '+00:00'))
                    # Status kontrolÃ¼ - eÄŸer status yoksa pending kabul et
                    tweet_status = tweet.get("status", "pending")
                    if created_date.date() == today and tweet_status == "pending":
                        today_pending.append(tweet)
                except (ValueError, TypeError):
                    continue
        
        stats["today_pending"] = len(today_pending)
        
        # Debug iÃ§in log ekle
        print(f"[DEBUG] BugÃ¼nkÃ¼ pending tweet'ler: {len(today_pending)} / {len(pending_tweets)} toplam")
        
        # Ã–zetler
        summaries = load_json("summaries.json")
        stats["summaries"] = len(summaries)
        
        # Hashtag'ler
        hashtags = load_json("hashtags.json")
        stats["hashtags"] = len(hashtags)
        
        # Hesaplar
        accounts = load_json("accounts.json")
        stats["accounts"] = len(accounts)
        
        # BugÃ¼nkÃ¼ toplam aktivite
        stats["today_total_activity"] = stats["today_articles"] + stats["today_pending"]
        
        # Dosya boyutlarÄ±nÄ± hesapla
        import os
        
        def format_file_size(size_bytes):
            """Dosya boyutunu okunabilir formata Ã§evir"""
            if size_bytes == 0:
                return "0 B"
            size_names = ["B", "KB", "MB", "GB"]
            i = 0
            while size_bytes >= 1024 and i < len(size_names) - 1:
                size_bytes /= 1024.0
                i += 1
            return f"{size_bytes:.1f} {size_names[i]}"
        
        def get_file_size(filename):
            """Dosya boyutunu gÃ¼venli ÅŸekilde al"""
            try:
                if os.path.exists(filename):
                    size = os.path.getsize(filename)
                    return format_file_size(size)
                else:
                    return "Dosya yok"
            except Exception as e:
                return f"Hata: {str(e)}"
        
        # Dosya boyutlarÄ±nÄ± hesapla
        stats["articles_file_size"] = get_file_size("posted_articles.json")
        stats["pending_file_size"] = get_file_size("pending_tweets.json")
        stats["settings_file_size"] = get_file_size("automation_settings.json")
        
        print(f"[DEBUG] Ä°statistikler: BugÃ¼n {stats['today_articles']} paylaÅŸÄ±m, {stats['today_pending']} bekleyen (PaylaÅŸÄ±m tarihine gÃ¶re)")
        
        return stats
        
    except Exception as e:
        print(f"Ä°statistik hatasÄ±: {e}")
        return {
            "posted_articles": 0,
            "total_articles": 0,
            "today_articles": 0,
            "pending_tweets": 0,
            "today_pending": 0,
            "posted_tweets_in_pending": 0,
            "summaries": 0,
            "hashtags": 0,
            "accounts": 0,
            "today_total_activity": 0,
            "articles_file_size": "N/A",
            "pending_file_size": "N/A",
            "settings_file_size": "N/A"
        }

def load_automation_settings():
    """OtomatikleÅŸtirme ayarlarÄ±nÄ± yÃ¼kle"""
    try:
        settings_data = load_json("automation_settings.json")
        
        # EÄŸer liste ise (eski format), boÅŸ dict dÃ¶ndÃ¼r
        if isinstance(settings_data, list):
            settings = {}
        else:
            settings = settings_data
        
        # VarsayÄ±lan ayarlar
        default_settings = {
            "auto_mode": False,
            "min_score": 5,
            "check_interval_hours": 3,
            "max_articles_per_run": 10,
            "auto_post_enabled": False,
            "require_manual_approval": True,
            "working_hours_only": False,
            "working_hours_start": "09:00",
            "working_hours_end": "18:00",
            "weekend_enabled": True,
            "last_updated": datetime.now().isoformat()
        }
        
        # Eksik ayarlarÄ± varsayÄ±lanlarla doldur
        for key, value in default_settings.items():
            if key not in settings:
                settings[key] = value
        
        return settings
        
    except Exception as e:
        print(f"Ayarlar yÃ¼kleme hatasÄ±: {e}")
        # VarsayÄ±lan ayarlarÄ± dÃ¶ndÃ¼r
        return {
            "auto_mode": False,
            "min_score": 5,
            "check_interval_hours": 3,
            "max_articles_per_run": 10,
            "auto_post_enabled": False,
            "require_manual_approval": True,
            "working_hours_only": False,
            "working_hours_start": "09:00",
            "working_hours_end": "18:00",
            "weekend_enabled": True,
            "last_updated": datetime.now().isoformat()
        }

def save_automation_settings(settings):
    """OtomatikleÅŸtirme ayarlarÄ±nÄ± kaydet"""
    try:
        settings["last_updated"] = datetime.now().isoformat()
        save_json("automation_settings.json", settings)
        return {"success": True, "message": "âœ… Ayarlar baÅŸarÄ±yla kaydedildi"}
    except Exception as e:
        return {"success": False, "message": f"âŒ Ayarlar kaydedilemedi: {e}"}

def get_automation_status():
    """OtomatikleÅŸtirme durumunu kontrol et"""
    try:
        settings = load_automation_settings()
        
        # Ã‡alÄ±ÅŸma saatleri kontrolÃ¼
        if settings.get("working_hours_only", False):
            from datetime import datetime, time
            now = datetime.now()
            start_time = datetime.strptime(settings.get("working_hours_start", "09:00"), "%H:%M").time()
            end_time = datetime.strptime(settings.get("working_hours_end", "18:00"), "%H:%M").time()
            
            current_time = now.time()
            is_working_hours = start_time <= current_time <= end_time
            
            # Hafta sonu kontrolÃ¼
            is_weekend = now.weekday() >= 5  # 5=Cumartesi, 6=Pazar
            weekend_allowed = settings.get("weekend_enabled", True)
            
            if is_weekend and not weekend_allowed:
                return {
                    "active": False,
                    "auto_mode": False,
                    "check_interval_hours": settings.get("check_interval_hours", 2),
                    "min_score_threshold": settings.get("min_score_threshold", 5),
                    "reason": "Hafta sonu Ã§alÄ±ÅŸma devre dÄ±ÅŸÄ±",
                    "settings": settings
                }
            
            if not is_working_hours:
                return {
                    "active": False,
                    "auto_mode": False,
                    "check_interval_hours": settings.get("check_interval_hours", 2),
                    "min_score_threshold": settings.get("min_score_threshold", 5),
                    "reason": f"Ã‡alÄ±ÅŸma saatleri dÄ±ÅŸÄ±nda ({start_time}-{end_time})",
                    "settings": settings
                }
        
        return {
            "active": settings.get("auto_mode", False),
            "auto_mode": settings.get("auto_mode", False),  # Template uyumluluÄŸu iÃ§in
            "check_interval_hours": settings.get("check_interval_hours", 2),
            "min_score_threshold": settings.get("min_score_threshold", 5),
            "reason": "Aktif" if settings.get("auto_mode", False) else "Manuel mod",
            "settings": settings
        }
        
    except Exception as e:
        return {
            "active": False,
            "auto_mode": False,
            "check_interval_hours": 2,
            "min_score_threshold": 5,
            "reason": f"Hata: {e}",
            "settings": {}
        }

def update_scheduler_settings():
    """Scheduler ayarlarÄ±nÄ± gÃ¼ncelle (scheduler.py iÃ§in)"""
    try:
        settings = load_automation_settings()
        
        # Scheduler iÃ§in ayarlar dosyasÄ± oluÅŸtur
        scheduler_config = {
            "auto_mode": settings.get("auto_mode", False),
            "min_score": settings.get("min_score", 5),
            "check_interval_hours": settings.get("check_interval_hours", 3),
            "max_articles_per_run": settings.get("max_articles_per_run", 10),
            "auto_post_enabled": settings.get("auto_post_enabled", False),
            "last_updated": datetime.now().isoformat()
        }
        
        save_json("scheduler_config.json", scheduler_config)
        return {"success": True, "message": "Scheduler ayarlarÄ± gÃ¼ncellendi"}
        
    except Exception as e:
        return {"success": False, "message": f"Scheduler ayarlarÄ± gÃ¼ncellenemedi: {e}"}

def validate_automation_settings(settings):
    """OtomatikleÅŸtirme ayarlarÄ±nÄ± doÄŸrula"""
    errors = []
    
    # Minimum skor kontrolÃ¼
    min_score = settings.get("min_score", 5)
    if not isinstance(min_score, int) or min_score < 1 or min_score > 10:
        errors.append("Minimum skor 1-10 arasÄ±nda olmalÄ±")
    
    # Kontrol aralÄ±ÄŸÄ± kontrolÃ¼
    interval = settings.get("check_interval_hours", 3)
    if not isinstance(interval, (int, float)) or interval < 0.5 or interval > 24:
        errors.append("Kontrol aralÄ±ÄŸÄ± 0.5-24 saat arasÄ±nda olmalÄ±")
    
    # Maksimum makale sayÄ±sÄ± kontrolÃ¼
    max_articles = settings.get("max_articles_per_run", 10)
    if not isinstance(max_articles, int) or max_articles < 1 or max_articles > 50:
        errors.append("Maksimum makale sayÄ±sÄ± 1-50 arasÄ±nda olmalÄ±")
    
    # Ã‡alÄ±ÅŸma saatleri kontrolÃ¼
    try:
        start_time = settings.get("working_hours_start", "09:00")
        end_time = settings.get("working_hours_end", "18:00")
        datetime.strptime(start_time, "%H:%M")
        datetime.strptime(end_time, "%H:%M")
    except ValueError:
        errors.append("Ã‡alÄ±ÅŸma saatleri HH:MM formatÄ±nda olmalÄ±")
    

    
    return errors

def send_telegram_notification(message, tweet_url="", article_title=""):
    """Telegram bot'a bildirim gÃ¶nder - Bot token env'den, Chat ID settings'den"""
    try:
        # Bot token'Ä± environment variable'dan Ã§ek
        bot_token = os.getenv("TELEGRAM_BOT_TOKEN", "").strip()
        
        # Chat ID'yi settings'den Ã§ek
        settings = load_automation_settings()
        chat_id = settings.get("telegram_chat_id", "").strip()
        
        # EÄŸer bot token env'de yoksa settings'den dene (geriye dÃ¶nÃ¼k uyumluluk)
        if not bot_token:
            bot_token = settings.get("telegram_bot_token", "").strip()
        
        # Telegram bildirimleri kapalÄ± mÄ± kontrol et
        if not settings.get("telegram_notifications", True):  # VarsayÄ±lan True
            print("[DEBUG] Telegram bildirimleri kapalÄ±")
            return {"success": False, "reason": "disabled"}
        
        if not bot_token:
            safe_log("Telegram bot token eksik. .env dosyasÄ±nda TELEGRAM_BOT_TOKEN ayarlayÄ±n.", "WARNING")
            return {"success": False, "reason": "missing_bot_token"}
            
        if not chat_id:
            print("[INFO] Chat ID eksik, otomatik algÄ±lama deneniyor...")
            # Otomatik Chat ID algÄ±lamayÄ± dene
            auto_result = auto_detect_and_save_chat_id()
            
            if auto_result["success"]:
                chat_id = auto_result["chat_id"]
                print(f"[SUCCESS] Chat ID otomatik algÄ±landÄ±: {chat_id}")
            else:
                print(f"[WARNING] Chat ID otomatik algÄ±lanamadÄ±: {auto_result.get('error', 'Bilinmeyen hata')}")
                print("[INFO] Bot'a @tweet62_bot adresinden bir mesaj gÃ¶nderin ve tekrar deneyin.")
                return {"success": False, "reason": "missing_chat_id", "auto_detect_error": auto_result.get('error')}
        
        # Telegram mesajÄ±nÄ± hazÄ±rla
        telegram_message = f"ğŸ¤– **Yeni Tweet PaylaÅŸÄ±ldÄ±!**\n\n"
        
        if article_title:
            telegram_message += f"ğŸ“° **Makale:** {article_title}\n\n"
        
        telegram_message += f"ğŸ’¬ **Tweet:** {message}\n\n"
        
        if tweet_url:
            telegram_message += f"ğŸ”— **Link:** {tweet_url}\n\n"
        
        telegram_message += f"â° **Zaman:** {datetime.now().strftime('%d.%m.%Y %H:%M')}"
        
        # Telegram API'ye gÃ¶nder
        url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
        
        payload = {
            "chat_id": str(chat_id),
            "text": telegram_message,
            "parse_mode": "Markdown",
            "disable_web_page_preview": False
        }
        
        response = requests.post(url, json=payload, timeout=10)
        
        if response.status_code == 200:
            print(f"[SUCCESS] Telegram bildirimi gÃ¶nderildi: {chat_id}")
            return {"success": True, "message_id": response.json().get("result", {}).get("message_id")}
        else:
            print(f"[ERROR] Telegram API hatasÄ±: {response.status_code} - {response.text}")
            return {"success": False, "error": f"API Error: {response.status_code}"}
            
    except Exception as e:
        print(f"[ERROR] Telegram bildirim hatasÄ±: {e}")
        return {"success": False, "error": str(e)}

def test_telegram_connection():
    """Telegram bot baÄŸlantÄ±sÄ±nÄ± test et - Bot token env'den, Chat ID settings'den"""
    try:
        # Bot token'Ä± environment variable'dan Ã§ek
        bot_token = os.getenv("TELEGRAM_BOT_TOKEN", "").strip()
        
        # Chat ID'yi settings'den Ã§ek
        settings = load_automation_settings()
        chat_id = settings.get("telegram_chat_id", "").strip()
        
        # EÄŸer bot token env'de yoksa settings'den dene (geriye dÃ¶nÃ¼k uyumluluk)
        if not bot_token:
            bot_token = settings.get("telegram_bot_token", "").strip()
        
        if not bot_token:
            return {
                "success": False, 
                "error": "Bot token eksik. .env dosyasÄ±nda TELEGRAM_BOT_TOKEN ayarlayÄ±n."
            }
            
        if not chat_id:
            print("[INFO] Chat ID eksik, otomatik algÄ±lama deneniyor...")
            # Otomatik Chat ID algÄ±lamayÄ± dene
            auto_result = auto_detect_and_save_chat_id()
            
            if auto_result["success"]:
                chat_id = auto_result["chat_id"]
                print(f"[SUCCESS] Chat ID otomatik algÄ±landÄ±: {chat_id}")
            else:
                return {
                    "success": False, 
                    "error": f"Chat ID eksik ve otomatik algÄ±lanamadÄ±: {auto_result.get('error', 'Bilinmeyen hata')}. Bot'a @tweet62_bot adresinden bir mesaj gÃ¶nderin."
                }
        
        # Bot bilgilerini al
        url = f"https://api.telegram.org/bot{bot_token}/getMe"
        response = requests.get(url, timeout=10)
        
        if response.status_code != 200:
            return {"success": False, "error": f"Bot token geÃ§ersiz: {response.status_code}"}
        
        bot_info = response.json().get("result", {})
        
        # Test mesajÄ± gÃ¶nder
        test_message = f"ğŸ§ª **Test MesajÄ±**\n\nBot baÅŸarÄ±yla baÄŸlandÄ±!\n\nâ° {datetime.now().strftime('%d.%m.%Y %H:%M')}"
        
        send_url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
        payload = {
            "chat_id": str(chat_id),
            "text": test_message,
            "parse_mode": "Markdown"
        }
        
        send_response = requests.post(send_url, json=payload, timeout=10)
        
        if send_response.status_code == 200:
            return {
                "success": True, 
                "bot_name": bot_info.get("first_name", "Unknown"),
                "bot_username": bot_info.get("username", "Unknown")
            }
        else:
            error_detail = send_response.text
            return {"success": False, "error": f"Mesaj gÃ¶nderilemedi: {send_response.status_code} - {error_detail}"}
            
    except Exception as e:
        return {"success": False, "error": str(e)}

def check_telegram_configuration():
    """Telegram konfigÃ¼rasyonunu kontrol et - Bot token env'den, Chat ID settings'den"""
    try:
        # Bot token environment variable'dan
        bot_token = os.getenv("TELEGRAM_BOT_TOKEN", "").strip()
        
        # Chat ID settings'den
        settings = load_automation_settings()
        chat_id = settings.get("telegram_chat_id", "").strip()
        
        # Geriye dÃ¶nÃ¼k uyumluluk iÃ§in settings'den bot token kontrol et
        settings_bot_token = settings.get("telegram_bot_token", "").strip()
        
        status = {
            "bot_token_env": bool(bot_token),
            "bot_token_settings": bool(settings_bot_token),
            "chat_id_set": bool(chat_id),
            "ready": bool((bot_token or settings_bot_token) and chat_id)
        }
        
        if status["ready"]:
            if status["bot_token_env"]:
                status["message"] = "âœ… Telegram yapÄ±landÄ±rmasÄ± tamamlanmÄ±ÅŸ (Bot token: ENV, Chat ID: Ayarlar)"
            else:
                status["message"] = "âœ… Telegram yapÄ±landÄ±rmasÄ± tamamlanmÄ±ÅŸ (Bot token: Ayarlar, Chat ID: Ayarlar)"
            status["status"] = "ready"
        elif status["bot_token_env"] or status["bot_token_settings"]:
            if not status["chat_id_set"]:
                status["message"] = "âš ï¸ Bot token var, Chat ID eksik - 'Chat ID Bul' butonu ile ayarlayÄ±n"
                status["status"] = "partial"
            else:
                status["message"] = "âœ… Telegram yapÄ±landÄ±rmasÄ± tamamlanmÄ±ÅŸ"
                status["status"] = "ready"
        else:
            status["message"] = "âŒ Bot token eksik - .env dosyasÄ±nda TELEGRAM_BOT_TOKEN ayarlayÄ±n"
            status["status"] = "missing"
            
        return status
        
    except Exception as e:
        return {
            "bot_token_env": False,
            "bot_token_settings": False,
            "chat_id_set": False,
            "ready": False,
            "message": f"âŒ Kontrol hatasÄ±: {e}",
            "status": "error"
        }

def get_telegram_chat_id(bot_token=None):
    """Bot'a mesaj gÃ¶nderen kullanÄ±cÄ±larÄ±n chat ID'lerini al - Environment variable'lardan token Ã§eker"""
    try:
        # EÄŸer bot_token parametre olarak verilmemiÅŸse env'den Ã§ek
        if not bot_token:
            bot_token = os.getenv("TELEGRAM_BOT_TOKEN", "").strip()
            
            # EÄŸer env'de yoksa settings'den dene
            if not bot_token:
                settings = load_automation_settings()
                bot_token = settings.get("telegram_bot_token", "").strip()
        
        if not bot_token:
            return {
                "success": False, 
                "error": "Bot token eksik. .env dosyasÄ±nda TELEGRAM_BOT_TOKEN ayarlayÄ±n."
            }
        
        url = f"https://api.telegram.org/bot{bot_token}/getUpdates"
        response = requests.get(url, timeout=10)
        
        if response.status_code != 200:
            return {"success": False, "error": "Bot token geÃ§ersiz"}
        
        data = response.json()
        updates = data.get("result", [])
        
        chat_ids = []
        for update in updates[-10:]:  # Son 10 mesaj
            message = update.get("message", {})
            chat = message.get("chat", {})
            if chat.get("id"):
                chat_info = {
                    "chat_id": chat.get("id"),
                    "type": chat.get("type"),
                    "title": chat.get("title") or f"{chat.get('first_name', '')} {chat.get('last_name', '')}".strip()
                }
                if chat_info not in chat_ids:
                    chat_ids.append(chat_info)
        
        return {"success": True, "chat_ids": chat_ids}
        
    except Exception as e:
        return {"success": False, "error": str(e)}

def save_telegram_chat_id(chat_id):
    """Chat ID'yi otomatik olarak ayarlara kaydet"""
    try:
        settings = load_automation_settings()
        settings["telegram_chat_id"] = str(chat_id).strip()
        
        save_result = save_automation_settings(settings)
        
        if save_result["success"]:
            print(f"[SUCCESS] Chat ID otomatik kaydedildi: {chat_id}")
            return {"success": True, "message": f"âœ… Chat ID kaydedildi: {chat_id}"}
        else:
            print(f"[ERROR] Chat ID kaydetme hatasÄ±: {save_result['message']}")
            return {"success": False, "error": f"Kaydetme hatasÄ±: {save_result['message']}"}
            
    except Exception as e:
        print(f"[ERROR] Chat ID kaydetme hatasÄ±: {e}")
        return {"success": False, "error": str(e)}

def auto_detect_and_save_chat_id():
    """Otomatik chat ID tespit et ve kaydet"""
    try:
        # Mevcut chat ID'yi kontrol et
        settings = load_automation_settings()
        current_chat_id = settings.get("telegram_chat_id", "").strip()
        
        if current_chat_id:
            return {
                "success": True, 
                "message": f"Chat ID zaten ayarlanmÄ±ÅŸ: {current_chat_id}",
                "chat_id": current_chat_id,
                "auto_detected": False
            }
        
        # Chat ID'leri bul
        result = get_telegram_chat_id()
        
        if not result["success"]:
            return {
                "success": False,
                "error": result["error"],
                "auto_detected": False
            }
        
        chat_ids = result.get("chat_ids", [])
        
        if not chat_ids:
            return {
                "success": False,
                "error": "Chat ID bulunamadÄ±. Bot'a Ã¶nce bir mesaj gÃ¶nderin.",
                "auto_detected": False
            }
        
        # Ä°lk chat ID'yi otomatik seÃ§ (genellikle en son mesaj)
        selected_chat = chat_ids[0]
        chat_id = selected_chat["chat_id"]
        
        # Chat ID'yi kaydet
        save_result = save_telegram_chat_id(chat_id)
        
        if save_result["success"]:
            return {
                "success": True,
                "message": f"âœ… Chat ID otomatik tespit edildi ve kaydedildi: {chat_id}",
                "chat_id": chat_id,
                "chat_info": selected_chat,
                "auto_detected": True,
                "all_chats": chat_ids
            }
        else:
            return {
                "success": False,
                "error": save_result["error"],
                "auto_detected": False
            }
            
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "auto_detected": False
        }

def load_mcp_config():
    """MCP konfigÃ¼rasyonunu yÃ¼kle"""
    try:
        if os.path.exists(MCP_CONFIG_FILE):
            config = load_json(MCP_CONFIG_FILE)
        else:
            # VarsayÄ±lan konfigÃ¼rasyon
            config = {
                "mcp_enabled": False,
                "firecrawl_mcp": {
                    "enabled": False,
                    "server_url": "http://localhost:3000",
                    "api_key": "",
                    "timeout": 30,
                    "retry_count": 3,
                    "fallback_enabled": True
                },
                "content_extraction": {
                    "max_content_length": 2500,
                    "min_content_length": 100,
                    "wait_time": 3000,
                    "remove_base64_images": True,
                    "only_main_content": True
                },
                "ai_analysis": {
                    "enabled": True,
                    "max_tokens": 300,
                    "temperature": 0.7,
                    "model": "deepseek/deepseek-chat-v3-0324:free",
                    "fallback_enabled": True
                },
                "last_updated": datetime.now().isoformat()
            }
            save_json(MCP_CONFIG_FILE, config)
        
        return config
        
    except Exception as e:
        print(f"MCP konfigÃ¼rasyon yÃ¼kleme hatasÄ±: {e}")
        return {
            "mcp_enabled": False,
            "firecrawl_mcp": {"enabled": False, "fallback_enabled": True},
            "ai_analysis": {"enabled": True, "fallback_enabled": True}
        }

def save_mcp_config(config):
    """MCP konfigÃ¼rasyonunu kaydet"""
    try:
        config["last_updated"] = datetime.now().isoformat()
        save_json(MCP_CONFIG_FILE, config)
        return {"success": True, "message": "âœ… MCP konfigÃ¼rasyonu kaydedildi"}
    except Exception as e:
        return {"success": False, "message": f"âŒ MCP konfigÃ¼rasyonu kaydedilemedi: {e}"}

def get_mcp_status():
    """MCP durumunu kontrol et"""
    try:
        config = load_mcp_config()
        
        status = {
            "mcp_enabled": config.get("mcp_enabled", False),
            "firecrawl_enabled": config.get("firecrawl_mcp", {}).get("enabled", False),
            "ai_analysis_enabled": config.get("ai_analysis", {}).get("enabled", True),
            "fallback_available": True,
            "ready": False
        }
        
        # MCP hazÄ±r mÄ± kontrol et
        if status["mcp_enabled"] and status["firecrawl_enabled"]:
            # Firecrawl MCP server baÄŸlantÄ±sÄ±nÄ± test et
            try:
                server_url = config.get("firecrawl_mcp", {}).get("server_url", "")
                if server_url:
                    # Basit ping testi (gerÃ§ek implementasyonda MCP server'a ping atÄ±lacak)
                    status["ready"] = True
                    status["message"] = "âœ… MCP Firecrawl aktif ve hazÄ±r"
                else:
                    status["message"] = "âš ï¸ MCP server URL'si eksik"
            except:
                status["message"] = "âŒ MCP server baÄŸlantÄ±sÄ± baÅŸarÄ±sÄ±z"
        elif status["ai_analysis_enabled"]:
            status["message"] = "âœ… AI analizi aktif (MCP olmadan)"
        else:
            status["message"] = "âš ï¸ MCP ve AI analizi devre dÄ±ÅŸÄ±"
        
        return status
        
    except Exception as e:
        return {
            "mcp_enabled": False,
            "firecrawl_enabled": False,
            "ai_analysis_enabled": True,
            "fallback_available": True,
            "ready": False,
            "message": f"âŒ MCP durum kontrolÃ¼ hatasÄ±: {e}"
        }

def test_mcp_connection():
    """MCP baÄŸlantÄ±sÄ±nÄ± test et"""
    try:
        config = load_mcp_config()
        
        if not config.get("mcp_enabled", False):
            return {
                "success": False,
                "message": "MCP devre dÄ±ÅŸÄ±",
                "details": "MCP konfigÃ¼rasyondan etkinleÅŸtirilmeli"
            }
        
        firecrawl_config = config.get("firecrawl_mcp", {})
        
        if not firecrawl_config.get("enabled", False):
            return {
                "success": False,
                "message": "Firecrawl MCP devre dÄ±ÅŸÄ±",
                "details": "Firecrawl MCP konfigÃ¼rasyondan etkinleÅŸtirilmeli"
            }
        
        server_url = firecrawl_config.get("server_url", "")
        
        if not server_url:
            return {
                "success": False,
                "message": "MCP server URL'si eksik",
                "details": "KonfigÃ¼rasyonda server_url ayarlanmalÄ±"
            }
        
        # GerÃ§ek MCP implementasyonunda burada MCP server'a test Ã§aÄŸrÄ±sÄ± yapÄ±lacak
        # Åimdilik simÃ¼le ediyoruz
        print(f"[TEST] MCP server test ediliyor: {server_url}")
        
        # Test URL'si ile basit scrape denemesi
        test_result = mcp_firecrawl_scrape({
            "url": "https://example.com",
            "formats": ["markdown"],
            "onlyMainContent": True
        })
        
        if test_result.get("success", False):
            return {
                "success": True,
                "message": "âœ… MCP Firecrawl baÄŸlantÄ±sÄ± baÅŸarÄ±lÄ±",
                "details": f"Server: {server_url}"
            }
        else:
            return {
                "success": False,
                "message": "âŒ MCP Firecrawl test baÅŸarÄ±sÄ±z",
                "details": test_result.get("reason", "Bilinmeyen hata")
            }
        
    except Exception as e:
        return {
            "success": False,
            "message": f"âŒ MCP test hatasÄ±: {e}",
            "details": "BaÄŸlantÄ± testi sÄ±rasÄ±nda hata oluÅŸtu"
        }

def gemini_ocr_image(image_path):
    import google.generativeai as genai
    import os
    from PIL import Image

    api_key = os.environ.get('GOOGLE_API_KEY')
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel('gemini-2.0-flash')

    # Resmi PIL ile aÃ§
    image = Image.open(image_path)
    prompt = "Bu gÃ¶rseldeki tÃ¼m metni OCR ile Ã§Ä±kar ve sadece metni dÃ¶ndÃ¼r."
    response = model.generate_content([prompt, image])
    return response.text.strip()

def setup_twitter_v2_client():
    """Tweepy v2 API Client ile kimlik doÄŸrulama (sadece metinli tweet iÃ§in)"""
    import tweepy
    import os
    # v2 API Client
    client = tweepy.Client(
        bearer_token=os.environ.get('TWITTER_BEARER_TOKEN'),
        consumer_key=os.environ.get('TWITTER_API_KEY'),
        consumer_secret=os.environ.get('TWITTER_API_SECRET'),
        access_token=os.environ.get('TWITTER_ACCESS_TOKEN'),
        access_token_secret=os.environ.get('TWITTER_ACCESS_TOKEN_SECRET')
    )
    return client



def post_text_tweet_v2(tweet_text):
    """Sadece metinli tweet atmak iÃ§in Tweepy v2 API kullanÄ±mÄ± - Rate limit kontrolÃ¼ ile"""
    try:
        import tweepy
        
        # Rate limit kontrolÃ¼ yap
        rate_check = check_rate_limit("tweets")
        if not rate_check.get("allowed", True):
            wait_minutes = int(rate_check.get("wait_time", 0) / 60) + 1
            error_msg = f"Twitter API rate limit aÅŸÄ±ldÄ±. {wait_minutes} dakika sonra tekrar deneyin. ({rate_check.get('requests_made', 0)}/{rate_check.get('limit', 300)} istek kullanÄ±ldÄ±)"
            safe_log(error_msg, "WARNING")
            return {"success": False, "error": error_msg, "rate_limited": True, "wait_minutes": wait_minutes}
        
        client = setup_twitter_v2_client()
        TWITTER_LIMIT = 280
        if len(tweet_text) > TWITTER_LIMIT:
            tweet_text = tweet_text[:TWITTER_LIMIT-3] + "..."
        safe_log(f"Tweet uzunluÄŸu: {len(tweet_text)}", "DEBUG")
        
        response = client.create_tweet(text=tweet_text)
        
        # Rate limit kullanÄ±mÄ±nÄ± gÃ¼ncelle
        update_rate_limit_usage("tweets")
        
        if hasattr(response, 'data') and response.data and 'id' in response.data:
            tweet_id = response.data['id']
            tweet_url = f"https://twitter.com/user/status/{tweet_id}"
            safe_log(f"Tweet baÅŸarÄ±yla gÃ¶nderildi: {tweet_url}", "INFO")
            return {"success": True, "tweet_id": tweet_id, "url": tweet_url}
        else:
            safe_log(f"Tweet gÃ¶nderilemedi: {response}", "ERROR")
            return {"success": False, "error": "Tweet gÃ¶nderilemedi"}
            
    except tweepy.TooManyRequests as rate_limit_error:
        # API'den gelen rate limit bilgilerini gÃ¼ncelle
        try:
            # Rate limit durumunu gÃ¼ncelle
            status = load_rate_limit_status()
            current_time = time.time()
            if "tweets" not in status:
                status["tweets"] = {}
            status["tweets"]["requests"] = TWITTER_RATE_LIMITS["tweets"]["limit"]  # Limit dolu
            status["tweets"]["reset_time"] = current_time + 3600  # 1 saat sonra reset (Free plan iÃ§in gÃ¼venli)
            save_rate_limit_status(status)
        except:
            pass
        
        wait_minutes = 15  # Twitter API rate limit genelde 15 dakika
        error_msg = f"Twitter API rate limit aÅŸÄ±ldÄ±: {rate_limit_error}\n{wait_minutes} dakika sonra tekrar deneyin."
        safe_log(error_msg, "WARNING")
        return {"success": False, "error": error_msg, "rate_limited": True, "wait_minutes": wait_minutes}
        
    except tweepy.Unauthorized as auth_error:
        safe_log(f"Twitter API yetkilendirme hatasÄ±: {auth_error}", "ERROR")
        return {"success": False, "error": f"Twitter API yetkilendirme hatasÄ±: {auth_error}"}
        
    except tweepy.Forbidden as forbidden_error:
        safe_log(f"Twitter API yasak iÅŸlem: {forbidden_error}", "ERROR")
        return {"success": False, "error": f"Twitter API yasak iÅŸlem: {forbidden_error}"}
        
    except Exception as e:
        safe_log(f"Tweet paylaÅŸÄ±m genel hatasÄ±: {e}", "ERROR")
        return {"success": False, "error": str(e)}

def fetch_url_content_with_mcp(url):
    """MCP ile URL iÃ§eriÄŸi Ã§ekme - Tweet oluÅŸturma iÃ§in"""
    try:
        print(f"ğŸ” MCP ile URL iÃ§eriÄŸi Ã§ekiliyor: {url}")
        
        # Firecrawl MCP scrape fonksiyonunu kullan
        scrape_result = mcp_firecrawl_scrape({
            "url": url,
            "formats": ["markdown"],
            "onlyMainContent": True,
            "waitFor": 3000,
            "removeBase64Images": True
        })
        
        if not scrape_result.get("success", False):
            print(f"âš ï¸ MCP baÅŸarÄ±sÄ±z, fallback deneniyor...")
            return fetch_url_content_fallback(url)
        
        # Markdown iÃ§eriÄŸini al
        markdown_content = scrape_result.get("markdown", "")
        
        if not markdown_content or len(markdown_content) < 100:
            print(f"âš ï¸ MCP'den yetersiz iÃ§erik, fallback deneniyor...")
            return fetch_url_content_fallback(url)
        
        # BaÅŸlÄ±ÄŸÄ± Ã§Ä±kar (genellikle ilk # ile baÅŸlar)
        lines = markdown_content.split('\n')
        title = ""
        content_lines = []
        
        for line in lines:
            line = line.strip()
            if line.startswith('# ') and not title:
                title = line[2:].strip()
            elif line and not line.startswith('#') and len(line) > 20:
                content_lines.append(line)
        
        # Ä°Ã§eriÄŸi birleÅŸtir ve temizle
        content = '\n'.join(content_lines)
        
        # Gereksiz karakterleri temizle
        content = content.replace('*', '').replace('**', '').replace('_', '')
        content = ' '.join(content.split())  # Ã‡oklu boÅŸluklarÄ± tek boÅŸluÄŸa Ã§evir
        
        # Ä°Ã§eriÄŸi sÄ±nÄ±rla
        content = content[:2000]
        
        print(f"âœ… MCP ile URL iÃ§eriÄŸi Ã§ekildi: {len(content)} karakter")
        
        return {
            "title": title or "BaÅŸlÄ±k bulunamadÄ±",
            "content": content,
            "url": url,
            "source": "mcp"
        }
        
    except Exception as e:
        print(f"âŒ MCP URL Ã§ekme hatasÄ± ({url}): {e}")
        print("ğŸ”„ Fallback yÃ¶nteme geÃ§iliyor...")
        return fetch_url_content_fallback(url)

def fetch_url_content_fallback(url):
    """Fallback URL iÃ§eriÄŸi Ã§ekme - BeautifulSoup ile"""
    try:
        print(f"ğŸ”„ Fallback ile URL iÃ§eriÄŸi Ã§ekiliyor: {url}")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, "html.parser")
        
        # BaÅŸlÄ±ÄŸÄ± bul
        title = ""
        title_selectors = ["h1", "h1.entry-title", "h1.post-title", ".article-title h1", "title"]
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                title = title_elem.text.strip()
                break
        
        # Ä°Ã§eriÄŸi bul - Ã§oklu selector deneme
        content_selectors = [
            "article p",
            "div.article-content p",
            "div.entry-content p", 
            "div.post-content p",
            "div.content p",
            ".article-body p",
            "main p",
            ".post p"
        ]
        
        content = ""
        for selector in content_selectors:
            paragraphs = soup.select(selector)
            if paragraphs:
                content = "\n".join([p.text.strip() for p in paragraphs if p.text.strip() and len(p.text.strip()) > 20])
                if len(content) > 200:  # Yeterli iÃ§erik bulundu
                    break
        
        # EÄŸer hala iÃ§erik bulunamadÄ±ysa, tÃ¼m p etiketlerini dene
        if not content:
            all_paragraphs = soup.find_all('p')
            content = "\n".join([p.text.strip() for p in all_paragraphs if len(p.text.strip()) > 30])
        
        # Ä°Ã§eriÄŸi temizle ve sÄ±nÄ±rla
        content = ' '.join(content.split())  # Ã‡oklu boÅŸluklarÄ± temizle
        content = content[:2000]  # Ä°Ã§eriÄŸi sÄ±nÄ±rla
        
        print(f"âœ… Fallback ile URL iÃ§eriÄŸi Ã§ekildi: {len(content)} karakter")
        
        return {
            "title": title or "BaÅŸlÄ±k bulunamadÄ±",
            "content": content,
            "url": url,
            "source": "fallback"
        }
        
    except Exception as e:
        print(f"âŒ Fallback URL Ã§ekme hatasÄ± ({url}): {e}")
        return {
            "title": "Ä°Ã§erik Ã§ekilemedi",
            "content": f"URL: {url} - Ä°Ã§erik Ã§ekilemedi: {str(e)}",
            "url": url,
            "source": "error"
        }

# =============================================================================
# GMAIL E-POSTA BÄ°LDÄ°RÄ°M SÄ°STEMÄ°
# =============================================================================

def send_gmail_notification(message, tweet_url="", article_title=""):
    """Gmail SMTP ile e-posta bildirimi gÃ¶nder"""
    try:
        import smtplib
        from email.mime.text import MIMEText
        from email.mime.multipart import MIMEMultipart
        from datetime import datetime
        import os
        
        # Gmail SMTP ayarlarÄ±
        gmail_email = os.getenv("GMAIL_EMAIL", "").strip()
        gmail_password = os.getenv("GMAIL_APP_PASSWORD", "").strip()
        
        # AyarlarÄ± kontrol et
        settings = load_automation_settings()
        email_notifications = settings.get("email_notifications", True)
        
        if not email_notifications:
            print("[DEBUG] E-posta bildirimleri kapalÄ±")
            return {"success": False, "reason": "disabled"}
        
        if not gmail_email:
            print("[WARNING] Gmail e-posta adresi eksik. .env dosyasÄ±nda GMAIL_EMAIL ayarlayÄ±n.")
            return {"success": False, "reason": "missing_email"}
            
        if not gmail_password:
            safe_log("Gmail uygulama ÅŸifresi eksik. .env dosyasÄ±nda GMAIL_APP_PASSWORD ayarlayÄ±n.", "WARNING")
            return {"success": False, "reason": "missing_password"}
        
        # E-posta iÃ§eriÄŸini hazÄ±rla
        subject = "ğŸ¤– AI Tweet Bot - Yeni Tweet PaylaÅŸÄ±ldÄ±!"
        
        # HTML e-posta iÃ§eriÄŸi
        html_content = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <meta charset="UTF-8">
            <style>
                body {{ font-family: Arial, sans-serif; line-height: 1.6; color: #333; }}
                .container {{ max-width: 600px; margin: 0 auto; padding: 20px; }}
                .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 10px 10px 0 0; text-align: center; }}
                .content {{ background: #f9f9f9; padding: 20px; border-radius: 0 0 10px 10px; }}
                .tweet-box {{ background: white; border-left: 4px solid #1da1f2; padding: 15px; margin: 15px 0; border-radius: 5px; }}
                .article-box {{ background: white; border-left: 4px solid #28a745; padding: 15px; margin: 15px 0; border-radius: 5px; }}
                .footer {{ text-align: center; margin-top: 20px; color: #666; font-size: 12px; }}
                .btn {{ display: inline-block; background: #1da1f2; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px; margin: 10px 5px; }}
                .stats {{ background: #e3f2fd; padding: 10px; border-radius: 5px; margin: 10px 0; }}
            </style>
        </head>
        <body>
            <div class="container">
                <div class="header">
                    <h1>ğŸ¤– AI Tweet Bot</h1>
                    <p>Yeni Tweet BaÅŸarÄ±yla PaylaÅŸÄ±ldÄ±!</p>
                </div>
                
                <div class="content">
                    <div class="tweet-box">
                        <h3>ğŸ’¬ PaylaÅŸÄ±lan Tweet:</h3>
                        <p><strong>{message}</strong></p>
                        {f'<a href="{tweet_url}" class="btn">ğŸ”— Tweet&apos;i GÃ¶rÃ¼ntÃ¼le</a>' if tweet_url else ''}
                    </div>
                    
                    {f'''
                    <div class="article-box">
                        <h3>ğŸ“° Kaynak Makale:</h3>
                        <p><strong>{article_title}</strong></p>
                    </div>
                    ''' if article_title else ''}
                    
                    <div class="stats">
                        <h3>ğŸ“Š Sistem Bilgileri:</h3>
                        <p><strong>â° Zaman:</strong> {datetime.now().strftime('%d.%m.%Y %H:%M:%S')}</p>
                        <p><strong>ğŸ¤– Bot:</strong> AI Tweet Bot v2.0</p>
                        <p><strong>ğŸ”„ Durum:</strong> Otomatik paylaÅŸÄ±m aktif</p>
                    </div>
                </div>
                
                <div class="footer">
                    <p>Bu e-posta AI Tweet Bot tarafÄ±ndan otomatik olarak gÃ¶nderilmiÅŸtir.</p>
                    <p>Bildirimleri kapatmak iÃ§in uygulama ayarlarÄ±ndan e-posta bildirimlerini devre dÄ±ÅŸÄ± bÄ±rakabilirsiniz.</p>
                </div>
            </div>
        </body>
        </html>
        """
        
        # Metin versiyonu (fallback)
        text_content = f"""
ğŸ¤– AI Tweet Bot - Yeni Tweet PaylaÅŸÄ±ldÄ±!

ğŸ’¬ Tweet: {message}

{f'ğŸ“° Makale: {article_title}' if article_title else ''}

{f'ğŸ”— Tweet Linki: {tweet_url}' if tweet_url else ''}

â° Zaman: {datetime.now().strftime('%d.%m.%Y %H:%M:%S')}

Bu e-posta AI Tweet Bot tarafÄ±ndan otomatik olarak gÃ¶nderilmiÅŸtir.
        """
        
        # E-posta mesajÄ±nÄ± oluÅŸtur
        msg = MIMEMultipart('alternative')
        msg['Subject'] = subject
        msg['From'] = gmail_email
        msg['To'] = gmail_email  # Kendine gÃ¶nder
        
        # Metin ve HTML parÃ§alarÄ±nÄ± ekle
        text_part = MIMEText(text_content, 'plain', 'utf-8')
        html_part = MIMEText(html_content, 'html', 'utf-8')
        
        msg.attach(text_part)
        msg.attach(html_part)
        
        # Gmail SMTP ile gÃ¶nder
        with smtplib.SMTP('smtp.gmail.com', 587) as server:
            server.starttls()
            server.login(gmail_email, gmail_password)
            server.send_message(msg)
        
        print(f"[SUCCESS] Gmail bildirimi gÃ¶nderildi: {gmail_email}")
        return {"success": True, "email": gmail_email}
        
    except Exception as e:
        print(f"[ERROR] Gmail bildirim hatasÄ±: {e}")
        return {"success": False, "error": str(e)}

def test_gmail_connection():
    """Gmail SMTP baÄŸlantÄ±sÄ±nÄ± test et"""
    try:
        import smtplib
        from email.mime.text import MIMEText
        from datetime import datetime
        import os
        
        gmail_email = os.getenv("GMAIL_EMAIL", "").strip()
        gmail_password = os.getenv("GMAIL_APP_PASSWORD", "").strip()
        
        if not gmail_email:
            return {
                "success": False, 
                "error": "Gmail e-posta adresi eksik. .env dosyasÄ±nda GMAIL_EMAIL ayarlayÄ±n."
            }
            
        if not gmail_password:
            return {
                "success": False, 
                "error": "Gmail uygulama ÅŸifresi eksik. .env dosyasÄ±nda GMAIL_APP_PASSWORD ayarlayÄ±n."
            }
        
        # Test e-postasÄ± gÃ¶nder
        subject = "ğŸ§ª AI Tweet Bot - Test E-postasÄ±"
        body = f"""
ğŸ§ª Test E-postasÄ±

Gmail SMTP baÄŸlantÄ±sÄ± baÅŸarÄ±yla test edildi!

â° Test ZamanÄ±: {datetime.now().strftime('%d.%m.%Y %H:%M:%S')}
ğŸ“§ E-posta: {gmail_email}
ğŸ¤– AI Tweet Bot v2.0

Bu bir test e-postasÄ±dÄ±r.
        """
        
        msg = MIMEText(body, 'plain', 'utf-8')
        msg['Subject'] = subject
        msg['From'] = gmail_email
        msg['To'] = gmail_email
        
        with smtplib.SMTP('smtp.gmail.com', 587) as server:
            server.starttls()
            server.login(gmail_email, gmail_password)
            server.send_message(msg)
        
        return {
            "success": True, 
            "message": f"âœ… Test e-postasÄ± baÅŸarÄ±yla gÃ¶nderildi: {gmail_email}"
        }
        
    except Exception as e:
        return {"success": False, "error": str(e)}

def check_gmail_configuration():
    """Gmail konfigÃ¼rasyonunu kontrol et"""
    try:
        import os
        
        gmail_email = os.getenv("GMAIL_EMAIL", "").strip()
        gmail_password = os.getenv("GMAIL_APP_PASSWORD", "").strip()
        
        # AyarlarÄ± kontrol et
        settings = load_automation_settings()
        email_notifications = settings.get("email_notifications", True)
        
        status = {
            "email_set": bool(gmail_email and "@" in gmail_email),
            "password_set": bool(gmail_password),
            "notifications_enabled": email_notifications,
            "ready": False
        }
        
        if status["email_set"] and status["password_set"] and status["notifications_enabled"]:
            status["ready"] = True
            status["message"] = "âœ… Gmail yapÄ±landÄ±rmasÄ± tamamlanmÄ±ÅŸ ve aktif"
            status["status"] = "ready"
        elif status["email_set"] and status["password_set"]:
            status["message"] = "âš ï¸ Gmail yapÄ±landÄ±rÄ±lmÄ±ÅŸ ama bildirimler kapalÄ±"
            status["status"] = "disabled"
        elif status["email_set"]:
            status["message"] = "âš ï¸ E-posta var, uygulama ÅŸifresi eksik"
            status["status"] = "partial"
        else:
            status["message"] = "âŒ Gmail yapÄ±landÄ±rmasÄ± eksik"
            status["status"] = "missing"
            
        return status
        
    except Exception as e:
        return {
            "email_set": False,
            "password_set": False,
            "notifications_enabled": False,
            "ready": False,
            "message": f"âŒ Kontrol hatasÄ±: {e}",
            "status": "error"
        }

# ==========================================
# Ã–ZEL HABER KAYNAKLARI YÃ–NETÄ°MÄ°
# ==========================================

NEWS_SOURCES_FILE = "news_sources.json"

def load_news_sources():
    """Haber kaynaklarÄ±nÄ± yÃ¼kle"""
    try:
        if os.path.exists(NEWS_SOURCES_FILE):
            return load_json(NEWS_SOURCES_FILE)
        else:
            # VarsayÄ±lan konfigÃ¼rasyon
            default_config = {
                "sources": [
                    {
                        "id": "techcrunch_ai",
                        "name": "TechCrunch AI",
                        "url": "https://techcrunch.com/category/artificial-intelligence/",
                        "description": "TechCrunch Yapay Zeka haberleri",
                        "enabled": True,
                        "selector_type": "rss_like",
                        "article_selectors": {
                            "container": "article.post-block",
                            "title": "h2.post-block__title a",
                            "link": "h2.post-block__title a",
                            "date": "time.river-byline__time",
                            "excerpt": ".post-block__content"
                        },
                        "added_date": datetime.now().isoformat(),
                        "last_checked": None,
                        "article_count": 0,
                        "success_rate": 100
                    }
                ],
                "settings": {
                    "max_sources": 10,
                    "check_all_sources": True,
                    "source_timeout": 30,
                    "articles_per_source": 5,
                    "last_updated": datetime.now().isoformat()
                }
            }
            save_json(NEWS_SOURCES_FILE, default_config)
            return default_config
    except Exception as e:
        print(f"Haber kaynaklarÄ± yÃ¼kleme hatasÄ±: {e}")
        return {"sources": [], "settings": {}}

def save_news_sources(config):
    """Haber kaynaklarÄ±nÄ± kaydet"""
    try:
        config["settings"]["last_updated"] = datetime.now().isoformat()
        save_json(NEWS_SOURCES_FILE, config)
        return {"success": True, "message": "âœ… Haber kaynaklarÄ± kaydedildi"}
    except Exception as e:
        return {"success": False, "message": f"âŒ Kaydetme hatasÄ±: {e}"}

def test_selectors_for_url(url):
    """URL iÃ§in selector'larÄ± test et ve otomatik tespit et - GeliÅŸmiÅŸ sistem"""
    try:
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        
        print(f"ğŸ§ª URL test ediliyor: {url}")
        
        # GeliÅŸmiÅŸ scraper ile sayfayÄ± Ã§ek
        scrape_result = advanced_web_scraper(url, wait_time=3, use_js=True)
        
        if not scrape_result.get("success"):
            # Fallback: Basit HTTP request
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            scrape_method = "simple_request"
        else:
            soup = BeautifulSoup(scrape_result.get("html", ""), 'html.parser')
            scrape_method = scrape_result.get("method", "advanced")
        
        print(f"âœ… Sayfa Ã§ekildi ({scrape_method})")
        
        # Otomatik selector tespiti
        print("ğŸ” Otomatik selector tespiti baÅŸlatÄ±lÄ±yor...")
        selectors, container_count = auto_detect_selectors(soup, url)
        
        # Tespit edilen selector'larÄ± doÄŸrula
        print("âœ… Selector'lar doÄŸrulanÄ±yor...")
        is_valid, validation_msg = validate_selectors(soup, selectors)
        
        if is_valid:
            # Ã–rnek makaleleri Ã§ek
            print("ğŸ“° Ã–rnek makaleler Ã§ekiliyor...")
            containers = soup.select(selectors["container"])
            sample_articles = []
            
            for i, container in enumerate(containers[:5]):  # Ä°lk 5 makale
                try:
                    print(f"ğŸ” Makale {i+1} iÅŸleniyor...")
                    
                    # BaÅŸlÄ±k ve link Ã§ek
                    title_elem = container.select_one(selectors["title"])
                    link_elem = container.select_one(selectors["link"])
                    
                    if title_elem and link_elem:
                        title = title_elem.get_text(strip=True)
                        link = link_elem.get('href', '')
                        
                        # Relative URL'leri absolute yap
                        if link.startswith('/'):
                            from urllib.parse import urljoin
                            link = urljoin(url, link)
                        
                        # Tarih Ã§ek (opsiyonel)
                        date_text = ""
                        if selectors.get("date"):
                            date_elem = container.select_one(selectors["date"])
                            if date_elem:
                                date_text = date_elem.get_text(strip=True)
                        
                        # Ã–zet Ã§ek (opsiyonel)
                        excerpt_text = ""
                        if selectors.get("excerpt"):
                            excerpt_elem = container.select_one(selectors["excerpt"])
                            if excerpt_elem:
                                excerpt_text = excerpt_elem.get_text(strip=True)[:200]
                        
                        if title and link and len(title) > 10:
                            sample_articles.append({
                                "title": title,
                                "url": link,
                                "date": date_text,
                                "excerpt": excerpt_text
                            })
                            print(f"âœ… Makale {i+1}: {title[:50]}...")
                        
                except Exception as article_error:
                    print(f"âš ï¸ Makale {i+1} hatasÄ±: {article_error}")
                    continue
            
            # Site tÃ¼rÃ¼ tespiti
            site_info = detect_site_type(url, soup)
            
            # BaÅŸarÄ± mesajÄ±
            success_msg = f"âœ… {container_count} konteyner bulundu, {len(sample_articles)} Ã¶rnek makale Ã§ekildi"
            
            return {
                "success": True,
                "message": success_msg,
                "selectors": selectors,
                "container_count": container_count,
                "sample_articles": sample_articles,
                "validation_message": validation_msg,
                "scrape_method": scrape_method,
                "site_info": site_info,
                "test_details": {
                    "total_containers": container_count,
                    "successful_articles": len(sample_articles),
                    "selector_quality": "high" if len(sample_articles) >= 3 else "medium" if len(sample_articles) >= 1 else "low"
                }
            }
        else:
            return {
                "success": False,
                "message": f"âŒ Selector tespiti baÅŸarÄ±sÄ±z: {validation_msg}",
                "selectors": selectors,
                "container_count": container_count,
                "scrape_method": scrape_method,
                "test_details": {
                    "error": validation_msg,
                    "total_containers": container_count
                }
            }
            
    except Exception as e:
        print(f"âŒ URL test hatasÄ±: {e}")
        return {
            "success": False,
            "message": f"âŒ URL test hatasÄ±: {str(e)}",
            "test_details": {
                "error": str(e)
            }
        }

def detect_site_type(url, soup):
    """Site tÃ¼rÃ¼nÃ¼ ve CMS'ini tespit et"""
    try:
        site_info = {
            "cms": "unknown",
            "type": "unknown",
            "features": []
        }
        
        # WordPress tespiti
        wp_indicators = [
            'wp-content', 'wp-includes', 'wp-json',
            'class*="wp-"', 'id*="wp-"'
        ]
        
        for indicator in wp_indicators:
            if indicator in str(soup).lower():
                site_info["cms"] = "wordpress"
                site_info["features"].append("WordPress")
                break
        
        # Site tÃ¼rÃ¼ tespiti
        if any(domain in url.lower() for domain in ['techcrunch', 'theverge', 'wired', 'arstechnica']):
            site_info["type"] = "tech_news"
            site_info["features"].append("Tech News Site")
        elif any(keyword in str(soup).lower() for keyword in ['news', 'article', 'story', 'journalism']):
            site_info["type"] = "news"
            site_info["features"].append("News Site")
        elif any(keyword in str(soup).lower() for keyword in ['blog', 'post', 'author']):
            site_info["type"] = "blog"
            site_info["features"].append("Blog")
        
        # JavaScript framework tespiti
        if 'react' in str(soup).lower():
            site_info["features"].append("React")
        if 'vue' in str(soup).lower():
            site_info["features"].append("Vue.js")
        if 'angular' in str(soup).lower():
            site_info["features"].append("Angular")
        
        return site_info
        
    except Exception as e:
        return {"cms": "unknown", "type": "unknown", "features": [], "error": str(e)}

def test_manual_selectors_for_url(url, selectors):
    """Manuel selector'larÄ± test et"""
    try:
        safe_log(f"Manuel selector test ediliyor: {url}", "INFO")
        
        # Selector'larÄ± temizle
        article_container = selectors.get('article_container', '').strip()
        title_selector = selectors.get('title_selector', '').strip()
        link_selector = selectors.get('link_selector', '').strip()
        date_selector = selectors.get('date_selector', '').strip()
        summary_selector = selectors.get('summary_selector', '').strip()
        base_url = selectors.get('base_url', '').strip()
        
        if not base_url:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            base_url = f"{parsed.scheme}://{parsed.netloc}"
        
        # SayfayÄ± Ã§ek
        try:
            response = requests.get(url, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }, timeout=15)
            
            if response.status_code != 200:
                return {
                    'success': False,
                    'error': f'Sayfa yÃ¼klenemedi (HTTP {response.status_code})',
                    'details': {
                        'status_code': response.status_code,
                        'url': url
                    }
                }
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
        except Exception as e:
            return {
                'success': False,
                'error': f'Sayfa Ã§ekme hatasÄ±: {str(e)}',
                'details': {
                    'error_type': type(e).__name__,
                    'url': url
                }
            }
        
        # Makale konteynerlerini bul
        try:
            containers = soup.select(article_container)
            
            if not containers:
                return {
                    'success': False,
                    'error': f'Makale konteyneri bulunamadÄ±',
                    'details': {
                        'article_container': article_container,
                        'page_title': soup.title.get_text() if soup.title else 'BaÅŸlÄ±k yok',
                        'total_elements': len(soup.find_all())
                    }
                }
            
            safe_log(f"Bulunan konteyner sayÄ±sÄ±: {len(containers)}", "INFO")
            
        except Exception as e:
            return {
                'success': False,
                'error': f'Konteyner selector hatasÄ±: {str(e)}',
                'details': {
                    'article_container': article_container,
                    'error_type': type(e).__name__
                }
            }
        
        # Her konteynerden makale bilgilerini Ã§ek
        articles = []
        test_details = {
            'container_count': len(containers),
            'successful_extractions': 0,
            'failed_extractions': 0
        }
        
        for i, container in enumerate(containers[:5]):  # Ä°lk 5 konteyner
            try:
                article_data = {}
                extraction_success = True
                
                # BaÅŸlÄ±k Ã§ek
                try:
                    title_elem = container.select_one(title_selector)
                    if title_elem:
                        article_data['title'] = title_elem.get_text(strip=True)
                    else:
                        article_data['title'] = None
                        extraction_success = False
                except Exception as e:
                    article_data['title'] = None
                    extraction_success = False
                    safe_log(f"BaÅŸlÄ±k Ã§ekme hatasÄ± (konteyner {i+1}): {e}", "WARNING")
                
                # Link Ã§ek
                try:
                    link_elem = container.select_one(link_selector)
                    if link_elem:
                        href = link_elem.get('href', '')
                        if href:
                            # Relative link kontrolÃ¼
                            if href.startswith('/'):
                                from urllib.parse import urljoin
                                href = urljoin(base_url, href)
                            elif not href.startswith(('http://', 'https://')):
                                href = base_url + '/' + href.lstrip('/')
                            article_data['url'] = href
                        else:
                            article_data['url'] = None
                            extraction_success = False
                    else:
                        article_data['url'] = None
                        extraction_success = False
                except Exception as e:
                    article_data['url'] = None
                    extraction_success = False
                    safe_log(f"Link Ã§ekme hatasÄ± (konteyner {i+1}): {e}", "WARNING")
                
                # Tarih Ã§ek (opsiyonel)
                if date_selector:
                    try:
                        date_elem = container.select_one(date_selector)
                        if date_elem:
                            article_data['date'] = date_elem.get_text(strip=True)
                        else:
                            article_data['date'] = None
                    except Exception as e:
                        article_data['date'] = None
                        safe_log(f"Tarih Ã§ekme hatasÄ± (konteyner {i+1}): {e}", "WARNING")
                
                # Ã–zet Ã§ek (opsiyonel)
                if summary_selector:
                    try:
                        summary_elem = container.select_one(summary_selector)
                        if summary_elem:
                            summary_text = summary_elem.get_text(strip=True)
                            article_data['summary'] = summary_text[:200] + '...' if len(summary_text) > 200 else summary_text
                        else:
                            article_data['summary'] = None
                    except Exception as e:
                        article_data['summary'] = None
                        safe_log(f"Ã–zet Ã§ekme hatasÄ± (konteyner {i+1}): {e}", "WARNING")
                
                # BaÅŸarÄ± kontrolÃ¼
                if extraction_success and article_data.get('title') and article_data.get('url'):
                    articles.append(article_data)
                    test_details['successful_extractions'] += 1
                else:
                    test_details['failed_extractions'] += 1
                    safe_log(f"Konteyner {i+1} baÅŸarÄ±sÄ±z: title={bool(article_data.get('title'))}, url={bool(article_data.get('url'))}", "WARNING")
                
            except Exception as e:
                test_details['failed_extractions'] += 1
                safe_log(f"Konteyner {i+1} iÅŸleme hatasÄ±: {e}", "ERROR")
                continue
        
        # SonuÃ§ deÄŸerlendirmesi
        if not articles:
            return {
                'success': False,
                'error': 'HiÃ§bir makale Ã§ekilemedi',
                'details': {
                    **test_details,
                    'selectors_used': {
                        'article_container': article_container,
                        'title_selector': title_selector,
                        'link_selector': link_selector,
                        'date_selector': date_selector,
                        'summary_selector': summary_selector
                    }
                }
            }
        
        success_rate = (test_details['successful_extractions'] / test_details['container_count']) * 100
        
        return {
            'success': True,
            'message': f'Manuel selector test baÅŸarÄ±lÄ±',
            'article_count': len(articles),
            'articles': articles,
            'test_details': test_details,
            'success_rate': round(success_rate, 1),
            'selectors_used': {
                'article_container': article_container,
                'title_selector': title_selector,
                'link_selector': link_selector,
                'date_selector': date_selector,
                'summary_selector': summary_selector,
                'base_url': base_url
            }
        }
        
    except Exception as e:
        safe_log(f"Manuel selector test hatasÄ±: {e}", "ERROR")
        return {
            'success': False,
            'error': f'Test hatasÄ±: {str(e)}',
            'details': {
                'error_type': type(e).__name__,
                'url': url
            }
        }

def add_news_source_with_validation(name, url, description="", auto_detect=True, manual_selectors=None):
    """DoÄŸrulama ile yeni haber kaynaÄŸÄ± ekle"""
    try:
        # Konsol log iÃ§in import (terminal kaldÄ±rÄ±ldÄ±)
        try:
            from app import terminal_log
        except ImportError:
            # EÄŸer app.py'den import edilemezse normal print kullan
            def terminal_log(msg, level='info'):
                import time
                timestamp = time.strftime('%H:%M:%S')
                print(f"[{timestamp}] [{level.upper()}] {msg}")
        
        terminal_log(f"ğŸ” Kaynak ekleme baÅŸlatÄ±ldÄ± - Name: {name}, URL: {url}, Auto: {auto_detect}", "debug")
        
        config = load_news_sources()
        
        # URL'yi temizle ve doÄŸrula
        url = url.strip()
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        
        # AynÄ± URL var mÄ± kontrol et
        for source in config["sources"]:
            if source["url"] == url:
                return {
                    "success": False,
                    "message": f"âŒ Bu URL zaten mevcut: {source['name']}"
                }
        
        # Maksimum kaynak sayÄ±sÄ±nÄ± kontrol et
        max_sources = config["settings"].get("max_sources", 10)
        if len(config["sources"]) >= max_sources:
            return {"success": False, "message": f"âŒ Maksimum {max_sources} kaynak eklenebilir"}
        
        # URL'yi test et
        if auto_detect:
            terminal_log(f"ğŸ” {name} kaynaÄŸÄ± otomatik tespit ile test ediliyor...", "info")
            test_result = test_selectors_for_url(url)
            
            if not test_result['success']:
                terminal_log(f"âŒ {name} otomatik tespit baÅŸarÄ±sÄ±z: {test_result['message']}", "error")
                return {
                    "success": False,
                    "message": f"âŒ Otomatik tespit baÅŸarÄ±sÄ±z: {test_result['message']}",
                    "test_details": test_result
                }
            
            selectors = test_result['selectors']
            selector_type = "auto_detected"
            terminal_log(f"âœ… {name}: {test_result['container_count']} konteyner bulundu", "success")
            
        else:
            # Manuel selector'larÄ± test et
            if not manual_selectors:
                terminal_log(f"âŒ {name} manuel mod iÃ§in selector'lar eksik", "error")
                return {
                    "success": False,
                    "message": "âŒ Manuel mod iÃ§in selector'lar gerekli"
                }
            
            terminal_log(f"ğŸ” {name} kaynaÄŸÄ± manuel selector'larla test ediliyor...", "info")
            test_result = test_manual_selectors_for_url(url, manual_selectors)
            
            if not test_result['success']:
                terminal_log(f"âŒ {name} manuel selector test baÅŸarÄ±sÄ±z: {test_result.get('error', 'Bilinmeyen hata')}", "error")
                return {
                    "success": False,
                    "message": f"âŒ Manuel selector test baÅŸarÄ±sÄ±z: {test_result.get('error', 'Bilinmeyen hata')}",
                    "test_details": test_result
                }
            
            # Manuel selector'larÄ± uygun formata Ã§evir
            selectors = {
                "container": manual_selectors['article_container'],
                "title": manual_selectors['title_selector'],
                "link": manual_selectors['link_selector'],
                "date": manual_selectors.get('date_selector', ''),
                "excerpt": manual_selectors.get('summary_selector', '')
            }
            selector_type = "manual"
            terminal_log(f"âœ… {name}: {test_result['article_count']} makale bulundu", "success")
        
        # Benzersiz ID oluÅŸtur
        import random
        source_id = f"custom_{len(config['sources']) + 1}_{random.randint(1000000000, 9999999999)}"
        
        # Yeni kaynak oluÅŸtur
        new_source = {
            "id": source_id,
            "name": name.strip(),
            "url": url,
            "description": description.strip(),
            "enabled": True,
            "selector_type": selector_type,
            "article_selectors": selectors,
            "added_date": datetime.now().isoformat(),
            "last_checked": datetime.now().isoformat(),
            "article_count": 0,
            "success_rate": 0
        }
        
        # KaynaÄŸÄ± ekle
        config["sources"].append(new_source)
        config["settings"]["last_updated"] = datetime.now().isoformat()
        
        # Kaydet
        result = save_news_sources(config)
        
        if result["success"]:
            terminal_log(f"âœ… '{name}' kaynaÄŸÄ± baÅŸarÄ±yla eklendi", "success")
            response = {
                "success": True,
                "message": f"âœ… '{name}' kaynaÄŸÄ± baÅŸarÄ±yla eklendi",
                "source": new_source
            }
            
            # Test sonuÃ§larÄ±nÄ± da ekle
            if auto_detect and 'test_result' in locals():
                response['test_details'] = test_result
            
            return response
        else:
            terminal_log(f"âŒ '{name}' kaynaÄŸÄ± kaydedilemedi: {result.get('message', 'Bilinmeyen hata')}", "error")
            return result
        
    except Exception as e:
        terminal_log(f"âŒ Kaynak ekleme hatasÄ±: {str(e)}", "error")
        return {
            "success": False,
            "message": f"âŒ Kaynak ekleme hatasÄ±: {str(e)}"
        }

def add_news_source(name, url, description=""):
    """Yeni haber kaynaÄŸÄ± ekle (otomatik doÄŸrulama ile)"""
    return add_news_source_with_validation(name, url, description, auto_detect=True)

def remove_news_source(source_id):
    """Haber kaynaÄŸÄ±nÄ± kaldÄ±r"""
    try:
        config = load_news_sources()
        
        # KaynaÄŸÄ± bul ve kaldÄ±r
        original_count = len(config["sources"])
        config["sources"] = [s for s in config["sources"] if s["id"] != source_id]
        
        if len(config["sources"]) == original_count:
            return {"success": False, "message": "âŒ Kaynak bulunamadÄ±"}
        
        result = save_news_sources(config)
        if result["success"]:
            return {"success": True, "message": "âœ… Kaynak baÅŸarÄ±yla kaldÄ±rÄ±ldÄ±"}
        else:
            return result
            
    except Exception as e:
        return {"success": False, "message": f"âŒ Kaynak kaldÄ±rma hatasÄ±: {e}"}

def toggle_news_source(source_id, enabled=None):
    """Haber kaynaÄŸÄ±nÄ± aktif/pasif yap - RSS dahil"""
    try:
        config = load_news_sources()
        
        # Normal kaynaklarda ara
        for source in config["sources"]:
            if source["id"] == source_id:
                if enabled is None:
                    source["enabled"] = not source["enabled"]
                else:
                    source["enabled"] = bool(enabled)
                
                result = save_news_sources(config)
                if result["success"]:
                    status = "aktif" if source["enabled"] else "pasif"
                    return {"success": True, "message": f"âœ… '{source['name']}' kaynaÄŸÄ± {status} yapÄ±ldÄ±"}
                else:
                    return result
        
        # RSS kaynaklarÄ±nda ara
        for rss_source in config.get("rss_sources", []):
            if rss_source["id"] == source_id:
                if enabled is None:
                    rss_source["enabled"] = not rss_source["enabled"]
                else:
                    rss_source["enabled"] = bool(enabled)
                
                result = save_news_sources(config)
                if result["success"]:
                    status = "aktif" if rss_source["enabled"] else "pasif"
                    return {"success": True, "message": f"âœ… '{rss_source['name']}' RSS kaynaÄŸÄ± {status} yapÄ±ldÄ±"}
                else:
                    return result
        
        return {"success": False, "message": "âŒ Kaynak bulunamadÄ±"}
        
    except Exception as e:
        return {"success": False, "message": f"âŒ Durum deÄŸiÅŸtirme hatasÄ±: {e}"}

def remove_rss_source(source_id):
    """RSS kaynaÄŸÄ±nÄ± kaldÄ±r"""
    try:
        config = load_news_sources()
        
        # RSS kaynaÄŸÄ±nÄ± bul ve kaldÄ±r
        original_count = len(config.get("rss_sources", []))
        config["rss_sources"] = [s for s in config.get("rss_sources", []) if s["id"] != source_id]
        
        if len(config.get("rss_sources", [])) == original_count:
            return {"success": False, "message": "âŒ RSS kaynaÄŸÄ± bulunamadÄ±"}
        
        result = save_news_sources(config)
        if result["success"]:
            return {"success": True, "message": "âœ… RSS kaynaÄŸÄ± baÅŸarÄ±yla kaldÄ±rÄ±ldÄ±"}
        else:
            return result
            
    except Exception as e:
        return {"success": False, "message": f"âŒ RSS kaynaÄŸÄ± kaldÄ±rma hatasÄ±: {e}"}

def add_rss_source(name, url, description=""):
    """Yeni RSS kaynaÄŸÄ± ekle"""
    try:
        config = load_news_sources()
        
        # RSS kaynaklarÄ± listesi yoksa oluÅŸtur
        if "rss_sources" not in config:
            config["rss_sources"] = []
        
        # URL'yi temizle ve doÄŸrula
        url = url.strip()
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        
        # AynÄ± URL var mÄ± kontrol et
        for rss_source in config["rss_sources"]:
            if rss_source["url"] == url:
                return {
                    "success": False,
                    "message": f"âŒ Bu RSS URL'si zaten mevcut: {rss_source['name']}"
                }
        
        # Benzersiz ID oluÅŸtur
        import random
        source_id = f"rss_{len(config['rss_sources']) + 1}_{random.randint(1000000000, 9999999999)}"
        
        # Yeni RSS kaynaÄŸÄ± oluÅŸtur
        new_rss_source = {
            "id": source_id,
            "name": name.strip(),
            "url": url,
            "description": description.strip(),
            "enabled": True,
            "type": "rss",
            "added_date": datetime.now().isoformat(),
            "last_checked": None,
            "article_count": 0,
            "success_rate": 100
        }
        
        # RSS kaynaÄŸÄ±nÄ± ekle
        config["rss_sources"].append(new_rss_source)
        config["settings"]["last_updated"] = datetime.now().isoformat()
        
        # Kaydet
        result = save_news_sources(config)
        
        if result["success"]:
            return {
                "success": True,
                "message": f"âœ… '{name}' RSS kaynaÄŸÄ± baÅŸarÄ±yla eklendi",
                "source": new_rss_source
            }
        else:
            return result
        
    except Exception as e:
        return {
            "success": False,
            "message": f"âŒ RSS kaynaÄŸÄ± ekleme hatasÄ±: {str(e)}"
        }

def fetch_articles_from_custom_sources():
    """Ã–zel haber kaynaklarÄ±ndan makale Ã§ek"""
    try:
        config = load_news_sources()
        all_articles = []
        
        enabled_sources = [s for s in config["sources"] if s.get("enabled", True)]
        
        if not enabled_sources:
            print("âš ï¸ Aktif haber kaynaÄŸÄ± bulunamadÄ±")
            return []
        
        # Konsol log iÃ§in import (terminal kaldÄ±rÄ±ldÄ±)
        try:
            from app import terminal_log
        except ImportError:
            # EÄŸer app.py'den import edilemezse normal print kullan
            def terminal_log(msg, level='info'):
                import time
                timestamp = time.strftime('%H:%M:%S')
                print(f"[{timestamp}] [{level.upper()}] {msg}")
        
        terminal_log(f"ğŸ” {len(enabled_sources)} haber kaynaÄŸÄ±ndan makale Ã§ekiliyor...", "info")
        
        for source in enabled_sources:
            try:
                terminal_log(f"ğŸ“° {source['name']} kaynaÄŸÄ± kontrol ediliyor...", "info")
                
                # Kaynak URL'sini Ã§ek
                articles = fetch_articles_from_single_source(source)
                
                if articles:
                    all_articles.extend(articles)
                    source["article_count"] = len(articles)
                    source["success_rate"] = min(100, source.get("success_rate", 0) + 10)
                    terminal_log(f"âœ… {source['name']}: {len(articles)} makale bulundu", "success")
                else:
                    source["success_rate"] = max(0, source.get("success_rate", 100) - 20)
                    terminal_log(f"âš ï¸ {source['name']}: Makale bulunamadÄ±", "warning")
                
                source["last_checked"] = datetime.now().isoformat()
                
            except Exception as e:
                terminal_log(f"âŒ {source['name']} hatasÄ±: {e}", "error")
                source["success_rate"] = max(0, source.get("success_rate", 100) - 30)
                source["last_checked"] = datetime.now().isoformat()
        
        # GÃ¼ncellenmiÅŸ istatistikleri kaydet
        save_news_sources(config)
        
        terminal_log(f"ğŸ“Š Toplam {len(all_articles)} makale Ã§ekildi", "info")
        
        # Duplikat filtreleme uygula
        if all_articles:
            filtered_articles = filter_duplicate_articles(all_articles)
            terminal_log(f"âœ… Duplikat filtreleme sonrasÄ± {len(filtered_articles)} benzersiz makale", "success")
            return filtered_articles
        
        return all_articles
        
    except Exception as e:
        print(f"âŒ Ã–zel kaynaklardan makale Ã§ekme hatasÄ±: {e}")
        return []

def auto_detect_selectors(soup, url):
    """Sayfadan otomatik olarak en iyi selector'larÄ± tespit et - GeliÅŸmiÅŸ AI destekli"""
    
    print(f"ğŸ” Otomatik selector tespiti baÅŸlatÄ±lÄ±yor: {url}")
    
    # Site tÃ¼rÃ¼ne gÃ¶re Ã¶zel pattern'ler
    site_patterns = {
        'techcrunch.com': {
            'containers': [".loop-card", "li.wp-block-post", "article.wp-block-post"],
            'titles': ["h3", "h2", "h3 a", "h2 a", ".loop-card__title a"],
            'dates': ["time", "[datetime]", ".loop-card__time"]
        },
        'theverge.com': {
            'containers': [
                "article[data-testid='story-card']", 
                ".duet--content-cards--content-card",
                ".c-entry-box", 
                ".c-compact-river__entry",
                "[data-testid*='story']",
                "[data-testid*='card']",
                ".group-hover",
                "article"
            ],
            'titles': [
                "h2 a", 
                "h3 a", 
                ".c-entry-box__title a", 
                "[data-testid*='headline'] a",
                ".font-polysans a",
                "a[href*='/2025/']",
                "a[href*='/2024/']"
            ],
            'dates': [
                ".c-byline__item time", 
                "time", 
                "[datetime]",
                ".text-gray-31"
            ]
        },
        'wired.com': {
            'containers': ["article", ".archive-item-component", ".card-component"],
            'titles': ["h3 a", "h2 a", ".card-component__title a"],
            'dates': ["time", ".publish-date"]
        }
    }
    
    # URL'den site tÃ¼rÃ¼nÃ¼ tespit et
    site_type = None
    for domain in site_patterns:
        if domain in url.lower():
            site_type = domain
            break
    
    # YaygÄ±n makale konteyner pattern'leri (Ã¶ncelik sÄ±rasÄ±na gÃ¶re)
    container_patterns = []
    
    # Site Ã¶zel pattern'leri Ã¶nce ekle
    if site_type and site_type in site_patterns:
        container_patterns.extend(site_patterns[site_type]['containers'])
        print(f"âœ… Site Ã¶zel pattern'ler eklendi: {site_type}")
    
    # Genel pattern'leri ekle
    container_patterns.extend([
        # Modern WordPress
        "li.wp-block-post", "article.wp-block-post", ".loop-card",
        
        # Modern haber siteleri
        "article[data-testid*='story']", "article[data-testid*='card']",
        ".story-card", ".news-item", ".article-card",
        
        # YaygÄ±n CMS pattern'leri
        ".post-item", ".entry-item", ".news-entry",
        ".content-item", ".article-item",
        
        # Genel yapÄ±lar
        "article.post", "article", ".article", ".post",
        ".entry", ".story", ".news",
        
        # Class iÃ§eren genel pattern'ler
        "[class*='article']", "[class*='post']", 
        "[class*='story']", "[class*='news']",
        "[class*='item']", "[class*='card']"
    ])
    
    best_selectors = {
        "container": "article",
        "title": "h2 a",
        "link": "h2 a", 
        "date": "time",
        "excerpt": "p"
    }
    
    max_containers = 0
    best_score = 0
    
    print(f"ğŸ” {len(container_patterns)} farklÄ± pattern test ediliyor...")
    
    # Her pattern'i test et
    for i, pattern in enumerate(container_patterns):
        try:
            containers = soup.select(pattern)
            container_count = len(containers)
            
            # Ä°deal aralÄ±k: 2-100 konteyner
            if 2 <= container_count <= 100:
                
                # Kalite skoru hesapla
                quality_score = calculate_selector_quality(containers, pattern, url)
                
                print(f"ğŸ“Š Pattern {i+1}: {pattern} -> {container_count} konteyner, skor: {quality_score}")
                
                if quality_score > best_score:
                    best_score = quality_score
                    max_containers = container_count
                    best_selectors["container"] = pattern
                    
                    # Bu konteyner iÃ§in en iyi title selector'Ä±nÄ± bul
                    if containers:
                        sample_container = containers[0]
                        
                        # Site Ã¶zel title pattern'leri Ã¶nce dene
                        title_patterns = []
                        if site_type and site_type in site_patterns:
                            title_patterns.extend(site_patterns[site_type]['titles'])
                        
                        # Genel title pattern'leri ekle
                        title_patterns.extend([
                            "h3 a", "h2 a", "h1 a", "h4 a",
                            ".title a", ".headline a", ".entry-title a",
                            "[class*='title'] a", "[class*='headline'] a",
                            "a[href*='article']", "a[href*='post']",
                            "a[href*='/20']",  # Tarih iÃ§eren URL'ler
                            "a"  # Son Ã§are
                        ])
                        
                        for title_pattern in title_patterns:
                            title_elem = sample_container.select_one(title_pattern)
                            if title_elem and title_elem.get_text(strip=True) and len(title_elem.get_text(strip=True)) > 10:
                                best_selectors["title"] = title_pattern
                                best_selectors["link"] = title_pattern
                                print(f"âœ… Title pattern bulundu: {title_pattern}")
                                break
                        
                        # Date selector
                        date_patterns = []
                        if site_type and site_type in site_patterns:
                            date_patterns.extend(site_patterns[site_type]['dates'])
                        
                        date_patterns.extend([
                            "time", ".date", ".published", ".publish-date",
                            "[class*='date']", "[class*='time']",
                            "[datetime]", ".meta-date"
                        ])
                        
                        for date_pattern in date_patterns:
                            if sample_container.select_one(date_pattern):
                                best_selectors["date"] = date_pattern
                                print(f"âœ… Date pattern bulundu: {date_pattern}")
                                break
                        
                        # Excerpt selector
                        excerpt_patterns = [
                            ".excerpt", ".summary", ".description",
                            ".entry-summary", ".post-excerpt",
                            "p", ".content p"
                        ]
                        
                        for excerpt_pattern in excerpt_patterns:
                            excerpt_elem = sample_container.select_one(excerpt_pattern)
                            if excerpt_elem and len(excerpt_elem.get_text(strip=True)) > 20:
                                best_selectors["excerpt"] = excerpt_pattern
                                print(f"âœ… Excerpt pattern bulundu: {excerpt_pattern}")
                                break
                        
        except Exception as pattern_error:
            print(f"âš ï¸ Pattern test hatasÄ± ({pattern}): {pattern_error}")
            continue
    
    print(f"ğŸ En iyi selector bulundu: {best_selectors['container']} ({max_containers} konteyner, skor: {best_score})")
    
    return best_selectors, max_containers

def calculate_selector_quality(containers, pattern, url):
    """Selector kalitesini hesapla"""
    try:
        if not containers:
            return 0
        
        score = 0
        sample_size = min(3, len(containers))  # Ä°lk 3 konteyner test et
        
        for container in containers[:sample_size]:
            # BaÅŸlÄ±k var mÄ±?
            title_found = False
            title_selectors = ["h1", "h2", "h3", "h4", ".title", ".headline", "a"]
            
            for ts in title_selectors:
                title_elem = container.select_one(ts)
                if title_elem and len(title_elem.get_text(strip=True)) > 10:
                    title_found = True
                    score += 10
                    break
            
            # Link var mÄ±?
            link_found = False
            links = container.select("a[href]")
            for link in links:
                href = link.get('href', '')
                if href and (href.startswith('http') or href.startswith('/')):
                    link_found = True
                    score += 10
                    break
            
            # Tarih var mÄ±?
            date_selectors = ["time", ".date", "[datetime]", "[class*='date']"]
            for ds in date_selectors:
                if container.select_one(ds):
                    score += 5
                    break
            
            # Ä°Ã§erik var mÄ±?
            content_length = len(container.get_text(strip=True))
            if content_length > 50:
                score += 5
            if content_length > 200:
                score += 5
        
        # Konteyner sayÄ±sÄ± bonusu (ideal aralÄ±k)
        container_count = len(containers)
        if 5 <= container_count <= 20:
            score += 20
        elif 3 <= container_count <= 30:
            score += 10
        elif container_count > 100:
            score -= 20  # Ã‡ok fazla konteyner ceza
        
        # Pattern spesifik bonuslar
        if 'article' in pattern.lower():
            score += 10
        if 'post' in pattern.lower():
            score += 8
        if 'story' in pattern.lower():
            score += 8
        if 'card' in pattern.lower():
            score += 5
        
        return score / sample_size  # Ortalama skor
        
    except Exception as e:
        print(f"âŒ Kalite hesaplama hatasÄ±: {e}")
        return 0

def validate_selectors(soup, selectors):
    """Selector'larÄ±n Ã§alÄ±ÅŸÄ±p Ã§alÄ±ÅŸmadÄ±ÄŸÄ±nÄ± kontrol et"""
    
    container_selector = selectors.get("container", "article")
    containers = soup.select(container_selector)
    
    if not containers:
        return False, "Konteyner bulunamadÄ±"
    
    if len(containers) > 100:
        return False, f"Ã‡ok fazla konteyner ({len(containers)}), selector Ã§ok genel"
    
    # Birden fazla konteyner test et
    valid_containers = 0
    sample_titles = []
    
    for i, container in enumerate(containers[:5]):  # Ä°lk 5 konteyner test et
        title_selector = selectors.get("title", "h2 a")
        title_elem = container.select_one(title_selector)
        
        # Alternatif title selector'larÄ± dene
        if not title_elem:
            alt_title_selectors = [
                "h1 a", "h2 a", "h3 a", "h4 a",
                ".title a", ".headline a", 
                "[class*='title'] a", "[class*='headline'] a",
                "a[href*='article']", "a[href*='post']",
                "a[href*='/20']",  # Tarih iÃ§eren URL'ler
                "a"  # Son Ã§are
            ]
            
            for alt_selector in alt_title_selectors:
                title_elem = container.select_one(alt_selector)
                if title_elem and title_elem.get_text(strip=True) and len(title_elem.get_text(strip=True)) > 10:
                    # BaÅŸarÄ±lÄ± selector'Ä± gÃ¼ncelle
                    selectors["title"] = alt_selector
                    selectors["link"] = alt_selector
                    break
        
        if title_elem:
            title_text = title_elem.get_text(strip=True)
            if len(title_text) >= 10:
                # Link kontrolÃ¼
                link_elem = title_elem if title_elem.name == 'a' else title_elem.find('a')
                if link_elem and link_elem.get('href'):
                    valid_containers += 1
                    sample_titles.append(title_text[:50])
    
    if valid_containers == 0:
        return False, "HiÃ§bir konteynerde geÃ§erli baÅŸlÄ±k/link bulunamadÄ±"
    
    success_rate = (valid_containers / min(len(containers), 5)) * 100
    
    if success_rate < 40:  # %40'dan az baÅŸarÄ± oranÄ±
        return False, f"DÃ¼ÅŸÃ¼k baÅŸarÄ± oranÄ±: %{success_rate:.1f} ({valid_containers}/{min(len(containers), 5)})"
    
    return True, f"âœ… {len(containers)} konteyner, {valid_containers} geÃ§erli, Ã¶rnek: {sample_titles[0] if sample_titles else 'N/A'}..."

def fetch_articles_from_single_source(source):
    """Tek bir kaynaktan makale Ã§ek"""
    try:
        url = source["url"]
        selectors = source.get("article_selectors", {})
        source_name = source.get("name", "Bilinmeyen")
        
        # SayfayÄ± Ã§ek
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        articles = []
        
        # EÄŸer selector_type auto_detect ise veya mevcut selector'lar Ã§alÄ±ÅŸmÄ±yorsa
        if source.get('selector_type') == 'auto_detect' or not selectors:
            print(f"ğŸ” {source_name} iÃ§in otomatik selector tespiti yapÄ±lÄ±yor...")
            auto_selectors, container_count = auto_detect_selectors(soup, url)
            
            # Otomatik tespit edilen selector'larÄ± doÄŸrula
            is_valid, validation_msg = validate_selectors(soup, auto_selectors)
            
            if is_valid:
                print(f"âœ… Otomatik tespit baÅŸarÄ±lÄ±: {validation_msg}")
                selectors = auto_selectors
                
                # BaÅŸarÄ±lÄ± selector'larÄ± kaydet
                source["article_selectors"] = auto_selectors
                source["selector_type"] = "auto_detected"
            else:
                print(f"âŒ Otomatik tespit baÅŸarÄ±sÄ±z: {validation_msg}")
                return []
        else:
            # Mevcut selector'larÄ± doÄŸrula
            is_valid, validation_msg = validate_selectors(soup, selectors)
            if not is_valid:
                print(f"âš ï¸ Mevcut selector'lar Ã§alÄ±ÅŸmÄ±yor: {validation_msg}")
                print(f"ğŸ”„ Otomatik tespit deneniyor...")
                
                auto_selectors, container_count = auto_detect_selectors(soup, url)
                is_auto_valid, auto_validation_msg = validate_selectors(soup, auto_selectors)
                
                if is_auto_valid:
                    print(f"âœ… Otomatik tespit ile dÃ¼zeltildi: {auto_validation_msg}")
                    selectors = auto_selectors
                else:
                    print(f"âŒ Otomatik tespit de baÅŸarÄ±sÄ±z: {auto_validation_msg}")
                    return []
        
        # Makale konteynerlerini bul
        container_selector = selectors.get("container", "article, .article, .post")
        containers = soup.select(container_selector)
        
        if not containers:
            # Alternatif selectors dene
            alternative_selectors = [
                "li.wp-block-post",  # TechCrunch modern
                "article[data-testid='story-card']",  # The Verge modern
                ".c-entry-box", ".c-compact-river__entry",  # The Verge classic
                "article", ".post", ".news-item", ".story", 
                ".entry", ".content-item", "[class*='article']",
                "[class*='post']", "[class*='news']", "[class*='loop-card']"
            ]
            
            for alt_selector in alternative_selectors:
                containers = soup.select(alt_selector)
                if containers:
                    print(f"ğŸ”„ Alternatif selector kullanÄ±ldÄ±: {alt_selector}")
                    break
        
        print(f"ğŸ” {source['name']}: {len(containers)} konteyner bulundu")
        
        for container in containers[:5]:  # Ä°lk 5 makale
            try:
                # BaÅŸlÄ±k bul
                title_selector = selectors.get("title", "h1, h2, h3, .title, .headline")
                title_elem = container.select_one(title_selector)
                
                # Alternatif title selector'larÄ± dene
                if not title_elem:
                    alt_title_selectors = [
                        "h3.loop-card__title a",  # TechCrunch modern
                        "h2 a", ".c-entry-box__title a",  # The Verge
                        "h1 a", "h2 a", "h3 a", "h4 a",
                        ".title a", ".headline a", "[class*='title'] a"
                    ]
                    
                    for alt_selector in alt_title_selectors:
                        title_elem = container.select_one(alt_selector)
                        if title_elem:
                            break
                
                if not title_elem:
                    continue
                
                title = title_elem.get_text(strip=True)
                
                # Link bul
                link_elem = title_elem.find('a') if title_elem.name != 'a' else title_elem
                if not link_elem:
                    link_elem = container.select_one('a')
                
                if not link_elem:
                    continue
                
                link = link_elem.get('href', '')
                
                # Relative URL'leri absolute yap
                if link.startswith('/'):
                    from urllib.parse import urljoin
                    link = urljoin(url, link)
                elif not link.startswith('http'):
                    continue
                
                # Ã–zet bul
                excerpt_selector = selectors.get("excerpt", ".excerpt, .summary, p")
                excerpt_elem = container.select_one(excerpt_selector)
                excerpt = excerpt_elem.get_text(strip=True)[:200] if excerpt_elem else ""
                
                # Tarih bul (opsiyonel)
                date_selector = selectors.get("date", "time, .date, .published")
                date_elem = container.select_one(date_selector)
                date_str = date_elem.get_text(strip=True) if date_elem else ""
                
                if title and link:
                    # Makale iÃ§eriÄŸini Ã§ek (basit yÃ¶ntem)
                    try:
                        content_response = requests.get(link, headers=headers, timeout=15)
                        content_soup = BeautifulSoup(content_response.text, 'html.parser')
                        
                        # Ana iÃ§eriÄŸi Ã§Ä±kar
                        content_text = ""
                        content_selectors = [
                            'article', '.post-content', '.entry-content', 
                            '.article-content', '.content', 'main', '.story-body'
                        ]
                        
                        for cs in content_selectors:
                            content_elem = content_soup.select_one(cs)
                            if content_elem:
                                # Script ve style taglarÄ±nÄ± kaldÄ±r
                                for script in content_elem(["script", "style", "nav", "footer", "header"]):
                                    script.decompose()
                                content_text = content_elem.get_text(strip=True)
                                break
                        
                        # EÄŸer ana iÃ§erik bulunamazsa body'den al
                        if not content_text:
                            body = content_soup.find('body')
                            if body:
                                for script in body(["script", "style", "nav", "footer", "header"]):
                                    script.decompose()
                                content_text = body.get_text(strip=True)
                        
                        # Ä°Ã§eriÄŸi temizle ve sÄ±nÄ±rla
                        content_text = ' '.join(content_text.split())[:2000]
                        
                    except Exception as content_error:
                        print(f"âš ï¸ Ä°Ã§erik Ã§ekme hatasÄ±: {content_error}")
                        content_text = excerpt or title  # Fallback olarak Ã¶zet veya baÅŸlÄ±ÄŸÄ± kullan
                    
                    # Hash oluÅŸtur
                    article_hash = hashlib.md5(title.encode()).hexdigest()
                    
                    articles.append({
                        "title": title,
                        "url": link,
                        "content": content_text or excerpt or title,
                        "excerpt": excerpt,
                        "date": date_str,
                        "hash": article_hash,
                        "source": source["name"],
                        "source_id": source["id"],
                        "fetch_date": datetime.now().isoformat(),
                        "is_new": True,
                        "already_posted": False
                    })
                    
            except Exception as e:
                print(f"âš ï¸ Makale parse hatasÄ±: {e}")
                continue
        
        return articles
        
    except Exception as e:
        print(f"âŒ {source.get('name', 'Bilinmeyen')} kaynak hatasÄ±: {e}")
        return []

def get_news_sources_stats():
    """Haber kaynaklarÄ± istatistiklerini al - RSS dahil"""
    try:
        config = load_news_sources()
        
        # Normal kaynaklar
        sources = config.get("sources", [])
        rss_sources = config.get("rss_sources", [])
        
        # TÃ¼m kaynaklarÄ± birleÅŸtir
        all_sources = []
        
        # Normal kaynaklarÄ± ekle
        for source in sources:
            source_copy = source.copy()
            source_copy["type"] = "scraping"
            source_copy["total_articles"] = source.get("article_count", 0)
            
            # Son kontrol zamanÄ±nÄ± hesapla
            last_checked = source.get("last_checked")
            if last_checked:
                try:
                    from datetime import datetime
                    last_check_time = datetime.fromisoformat(last_checked.replace('Z', '+00:00'))
                    hours_ago = (datetime.now() - last_check_time).total_seconds() / 3600
                    source_copy["last_check_hours"] = int(hours_ago)
                except:
                    source_copy["last_check_hours"] = 999
            else:
                source_copy["last_check_hours"] = 999
                
            all_sources.append(source_copy)
        
        # RSS kaynaklarÄ±nÄ± ekle
        for rss_source in rss_sources:
            rss_copy = rss_source.copy()
            rss_copy["type"] = "rss"
            rss_copy["total_articles"] = rss_source.get("article_count", 0)
            
            # Son kontrol zamanÄ±nÄ± hesapla
            last_checked = rss_source.get("last_checked")
            if last_checked:
                try:
                    from datetime import datetime
                    last_check_time = datetime.fromisoformat(last_checked.replace('Z', '+00:00'))
                    hours_ago = (datetime.now() - last_check_time).total_seconds() / 3600
                    rss_copy["last_check_hours"] = int(hours_ago)
                except:
                    rss_copy["last_check_hours"] = 999
            else:
                rss_copy["last_check_hours"] = 999
                
            all_sources.append(rss_copy)
        
        # Ä°statistikleri hesapla
        total_sources = len(all_sources)
        enabled_sources = len([s for s in all_sources if s.get("enabled", True)])
        total_articles = sum(s.get("total_articles", 0) for s in all_sources)
        avg_success_rate = sum(s.get("success_rate", 0) for s in all_sources) / max(1, total_sources)
        
        return {
            "total_sources": total_sources,
            "enabled_sources": enabled_sources,
            "disabled_sources": total_sources - enabled_sources,
            "total_articles_fetched": total_articles,
            "average_success_rate": round(avg_success_rate, 1),
            "sources": all_sources,  # BirleÅŸtirilmiÅŸ kaynaklar
            "scraping_sources": sources,  # Sadece scraping kaynaklarÄ±
            "rss_sources": rss_sources,  # Sadece RSS kaynaklarÄ±
            "settings": config["settings"]
        }
        
    except Exception as e:
        return {
            "total_sources": 0,
            "enabled_sources": 0,
            "disabled_sources": 0,
            "total_articles_fetched": 0,
            "average_success_rate": 0,
            "sources": [],
            "scraping_sources": [],
            "rss_sources": [],
            "settings": {},
            "error": str(e)
        }

def update_fetch_articles_function():
    """Ana makale Ã§ekme fonksiyonunu gÃ¼ncelle - Ã¶zel kaynaklarÄ± dahil et"""
    try:
        # Ã–nce Ã¶zel kaynaklardan makale Ã§ek
        custom_articles = fetch_articles_from_custom_sources()
        
        # Sonra mevcut TechCrunch fallback'i Ã§ek (eÄŸer Ã¶zel kaynaklarda TechCrunch yoksa)
        config = load_news_sources()
        has_techcrunch = any("techcrunch" in s.get("url", "").lower() for s in config["sources"] if s.get("enabled", True))
        
        if not has_techcrunch:
            print("ğŸ”„ TechCrunch fallback ekleniyor...")
            fallback_articles = fetch_latest_ai_articles_fallback()
            custom_articles.extend(fallback_articles)
        
        return custom_articles
        
    except Exception as e:
        print(f"âŒ GÃ¼ncellenmiÅŸ makale Ã§ekme hatasÄ±: {e}")
        # Fallback olarak eski fonksiyonu Ã§aÄŸÄ±r
        return fetch_latest_ai_articles_fallback()

# =============================================================================
# GÃœVENLÄ° LOGGING SÄ°STEMÄ°
# =============================================================================

def safe_log(message, level="INFO", sensitive_data=None):
    """GÃ¼venli logging - ÅŸifre ve API anahtarlarÄ±nÄ± gizler"""
    import os
    
    # Sadece debug modunda detaylÄ± log
    debug_mode = os.environ.get('DEBUG', 'False').lower() == 'true'
    
    if not debug_mode and level == "DEBUG":
        return
    
    # MesajÄ± gÃ¼venli hale getir
    safe_message = sanitize_log_message(str(message))
    
    # Hassas verileri gizle
    if sensitive_data:
        for key, value in sensitive_data.items():
            if value and len(str(value)) > 3:
                masked_value = str(value)[:3] + "*" * (len(str(value)) - 3)
                safe_message = safe_message.replace(str(value), masked_value)
    
    # Production'da sadece Ã¶nemli loglarÄ± gÃ¶ster
    if not debug_mode and level not in ["ERROR", "WARNING", "INFO"]:
        return
    
    print(f"[{level}] {safe_message}")

# =============================================================================
# GÃœVENLÄ°K KONTROL FONKSÄ°YONLARI
# =============================================================================

def check_security_configuration():
    """GÃ¼venlik yapÄ±landÄ±rmasÄ±nÄ± kontrol et - E-posta OTP sistemi iÃ§in gÃ¼ncellenmiÅŸ"""
    import os
    
    security_issues = []
    
    # 1. Debug mode kontrolÃ¼
    debug_mode = os.environ.get('DEBUG', 'False').lower() == 'true'
    flask_env = os.environ.get('FLASK_ENV', 'production')
    
    if debug_mode and flask_env == 'production':
        security_issues.append("âš ï¸ Production'da DEBUG modu aÃ§Ä±k!")
    
    # 2. E-posta OTP sistemi kontrolÃ¼
    admin_email = os.environ.get('ADMIN_EMAIL', '')
    email_address = os.environ.get('EMAIL_ADDRESS', '')
    email_password = os.environ.get('EMAIL_PASSWORD', '')
    
    if not admin_email:
        security_issues.append("ğŸ“§ ADMIN_EMAIL yapÄ±landÄ±rÄ±lmamÄ±ÅŸ! GiriÅŸ yapÄ±lamaz.")
    elif not '@' in admin_email or '.' not in admin_email:
        security_issues.append("ğŸ“§ ADMIN_EMAIL geÃ§ersiz format!")
    
    if not email_address:
        security_issues.append("ğŸ“§ EMAIL_ADDRESS yapÄ±landÄ±rÄ±lmamÄ±ÅŸ! OTP gÃ¶nderilemez.")
    elif not '@' in email_address or '.' not in email_address:
        security_issues.append("ğŸ“§ EMAIL_ADDRESS geÃ§ersiz format!")
    
    if not email_password:
        security_issues.append("ğŸ” EMAIL_PASSWORD yapÄ±landÄ±rÄ±lmamÄ±ÅŸ! SMTP baÄŸlantÄ±sÄ± kurulamaz.")
    elif len(email_password) < 8:
        security_issues.append("ğŸ” EMAIL_PASSWORD Ã§ok kÄ±sa! App Password kullanÄ±n.")
    
    # 3. Secret key kontrolÃ¼
    secret_key = os.environ.get('SECRET_KEY', 'your-secret-key-here')
    if secret_key == 'your-secret-key-here' or len(secret_key) < 32:
        security_issues.append("ğŸ”‘ GÃ¼Ã§lÃ¼ SECRET_KEY kullanÄ±n! (En az 32 karakter)")
    
    # 4. API anahtarlarÄ± kontrolÃ¼
    api_keys = [
        'GOOGLE_API_KEY',
        'TWITTER_API_KEY',
        'TWITTER_API_SECRET',
        'TWITTER_ACCESS_TOKEN',
        'TWITTER_ACCESS_TOKEN_SECRET',
        'TWITTER_BEARER_TOKEN'
    ]
    
    for key in api_keys:
        value = os.environ.get(key, '')
        if value and ('your-' in value.lower() or 'example' in value.lower() or 'test' in value.lower()):
            security_issues.append(f"ğŸ” {key} Ã¶rnek/test deÄŸer iÃ§eriyor!")
    
    # 5. Telegram gÃ¼venlik kontrolÃ¼
    telegram_token = os.environ.get('TELEGRAM_BOT_TOKEN', '')
    if telegram_token and len(telegram_token) < 40:
        security_issues.append("ğŸ¤– TELEGRAM_BOT_TOKEN Ã§ok kÄ±sa! GeÃ§erli token kullanÄ±n.")
    
    # 6. Gmail gÃ¼venlik kontrolÃ¼
    gmail_email = os.environ.get('GMAIL_EMAIL', '')
    gmail_password = os.environ.get('GMAIL_APP_PASSWORD', '')
    
    if gmail_email and not gmail_password:
        security_issues.append("ğŸ“§ GMAIL_EMAIL var ama GMAIL_APP_PASSWORD eksik!")
    
    # 7. GÃ¼venlik seviyesi deÄŸerlendirmesi
    security_score = 100 - (len(security_issues) * 10)
    security_level = "YÃ¼ksek" if security_score >= 80 else "Orta" if security_score >= 60 else "DÃ¼ÅŸÃ¼k"
    
    return {
        "secure": len(security_issues) == 0,
        "issues": security_issues,
        "debug_mode": debug_mode,
        "flask_env": flask_env,
        "auth_method": "E-posta OTP",
        "admin_email": admin_email,
        "email_configured": bool(email_address and email_password),
        "security_score": security_score,
        "security_level": security_level,
        "total_checks": 7,
        "passed_checks": 7 - len(security_issues)
    }

def sanitize_log_message(message):
    """Log mesajlarÄ±ndan hassas bilgileri temizle"""
    import re
    
    # API anahtarÄ± pattern'leri
    patterns = [
        r'AIza[0-9A-Za-z-_]{35}',  # Google API Key
        r'sk-[a-zA-Z0-9]{48}',     # OpenAI API Key
        r'[0-9]{10}:[A-Za-z0-9_-]{35}',  # Telegram Bot Token
        r'[A-Za-z0-9]{25}',        # Twitter Bearer Token
        r'[A-Za-z0-9]{15,25}',     # Twitter API Keys
    ]
    
    for pattern in patterns:
        message = re.sub(pattern, '***MASKED***', message)
    
    # Åifre pattern'leri
    password_patterns = [
        r'password["\s]*[:=]["\s]*[^"\s]+',
        r'pass["\s]*[:=]["\s]*[^"\s]+',
        r'token["\s]*[:=]["\s]*[^"\s]+',
        r'key["\s]*[:=]["\s]*[^"\s]+',
    ]
    
    for pattern in password_patterns:
        message = re.sub(pattern, 'password=***MASKED***', message, flags=re.IGNORECASE)
    
    return message

def calculate_text_similarity(text1, text2):
    """Ä°ki metin arasÄ±ndaki benzerlik oranÄ±nÄ± hesapla (0-1 arasÄ±)"""
    if not text1 or not text2:
        return 0.0
    
    # Metinleri normalize et
    text1 = text1.lower().strip()
    text2 = text2.lower().strip()
    
    # Ã‡ok kÄ±sa metinler iÃ§in direkt karÅŸÄ±laÅŸtÄ±rma
    if len(text1) < 10 or len(text2) < 10:
        return 1.0 if text1 == text2 else 0.0
    
    # SequenceMatcher ile benzerlik hesapla
    similarity = SequenceMatcher(None, text1, text2).ratio()
    return similarity

def normalize_title_for_comparison(title):
    """BaÅŸlÄ±ÄŸÄ± karÅŸÄ±laÅŸtÄ±rma iÃ§in normalize et"""
    if not title:
        return ""
    
    # KÃ¼Ã§Ã¼k harfe Ã§evir
    normalized = title.lower()
    
    # Gereksiz karakterleri kaldÄ±r
    normalized = re.sub(r'[^\w\s]', ' ', normalized)
    
    # Ã‡oklu boÅŸluklarÄ± tek boÅŸluÄŸa Ã§evir
    normalized = re.sub(r'\s+', ' ', normalized).strip()
    
    # YaygÄ±n kelimeler ve ifadeleri kaldÄ±r
    stop_words = ['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'says', 'announces', 'reveals', 'launches', 'introduces']
    words = normalized.split()
    filtered_words = [word for word in words if word not in stop_words and len(word) > 2]
    
    return ' '.join(filtered_words)

def extract_key_content_features(content):
    """Ä°Ã§erikten anahtar Ã¶zellikler Ã§Ä±kar"""
    if not content:
        return set()
    
    # Metni normalize et
    normalized = content.lower()
    
    # SayÄ±larÄ± ve Ã¶zel terimleri Ã§Ä±kar
    features = set()
    
    # SayÄ±sal deÄŸerler (para, yÃ¼zde, sayÄ±lar)
    numbers = re.findall(r'\$[\d,]+(?:\.\d+)?[bmk]?|\d+(?:\.\d+)?%|\d+(?:\.\d+)?(?:\s*(?:billion|million|thousand|percent))?', normalized)
    features.update(numbers)
    
    # Åirket isimleri (bÃ¼yÃ¼k harfle baÅŸlayan kelimeler)
    companies = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', content)
    features.update([c.lower() for c in companies])
    
    # Ã–nemli teknoloji terimleri
    tech_terms = re.findall(r'\b(?:ai|artificial intelligence|machine learning|deep learning|neural network|algorithm|api|cloud|blockchain|cryptocurrency|robot|automation|quantum|5g|iot|ar|vr|metaverse)\b', normalized)
    features.update(tech_terms)
    
    return features

def check_content_similarity(article1, article2, title_threshold=0.8, content_threshold=0.6):
    """Ä°ki makale arasÄ±ndaki benzerliÄŸi kontrol et"""
    try:
        title1 = article1.get('title', '')
        title2 = article2.get('title', '')
        content1 = article1.get('content', '')
        content2 = article2.get('content', '')
        
        # BaÅŸlÄ±k benzerliÄŸi
        normalized_title1 = normalize_title_for_comparison(title1)
        normalized_title2 = normalize_title_for_comparison(title2)
        title_similarity = calculate_text_similarity(normalized_title1, normalized_title2)
        
        # EÄŸer baÅŸlÄ±klar Ã§ok benzer ise, muhtemelen aynÄ± haber
        if title_similarity >= title_threshold:
            return True, title_similarity, "title"
        
        # Ä°Ã§erik Ã¶zellik benzerliÄŸi
        features1 = extract_key_content_features(content1)
        features2 = extract_key_content_features(content2)
        
        if features1 and features2:
            # Jaccard benzerliÄŸi (kesiÅŸim / birleÅŸim)
            intersection = len(features1.intersection(features2))
            union = len(features1.union(features2))
            feature_similarity = intersection / union if union > 0 else 0.0
            
            if feature_similarity >= content_threshold:
                return True, feature_similarity, "content_features"
        
        # Ä°Ã§erik metni benzerliÄŸi (sadece kÄ±sa iÃ§erikler iÃ§in)
        if len(content1) < 1000 and len(content2) < 1000:
            content_similarity = calculate_text_similarity(content1[:500], content2[:500])
            if content_similarity >= content_threshold:
                return True, content_similarity, "content_text"
        
        return False, max(title_similarity, feature_similarity if 'feature_similarity' in locals() else 0), "no_match"
        
    except Exception as e:
        print(f"Benzerlik kontrolÃ¼ hatasÄ±: {e}")
        return False, 0.0, "error"

def filter_duplicate_articles(new_articles, existing_articles=None):
    """Yeni makalelerden duplikatlarÄ± filtrele"""
    try:
        # AyarlarÄ± yÃ¼kle
        settings = load_automation_settings()
        
        # EÄŸer duplikat tespiti kapalÄ±ysa, sadece temel kontrolleri yap
        if not settings.get('enable_duplicate_detection', True):
            print("ğŸ”„ GeliÅŸmiÅŸ duplikat tespiti kapalÄ±, sadece temel kontroller yapÄ±lÄ±yor...")
            return basic_duplicate_filter(new_articles, existing_articles)
        
        # EÅŸik deÄŸerlerini ayarlardan al
        title_threshold = settings.get('title_similarity_threshold', 0.8)
        content_threshold = settings.get('content_similarity_threshold', 0.6)
        
        print(f"ğŸ” GeliÅŸmiÅŸ duplikat tespiti aktif (BaÅŸlÄ±k: {title_threshold:.0%}, Ä°Ã§erik: {content_threshold:.0%})")
        
        if existing_articles is None:
            # Mevcut paylaÅŸÄ±lan makaleleri yÃ¼kle
            existing_articles = load_json(HISTORY_FILE)
        
        # Bekleyen tweet'leri de kontrol et
        pending_tweets = load_json("pending_tweets.json")
        pending_articles = [tweet.get('article', {}) for tweet in pending_tweets if tweet.get('article')]
        
        # TÃ¼m mevcut makaleleri birleÅŸtir (silinen makaleler de dahil)
        all_existing = existing_articles + pending_articles
        
        # Silinen makaleleri de kontrol et (deleted=True olanlar)
        deleted_articles = [article for article in existing_articles if article.get('deleted', False)]
        if deleted_articles:
            print(f"ğŸ—‘ï¸ {len(deleted_articles)} silinen makale duplikat kontrole dahil edildi")
        
        filtered_articles = []
        duplicate_count = 0
        
        for new_article in new_articles:
            is_duplicate = False
            
            # Ã–nce URL kontrolÃ¼ (hÄ±zlÄ±)
            new_url = new_article.get('url', '')
            for existing in all_existing:
                if new_url == existing.get('url', ''):
                    is_duplicate = True
                    break
            
            if is_duplicate:
                duplicate_count += 1
                print(f"ğŸ”„ URL duplikatÄ± atlandÄ±: {new_article.get('title', '')[:50]}...")
                continue
            
            # Hash kontrolÃ¼ (hÄ±zlÄ±)
            new_hash = new_article.get('hash', '')
            if new_hash:
                for existing in all_existing:
                    if new_hash == existing.get('hash', ''):
                        is_duplicate = True
                        break
            
            if is_duplicate:
                duplicate_count += 1
                print(f"ğŸ”„ Hash duplikatÄ± atlandÄ±: {new_article.get('title', '')[:50]}...")
                continue
            
            # Ä°Ã§erik benzerliÄŸi kontrolÃ¼ (yavaÅŸ ama etkili)
            for existing in all_existing:
                is_similar, similarity_score, match_type = check_content_similarity(
                    new_article, existing, title_threshold, content_threshold
                )
                if is_similar:
                    is_duplicate = True
                    print(f"ğŸ”„ Ä°Ã§erik benzerliÄŸi ({match_type}: {similarity_score:.2f}) - atlandÄ±: {new_article.get('title', '')[:50]}...")
                    break
            
            if not is_duplicate:
                # AynÄ± batch iÃ§inde de kontrol et
                for other_article in filtered_articles:
                    is_similar, similarity_score, match_type = check_content_similarity(
                        new_article, other_article, title_threshold, content_threshold
                    )
                    if is_similar:
                        is_duplicate = True
                        print(f"ğŸ”„ Batch iÃ§i benzerlik ({match_type}: {similarity_score:.2f}) - atlandÄ±: {new_article.get('title', '')[:50]}...")
                        break
            
            if not is_duplicate:
                filtered_articles.append(new_article)
                print(f"âœ… Yeni makale eklendi: {new_article.get('title', '')[:50]}...")
            else:
                duplicate_count += 1
        
        print(f"ğŸ“Š Duplikat filtreleme tamamlandÄ±: {len(new_articles)} makale â†’ {len(filtered_articles)} benzersiz makale ({duplicate_count} duplikat)")
        return filtered_articles
        
    except Exception as e:
        print(f"Duplikat filtreleme hatasÄ±: {e}")
        return new_articles  # Hata durumunda orijinal listeyi dÃ¶ndÃ¼r

def basic_duplicate_filter(new_articles, existing_articles=None):
    """Temel duplikat filtreleme - sadece URL ve hash kontrolÃ¼"""
    try:
        if existing_articles is None:
            existing_articles = load_json(HISTORY_FILE)
        
        # Bekleyen tweet'leri de kontrol et
        pending_tweets = load_json("pending_tweets.json")
        pending_articles = [tweet.get('article', {}) for tweet in pending_tweets if tweet.get('article')]
        
        # TÃ¼m mevcut makaleleri birleÅŸtir (silinen makaleler de dahil)
        all_existing = existing_articles + pending_articles
        
        # Silinen makaleleri de kontrol et (deleted=True olanlar)
        deleted_articles = [article for article in existing_articles if article.get('deleted', False)]
        if deleted_articles:
            print(f"ğŸ—‘ï¸ {len(deleted_articles)} silinen makale temel duplikat kontrole dahil edildi")
        
        # Mevcut URL'ler ve hash'ler
        existing_urls = set(article.get('url', '') for article in all_existing)
        existing_hashes = set(article.get('hash', '') for article in all_existing)
        
        filtered_articles = []
        duplicate_count = 0
        
        for new_article in new_articles:
            new_url = new_article.get('url', '')
            new_hash = new_article.get('hash', '')
            
            # URL kontrolÃ¼
            if new_url in existing_urls:
                duplicate_count += 1
                print(f"ğŸ”„ URL duplikatÄ± atlandÄ±: {new_article.get('title', '')[:50]}...")
                continue
            
            # Hash kontrolÃ¼
            if new_hash and new_hash in existing_hashes:
                duplicate_count += 1
                print(f"ğŸ”„ Hash duplikatÄ± atlandÄ±: {new_article.get('title', '')[:50]}...")
                continue
            
            # AynÄ± batch iÃ§inde URL kontrolÃ¼
            batch_urls = set(article.get('url', '') for article in filtered_articles)
            if new_url in batch_urls:
                duplicate_count += 1
                print(f"ğŸ”„ Batch iÃ§i URL duplikatÄ± atlandÄ±: {new_article.get('title', '')[:50]}...")
                continue
            
            filtered_articles.append(new_article)
            print(f"âœ… Yeni makale eklendi: {new_article.get('title', '')[:50]}...")
        
        print(f"ğŸ“Š Temel duplikat filtreleme tamamlandÄ±: {len(new_articles)} makale â†’ {len(filtered_articles)} benzersiz makale ({duplicate_count} duplikat)")
        return filtered_articles
        
    except Exception as e:
        print(f"Temel duplikat filtreleme hatasÄ±: {e}")
        return new_articles

def clean_duplicate_pending_tweets():
    """Bekleyen tweet'lerdeki duplikatlarÄ± temizle"""
    try:
        pending_tweets = load_json("pending_tweets.json")
        
        if not pending_tweets:
            return {
                "success": True,
                "message": "Bekleyen tweet bulunamadÄ±",
                "original_count": 0,
                "cleaned_count": 0,
                "removed_count": 0
            }
        
        print(f"ğŸ” {len(pending_tweets)} bekleyen tweet kontrol ediliyor...")
        
        # Benzersiz tweet'leri saklamak iÃ§in
        unique_tweets = []
        seen_urls = set()
        seen_hashes = set()
        seen_titles = set()
        
        duplicate_count = 0
        
        for tweet in pending_tweets:
            article = tweet.get('article', {})
            url = article.get('url', '')
            hash_val = article.get('hash', '')
            title = article.get('title', '')
            
            # URL kontrolÃ¼
            if url and url in seen_urls:
                duplicate_count += 1
                print(f"ğŸ”„ URL duplikatÄ± atlandÄ±: {title[:50]}...")
                continue
            
            # Hash kontrolÃ¼
            if hash_val and hash_val in seen_hashes:
                duplicate_count += 1
                print(f"ğŸ”„ Hash duplikatÄ± atlandÄ±: {title[:50]}...")
                continue
            
            # BaÅŸlÄ±k kontrolÃ¼ (normalize edilmiÅŸ)
            if title:
                normalized_title = normalize_title_for_comparison(title)
                if normalized_title in seen_titles:
                    duplicate_count += 1
                    print(f"ğŸ”„ BaÅŸlÄ±k duplikatÄ± atlandÄ±: {title[:50]}...")
                    continue
                seen_titles.add(normalized_title)
            
            # Benzersiz tweet olarak ekle
            unique_tweets.append(tweet)
            if url:
                seen_urls.add(url)
            if hash_val:
                seen_hashes.add(hash_val)
            
            print(f"âœ… Benzersiz tweet korundu: {title[:50]}...")
        
        # TemizlenmiÅŸ listeyi kaydet
        save_json("pending_tweets.json", unique_tweets)
        
        result = {
            "success": True,
            "message": f"âœ… Bekleyen tweet'ler temizlendi",
            "original_count": len(pending_tweets),
            "cleaned_count": len(unique_tweets),
            "removed_count": duplicate_count
        }
        
        print(f"ğŸ“Š Duplikat temizleme tamamlandÄ±: {len(pending_tweets)} â†’ {len(unique_tweets)} tweet ({duplicate_count} duplikat kaldÄ±rÄ±ldÄ±)")
        
        return result
        
    except Exception as e:
        print(f"âŒ Duplikat temizleme hatasÄ±: {e}")
        return {
            "success": False,
            "message": f"âŒ Temizleme hatasÄ±: {str(e)}",
            "original_count": 0,
            "cleaned_count": 0,
            "removed_count": 0
        }

# =============================================================================
# Rate limit yÃ¶netimi iÃ§in global deÄŸiÅŸkenler
RATE_LIMIT_FILE = "rate_limit_status.json"
TWITTER_RATE_LIMITS = {
    "tweets": {"limit": 5, "window": 900},  # 15 dakikada 5 tweet (Free plan iÃ§in gÃ¼venli)
    "user_lookup": {"limit": 50, "window": 900},  # 15 dakikada 50 kullanÄ±cÄ± sorgusu
    "timeline": {"limit": 30, "window": 900}  # 15 dakikada 30 timeline sorgusu
}

def load_rate_limit_status():
    """Rate limit durumunu yÃ¼kle"""
    try:
        if os.path.exists(RATE_LIMIT_FILE):
            with open(RATE_LIMIT_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}
    except Exception as e:
        print(f"Rate limit durumu yÃ¼klenirken hata: {e}")
        return {}

def save_rate_limit_status(status):
    """Rate limit durumunu kaydet"""
    try:
        with open(RATE_LIMIT_FILE, 'w', encoding='utf-8') as f:
            json.dump(status, f, indent=2, ensure_ascii=False)
    except Exception as e:
        print(f"Rate limit durumu kaydedilirken hata: {e}")

def check_rate_limit(endpoint="tweets"):
    """Rate limit kontrolÃ¼ yap"""
    try:
        status = load_rate_limit_status()
        current_time = time.time()
        
        if endpoint not in status:
            status[endpoint] = {
                "requests": 0,
                "reset_time": current_time + TWITTER_RATE_LIMITS[endpoint]["window"],
                "last_request": current_time
            }
        
        endpoint_status = status[endpoint]
        
        # Reset zamanÄ± geÃ§tiyse sÄ±fÄ±rla
        if current_time >= endpoint_status["reset_time"]:
            endpoint_status["requests"] = 0
            endpoint_status["reset_time"] = current_time + TWITTER_RATE_LIMITS[endpoint]["window"]
        
        # Limit kontrolÃ¼
        if endpoint_status["requests"] >= TWITTER_RATE_LIMITS[endpoint]["limit"]:
            wait_time = endpoint_status["reset_time"] - current_time
            return {
                "allowed": False,
                "wait_time": wait_time,
                "requests_made": endpoint_status["requests"],
                "limit": TWITTER_RATE_LIMITS[endpoint]["limit"]
            }
        
        return {
            "allowed": True,
            "requests_made": endpoint_status["requests"],
            "limit": TWITTER_RATE_LIMITS[endpoint]["limit"],
            "reset_time": endpoint_status["reset_time"]
        }
        
    except Exception as e:
        print(f"Rate limit kontrolÃ¼ hatasÄ±: {e}")
        return {"allowed": True}  # Hata durumunda izin ver

def update_rate_limit_usage(endpoint="tweets"):
    """Rate limit kullanÄ±mÄ±nÄ± gÃ¼ncelle"""
    try:
        status = load_rate_limit_status()
        current_time = time.time()
        
        if endpoint not in status:
            status[endpoint] = {
                "requests": 0,
                "reset_time": current_time + TWITTER_RATE_LIMITS[endpoint]["window"],
                "last_request": current_time
            }
        
        # Reset zamanÄ± geÃ§tiyse sÄ±fÄ±rla
        if current_time >= status[endpoint]["reset_time"]:
            status[endpoint]["requests"] = 0
            status[endpoint]["reset_time"] = current_time + TWITTER_RATE_LIMITS[endpoint]["window"]
        
        # KullanÄ±mÄ± artÄ±r
        status[endpoint]["requests"] += 1
        status[endpoint]["last_request"] = current_time
        
        save_rate_limit_status(status)
        
        print(f"Rate limit gÃ¼ncellendi - {endpoint}: {status[endpoint]['requests']}/{TWITTER_RATE_LIMITS[endpoint]['limit']}")
        
    except Exception as e:
        print(f"Rate limit gÃ¼ncelleme hatasÄ±: {e}")

def get_rate_limit_info():
    """Rate limit bilgilerini al"""
    try:
        status = load_rate_limit_status()
        current_time = time.time()
        info = {}
        
        for endpoint in TWITTER_RATE_LIMITS:
            if endpoint in status:
                endpoint_status = status[endpoint]
                
                # Reset zamanÄ± geÃ§tiyse sÄ±fÄ±rla
                if current_time >= endpoint_status["reset_time"]:
                    requests_made = 0
                    reset_time = current_time + TWITTER_RATE_LIMITS[endpoint]["window"]
                else:
                    requests_made = endpoint_status["requests"]
                    reset_time = endpoint_status["reset_time"]
                
                info[endpoint] = {
                    "requests_made": requests_made,
                    "limit": TWITTER_RATE_LIMITS[endpoint]["limit"],
                    "remaining": TWITTER_RATE_LIMITS[endpoint]["limit"] - requests_made,
                    "reset_time": reset_time,
                    "reset_in_minutes": max(0, (reset_time - current_time) / 60)
                }
            else:
                info[endpoint] = {
                    "requests_made": 0,
                    "limit": TWITTER_RATE_LIMITS[endpoint]["limit"],
                    "remaining": TWITTER_RATE_LIMITS[endpoint]["limit"],
                    "reset_time": current_time + TWITTER_RATE_LIMITS[endpoint]["window"],
                    "reset_in_minutes": TWITTER_RATE_LIMITS[endpoint]["window"] / 60
                }
        
        return info
        
    except Exception as e:
        print(f"Rate limit bilgisi alma hatasÄ±: {e}")
        return {}

def retry_pending_tweets_after_rate_limit():
    """Rate limit sÄ±fÄ±rlandÄ±ktan sonra bekleyen tweet'leri tekrar dene"""
    try:
        # Rate limit durumunu kontrol et
        rate_check = check_rate_limit("tweets")
        if not rate_check.get("allowed", True):
            print(f"Rate limit hala aktif, {int(rate_check.get('wait_time', 0) / 60)} dakika daha beklenecek")
            return {"success": False, "message": "Rate limit hala aktif"}
        
        # Bekleyen tweet'leri yÃ¼kle
        pending_tweets = load_json("pending_tweets.json")
        if not pending_tweets:
            return {"success": True, "message": "Bekleyen tweet yok"}
        
        # Rate limit hatasÄ± olan tweet'leri filtrele
        rate_limited_tweets = []
        other_tweets = []
        
        for tweet in pending_tweets:
            error_reason = tweet.get('error_reason', '')
            if 'rate limit' in error_reason.lower() or '429' in error_reason:
                rate_limited_tweets.append(tweet)
            else:
                other_tweets.append(tweet)
        
        if not rate_limited_tweets:
            return {"success": True, "message": "Rate limit hatasÄ± olan tweet yok"}
        
        print(f"ğŸ”„ {len(rate_limited_tweets)} rate limit hatasÄ± olan tweet tekrar deneniyor...")
        
        successful_posts = 0
        failed_posts = 0
        
        for tweet in rate_limited_tweets:
            try:
                # Rate limit kontrolÃ¼ yap (her tweet iÃ§in)
                rate_check = check_rate_limit("tweets")
                if not rate_check.get("allowed", True):
                    print("Rate limit tekrar aÅŸÄ±ldÄ±, kalan tweet'ler beklemede kalacak")
                    break
                
                # Tweet'i paylaÅŸ
                tweet_text = tweet['tweet_data']['tweet']
                result = post_text_tweet_v2(tweet_text)
                
                if result.get('success'):
                    # BaÅŸarÄ±lÄ± paylaÅŸÄ±m - posted_articles'a ekle
                    article = tweet['article']
                    article['posted_date'] = datetime.now().isoformat()
                    article['tweet_text'] = tweet_text
                    article['tweet_url'] = result.get('url', '')
                    article['manual_post'] = False
                    
                    # Posted articles'a ekle
                    posted_articles = load_json("posted_articles.json")
                    posted_articles.append(article)
                    save_json("posted_articles.json", posted_articles)
                    
                    successful_posts += 1
                    print(f"âœ… Tweet baÅŸarÄ±yla paylaÅŸÄ±ldÄ±: {article.get('title', '')[:50]}...")
                    
                else:
                    # Hala hata var - tweet'i other_tweets'e ekle
                    tweet['retry_count'] = tweet.get('retry_count', 0) + 1
                    tweet['error_reason'] = result.get('error', 'Bilinmeyen hata')
                    other_tweets.append(tweet)
                    failed_posts += 1
                    print(f"âŒ Tweet paylaÅŸÄ±m hatasÄ±: {result.get('error', 'Bilinmeyen hata')}")
                    
                    # Rate limit hatasÄ± ise dur
                    if result.get('rate_limited'):
                        print("Rate limit tekrar aÅŸÄ±ldÄ±, kalan tweet'ler beklemede kalacak")
                        # Kalan tweet'leri de other_tweets'e ekle
                        remaining_tweets = rate_limited_tweets[rate_limited_tweets.index(tweet) + 1:]
                        other_tweets.extend(remaining_tweets)
                        break
                
            except Exception as e:
                print(f"Tweet retry hatasÄ±: {e}")
                tweet['retry_count'] = tweet.get('retry_count', 0) + 1
                tweet['error_reason'] = str(e)
                other_tweets.append(tweet)
                failed_posts += 1
        
        # GÃ¼ncellenmiÅŸ pending tweet'leri kaydet
        save_json("pending_tweets.json", other_tweets)
        
        message = f"âœ… {successful_posts} tweet baÅŸarÄ±yla paylaÅŸÄ±ldÄ±"
        if failed_posts > 0:
            message += f", {failed_posts} tweet hala beklemede"
        
        print(f"ğŸ“Š Retry tamamlandÄ±: {successful_posts} baÅŸarÄ±lÄ±, {failed_posts} baÅŸarÄ±sÄ±z")
        
        return {
            "success": True,
            "message": message,
            "successful_posts": successful_posts,
            "failed_posts": failed_posts
        }
        
    except Exception as e:
        print(f"âŒ Retry iÅŸlemi hatasÄ±: {e}")
        return {"success": False, "message": str(e)}

# ... existing code ...

def terminal_log(message, level='info'):
    """Konsol log fonksiyonu (terminal iÅŸlevi kaldÄ±rÄ±ldÄ±)"""
    import time
    
    # Konsola yazdÄ±r
    level_colors = {
        'info': '\033[92m',      # YeÅŸil
        'warning': '\033[93m',   # SarÄ±
        'error': '\033[91m',     # KÄ±rmÄ±zÄ±
        'debug': '\033[96m',     # Cyan
        'success': '\033[92m'    # YeÅŸil
    }
    
    color = level_colors.get(level, '\033[0m')
    reset = '\033[0m'
    timestamp = time.strftime('%H:%M:%S')
    
    print(f"{color}[{timestamp}] [{level.upper()}] {message}{reset}")

def advanced_web_scraper(url, wait_time=3, use_js=False, return_html=False):
    """GeliÅŸmiÅŸ web scraping - MCP'ye alternatif"""
    try:
        print(f"ğŸ” GeliÅŸmiÅŸ scraper ile Ã§ekiliyor: {url}")
        
        # KullanÄ±labilir yÃ¶ntemleri kontrol et
        available_methods = []
        if REQUESTS_HTML_AVAILABLE and requests_html:
            available_methods.append("requests-html")
        if SELENIUM_AVAILABLE and webdriver:
            available_methods.append("selenium")
        available_methods.append("requests")  # Her zaman mevcut
        
        print(f"ğŸ“‹ KullanÄ±labilir yÃ¶ntemler: {', '.join(available_methods)}")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,tr;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        }
        
        # YÃ¶ntem 1: requests-html (JavaScript desteÄŸi ile)
        if REQUESTS_HTML_AVAILABLE and use_js and requests_html:
            try:
                from requests_html import HTMLSession  # type: ignore
                session = HTMLSession()
                
                print("ğŸš€ requests-html ile JavaScript render ediliyor...")
                r = session.get(url, headers=headers, timeout=30)
                r.html.render(wait=wait_time, timeout=20)
                
                content = r.html.html
                soup = BeautifulSoup(content, 'html.parser')
                
                print(f"âœ… requests-html baÅŸarÄ±lÄ±: {len(content)} karakter")
                return {
                    "success": True,
                    "content": extract_main_content(soup),
                    "html": content,
                    "method": "requests-html"
                }
                
            except Exception as rh_error:
                print(f"âš ï¸ requests-html hatasÄ±: {rh_error}")
        
        # YÃ¶ntem 2: Selenium (headless Chrome)
        if SELENIUM_AVAILABLE and use_js and webdriver and Options:
            try:
                print("ğŸš€ Selenium ile JavaScript render ediliyor...")
                
                chrome_options = Options()
                chrome_options.add_argument('--headless')
                chrome_options.add_argument('--no-sandbox')
                chrome_options.add_argument('--disable-dev-shm-usage')
                chrome_options.add_argument('--disable-gpu')
                chrome_options.add_argument('--window-size=1920,1080')
                chrome_options.add_argument(f'--user-agent={headers["User-Agent"]}')
                
                driver = webdriver.Chrome(options=chrome_options)
                driver.set_page_load_timeout(30)
                
                driver.get(url)
                
                # SayfanÄ±n yÃ¼klenmesini bekle
                if WebDriverWait and EC and By:
                    WebDriverWait(driver, wait_time).until(
                        EC.presence_of_element_located((By.TAG_NAME, "body"))
                    )
                else:
                    time.sleep(wait_time)
                
                # Biraz daha bekle (dinamik iÃ§erik iÃ§in)
                time.sleep(wait_time)
                
                content = driver.page_source
                driver.quit()
                
                soup = BeautifulSoup(content, 'html.parser')
                
                print(f"âœ… Selenium baÅŸarÄ±lÄ±: {len(content)} karakter")
                return {
                    "success": True,
                    "content": extract_main_content(soup),
                    "html": content,
                    "method": "selenium"
                }
                
            except Exception as selenium_error:
                print(f"âš ï¸ Selenium hatasÄ±: {selenium_error}")
                try:
                    driver.quit()
                except:
                    pass
        
        # YÃ¶ntem 3: Basit requests (fallback)
        print("ğŸ”„ Basit HTTP request ile deneniyor...")
        
        session = requests.Session()
        session.headers.update(headers)
        
        response = session.get(url, timeout=30, allow_redirects=True)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        print(f"âœ… Basit request baÅŸarÄ±lÄ±: {len(response.text)} karakter")
        return {
            "success": True,
            "content": extract_main_content(soup),
            "html": response.text,
            "method": "requests"
        }
        
    except Exception as e:
        print(f"âŒ GeliÅŸmiÅŸ scraper hatasÄ±: {e}")
        return {"success": False, "error": str(e)}

def extract_main_content(soup):
    """Sayfadan ana iÃ§eriÄŸi Ã§Ä±kar"""
    try:
        # Ana iÃ§erik selector'larÄ± (Ã¶ncelik sÄ±rasÄ±na gÃ¶re)
        main_content_selectors = [
            'article',
            '[role="main"]',
            'main',
            '.post-content',
            '.entry-content',
            '.article-content',
            '.content',
            '.story-body',
            '.article-body',
            '#content',
            '.main-content'
        ]
        
        content_text = ""
        
        for selector in main_content_selectors:
            elements = soup.select(selector)
            if elements:
                element = elements[0]
                
                # Gereksiz elementleri kaldÄ±r
                for unwanted in element(["script", "style", "nav", "footer", "header", "aside", ".advertisement", ".ads", ".social-share"]):
                    unwanted.decompose()
                
                content_text = element.get_text(strip=True)
                if len(content_text) > 200:  # Yeterli iÃ§erik varsa
                    break
        
        # EÄŸer ana iÃ§erik bulunamazsa body'den al
        if not content_text or len(content_text) < 200:
            body = soup.find('body')
            if body:
                # Gereksiz elementleri kaldÄ±r
                for unwanted in body(["script", "style", "nav", "footer", "header", "aside", ".advertisement", ".ads", ".social-share", ".menu", ".sidebar"]):
                    unwanted.decompose()
                
                content_text = body.get_text(strip=True)
        
        # Ä°Ã§eriÄŸi temizle
        content_text = ' '.join(content_text.split())
        # content_text = content_text[:3000]  # Ä°lk 3000 karakter sÄ±nÄ±rÄ± kaldÄ±rÄ±ldÄ±
        
        return content_text
        
    except Exception as e:
        print(f"âŒ Ä°Ã§erik Ã§Ä±karma hatasÄ±: {e}")
        return ""

# ==========================================
# PYTHONANYWHERE UYUMLU HABER Ã‡EKME SÄ°STEMÄ°
# ==========================================

def fetch_latest_ai_articles_pythonanywhere():
    """PythonAnywhere iÃ§in optimize edilmiÅŸ haber Ã§ekme sistemi - Ã–zel kaynaklar + API'ler"""
    try:
        # Ã–nce mevcut yayÄ±nlanan makaleleri yÃ¼kle
        posted_articles = load_json(HISTORY_FILE)
        posted_urls = [article.get('url', '') for article in posted_articles]
        posted_hashes = [article.get('hash', '') for article in posted_articles]
        
        print("ğŸ” PythonAnywhere uyumlu haber Ã§ekme sistemi baÅŸlatÄ±lÄ±yor...")
        
        all_articles = []
        
        # 1. Ã–nce Ã¶zel haber kaynaklarÄ±ndan Ã§ek (news_sources.json)
        try:
            custom_articles = fetch_articles_from_custom_sources_pythonanywhere()
            if custom_articles:
                all_articles.extend(custom_articles)
                print(f"âœ… Ã–zel kaynaklardan {len(custom_articles)} makale bulundu")
        except Exception as custom_error:
            print(f"âš ï¸ Ã–zel kaynaklar hatasÄ±: {custom_error}")
        
        # 2. RSS Feed'lerden makale Ã§ek (sadece Ã¶zel kaynaklarda RSS yoksa)
        try:
            rss_articles = fetch_articles_from_rss_feeds()
            if rss_articles:
                all_articles.extend(rss_articles)
                print(f"âœ… RSS'den {len(rss_articles)} makale bulundu")
        except Exception as rss_error:
            print(f"âš ï¸ RSS Ã§ekme hatasÄ±: {rss_error}")
        
        # 3. Hacker News API'den AI ile ilgili haberleri Ã§ek
        try:
            hn_articles = fetch_articles_from_hackernews()
            if hn_articles:
                all_articles.extend(hn_articles)
                print(f"âœ… Hacker News'den {len(hn_articles)} makale bulundu")
        except Exception as hn_error:
            print(f"âš ï¸ Hacker News hatasÄ±: {hn_error}")
        
        # 4. Reddit API'den AI subreddit'lerinden makale Ã§ek
        try:
            reddit_articles = fetch_articles_from_reddit()
            if reddit_articles:
                all_articles.extend(reddit_articles)
                print(f"âœ… Reddit'den {len(reddit_articles)} makale bulundu")
        except Exception as reddit_error:
            print(f"âš ï¸ Reddit hatasÄ±: {reddit_error}")
        
        # Duplikat filtreleme
        unique_articles = []
        seen_hashes = set()
        seen_urls = set()
        
        for article in all_articles:
            article_hash = article.get('hash', '')
            article_url = article.get('url', '')
            
            # Zaten paylaÅŸÄ±lmÄ±ÅŸ mÄ± kontrol et
            if (article_hash not in posted_hashes and 
                article_url not in posted_urls and
                article_hash not in seen_hashes and
                article_url not in seen_urls):
                
                unique_articles.append(article)
                seen_hashes.add(article_hash)
                seen_urls.add(article_url)
        
        # En yeni makaleleri Ã¶nce getir
        unique_articles.sort(key=lambda x: x.get('fetch_date', ''), reverse=True)
        
        print(f"ğŸ“Š PythonAnywhere sistemi ile toplam {len(unique_articles)} benzersiz makale bulundu")
        
        return unique_articles[:10]  # En fazla 10 makale dÃ¶ndÃ¼r
        
    except Exception as e:
        print(f"âŒ PythonAnywhere haber Ã§ekme hatasÄ±: {e}")
        return []

def fetch_articles_from_custom_sources_pythonanywhere():
    """PythonAnywhere uyumlu Ã¶zel haber kaynaklarÄ± Ã§ekme"""
    try:
        config = load_news_sources()
        all_articles = []
        
        enabled_sources = [s for s in config["sources"] if s.get("enabled", True)]
        
        if not enabled_sources:
            print("âš ï¸ Aktif haber kaynaÄŸÄ± bulunamadÄ±")
            return []
        
        print(f"ğŸ” {len(enabled_sources)} Ã¶zel haber kaynaÄŸÄ±ndan makale Ã§ekiliyor (PythonAnywhere uyumlu)...")
        
        for source in enabled_sources:
            try:
                print(f"ğŸ“° {source['name']} kaynaÄŸÄ± kontrol ediliyor...")
                
                # PythonAnywhere uyumlu basit scraping kullan
                articles = fetch_articles_from_single_source_pythonanywhere(source)
                
                if articles:
                    all_articles.extend(articles)
                    source["article_count"] = len(articles)
                    source["success_rate"] = min(100, source.get("success_rate", 0) + 10)
                    print(f"âœ… {source['name']}: {len(articles)} makale bulundu")
                else:
                    source["success_rate"] = max(0, source.get("success_rate", 100) - 20)
                    print(f"âš ï¸ {source['name']}: Makale bulunamadÄ±")
                
                source["last_checked"] = datetime.now().isoformat()
                
            except Exception as e:
                print(f"âŒ {source['name']} hatasÄ±: {e}")
                source["success_rate"] = max(0, source.get("success_rate", 100) - 30)
                source["last_checked"] = datetime.now().isoformat()
        
        # GÃ¼ncellenmiÅŸ config'i kaydet
        try:
            save_json(NEWS_SOURCES_FILE, config)
        except Exception as save_error:
            print(f"âš ï¸ Haber kaynaklarÄ± kaydetme hatasÄ±: {save_error}")
        
        print(f"ğŸ“Š Ã–zel kaynaklardan toplam {len(all_articles)} makale bulundu")
        return all_articles
        
    except Exception as e:
        print(f"âŒ Ã–zel kaynaklar genel hatasÄ±: {e}")
        return []

def fetch_articles_from_single_source_pythonanywhere(source):
    """PythonAnywhere uyumlu tek kaynak makale Ã§ekme"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        # Timeout ile gÃ¼venli istek
        response = requests.get(source['url'], headers=headers, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Makale container'larÄ±nÄ± bul
        selectors = source.get('article_selectors', {})
        container_selector = selectors.get('container', 'article')
        
        articles = soup.select(container_selector)[:5]  # En fazla 5 makale
        
        if not articles:
            # Fallback selector'larÄ± dene
            fallback_selectors = ['article', '.post', '.news-item', '.article-item', 'li']
            for fallback in fallback_selectors:
                articles = soup.select(fallback)[:5]
                if articles:
                    break
        
        parsed_articles = []
        
        for article in articles:
            try:
                # BaÅŸlÄ±k bul
                title_selector = selectors.get('title', 'h1, h2, h3, .title')
                title_elem = article.select_one(title_selector)
                
                if not title_elem:
                    continue
                
                title = title_elem.get_text().strip()
                
                # Link bul
                link_selector = selectors.get('link', 'a')
                link_elem = article.select_one(link_selector)
                
                if not link_elem:
                    link_elem = title_elem.find('a') or title_elem.find_parent('a')
                
                if not link_elem:
                    continue
                
                link = link_elem.get('href', '')
                
                # TechCrunch iÃ§in Ã¶zel dÃ¼zeltme
                if 'techcrunch.com' in source['url'] and not link:
                    # BaÅŸlÄ±k iÃ§indeki a tag'Ä±nÄ± bul
                    title_link = title_elem.find('a')
                    if title_link:
                        link = title_link.get('href', '')
                
                # Relative URL'leri absolute yap
                if link.startswith('/'):
                    from urllib.parse import urljoin
                    link = urljoin(source['url'], link)
                elif not link.startswith('http'):
                    continue
                
                # AI ile ilgili mi kontrol et
                title_lower = title.lower()
                ai_keywords = ['ai', 'artificial intelligence', 'machine learning', 'deep learning', 'neural', 'gpt', 'llm', 'chatbot', 'automation', 'anthropic', 'openai', 'claude', 'gemini', 'copilot']
                is_ai_related = any(keyword in title_lower for keyword in ai_keywords)
                
                if not is_ai_related:
                    continue
                
                # Ã–zet bul
                excerpt_selector = selectors.get('excerpt', '.excerpt, .summary, p')
                excerpt_elem = article.select_one(excerpt_selector)
                excerpt = excerpt_elem.get_text().strip()[:500] if excerpt_elem else ""
                
                # Tarih bul
                date_selector = selectors.get('date', 'time, .date, .published')
                date_elem = article.select_one(date_selector)
                date_str = date_elem.get_text().strip() if date_elem else ""
                
                # Hash oluÅŸtur
                article_hash = hashlib.md5(title.encode()).hexdigest()
                
                parsed_articles.append({
                    "title": title,
                    "url": link,
                    "content": excerpt or title,
                    "excerpt": excerpt,
                    "date": date_str,
                    "hash": article_hash,
                    "source": f"{source['name']} (PA)",
                    "source_id": source["id"],
                    "fetch_date": datetime.now().isoformat(),
                    "is_new": True,
                    "already_posted": False
                })
                
            except Exception as article_error:
                print(f"âš ï¸ Makale parse hatasÄ±: {article_error}")
                continue
        
        return parsed_articles
        
    except Exception as e:
        print(f"âŒ {source.get('name', 'Bilinmeyen')} kaynak hatasÄ±: {e}")
        return []

def fetch_articles_with_rss_only():
    """Sadece RSS yÃ¶ntemi ile haber kaynaklarÄ±ndan makale Ã§ekme - Son 24 saat filtreli"""
    try:
        print("ğŸ” RSS yÃ¶ntemi ile haber Ã§ekme baÅŸlatÄ±lÄ±yor (Son 24 saat)...")
        
        # BugÃ¼nÃ¼n tarih ve saatini al
        now = datetime.now()
        twenty_four_hours_ago = now - timedelta(hours=24)
        
        print(f"ğŸ“… BugÃ¼n: {now.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"â° 24 saat Ã¶ncesi: {twenty_four_hours_ago.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # Ã–nce mevcut yayÄ±nlanan makaleleri yÃ¼kle
        posted_articles = load_json(HISTORY_FILE)
        posted_urls = [article.get('url', '') for article in posted_articles]
        posted_hashes = [article.get('hash', '') for article in posted_articles]
        
        # Son 24 saat iÃ§inde paylaÅŸÄ±lan makaleleri de kontrol et
        recent_posted_urls = []
        recent_posted_hashes = []
        
        for article in posted_articles:
            posted_date_str = article.get('posted_date') or article.get('fetch_date')
            if posted_date_str:
                try:
                    posted_date = datetime.fromisoformat(posted_date_str.replace('Z', '+00:00').replace('+00:00', ''))
                    if posted_date >= twenty_four_hours_ago:
                        recent_posted_urls.append(article.get('url', ''))
                        recent_posted_hashes.append(article.get('hash', ''))
                except Exception as date_error:
                    print(f"âš ï¸ Tarih parse hatasÄ±: {date_error}")
                    continue
        
        print(f"ğŸ“Š Son 24 saatte paylaÅŸÄ±lan makale sayÄ±sÄ±: {len(recent_posted_urls)}")
        
        # RSS kaynaklarÄ±nÄ± yÃ¼kle
        config = load_news_sources()
        rss_sources = config.get("rss_sources", [])
        enabled_rss_sources = [s for s in rss_sources if s.get("enabled", True)]
        
        if not enabled_rss_sources:
            print("âš ï¸ Aktif RSS kaynaÄŸÄ± bulunamadÄ±")
            return []
        
        print(f"ğŸ“° {len(enabled_rss_sources)} RSS kaynaÄŸÄ±ndan makale Ã§ekiliyor...")
        
        all_articles = []
        
        for rss_source in enabled_rss_sources:
            try:
                print(f"ğŸ” RSS Ã§ekiliyor: {rss_source['name']}")
                
                # RSS feed'i parse et
                import feedparser
                feed = feedparser.parse(rss_source['url'])
                
                if not feed.entries:
                    print(f"âš ï¸ {rss_source['name']}: RSS feed'de entry bulunamadÄ±")
                    rss_source["success_rate"] = max(0, rss_source.get("success_rate", 100) - 20)
                    continue
                
                source_articles = []
                
                for entry in feed.entries[:10]:  # Her feed'den en fazla 10 makale kontrol et
                    try:
                        title = getattr(entry, 'title', '')
                        url = getattr(entry, 'link', '')
                        
                        if not title or not url:
                            continue
                        
                        # URL kontrolÃ¼
                        if url in posted_urls or url in recent_posted_urls:
                            print(f"âœ… RSS makale zaten paylaÅŸÄ±lmÄ±ÅŸ: {title[:50]}...")
                            continue
                        
                        # Tarih kontrolÃ¼
                        entry_date = None
                        date_str = ""
                        
                        # RSS entry'den tarih al
                        if hasattr(entry, 'published_parsed') and entry.published_parsed:
                            try:
                                import time
                                entry_date = datetime(*entry.published_parsed[:6])
                                date_str = entry.published
                            except:
                                pass
                        elif hasattr(entry, 'published'):
                            try:
                                # RFC 2822 formatÄ±nÄ± parse et
                                from email.utils import parsedate_to_datetime
                                entry_date = parsedate_to_datetime(entry.published)
                                date_str = entry.published
                            except:
                                pass
                        
                        # 24 saat kontrolÃ¼
                        if entry_date:
                            if entry_date < twenty_four_hours_ago:
                                print(f"â° RSS makale 24 saatten eski: {entry_date.strftime('%Y-%m-%d %H:%M')} - {title[:50]}...")
                            continue
                        
                        # Ä°Ã§erik al
                        content = ""
                        if hasattr(entry, 'summary'):
                            content = entry.summary
                        elif hasattr(entry, 'description'):
                            content = entry.description
                        elif hasattr(entry, 'content'):
                            if isinstance(entry.content, list) and len(entry.content) > 0:
                                content = entry.content[0].value
                            else:
                                content = str(entry.content)
                        
                        # HTML etiketlerini temizle
                        if content:
                            from bs4 import BeautifulSoup
                            content = BeautifulSoup(content, 'html.parser').get_text()
                            content = ' '.join(content.split())[:2000]
                        
                        # EÄŸer iÃ§erik yoksa baÅŸlÄ±ÄŸÄ± kullan
                        if not content:
                            content = title
                        
                        # Hash oluÅŸtur
                        article_hash = hashlib.md5(title.encode()).hexdigest()
                        
                        # Tekrar kontrolÃ¼
                        if (article_hash not in posted_hashes and 
                            article_hash not in recent_posted_hashes):
                            
                            source_articles.append({
                            "title": title,
                            "url": url,
                                "content": content,
                            "hash": article_hash,
                            "fetch_date": datetime.now().isoformat(),
                            "is_new": True,
                                "already_posted": False,
                                "source": f"RSS - {rss_source['name']}",
                                "source_id": rss_source["id"],
                                "article_date": entry_date.isoformat() if entry_date else datetime.now().isoformat(),
                                "is_within_24h": True,
                                "rss_published": date_str
                            })
                            print(f"ğŸ†• RSS ile yeni makale (24h iÃ§inde): {title[:50]}...")
                        else:
                            if article_hash in recent_posted_hashes:
                                print(f"â° Son 24 saatte paylaÅŸÄ±lmÄ±ÅŸ: {title[:50]}...")
                            else:
                                print(f"âœ… Makale zaten paylaÅŸÄ±lmÄ±ÅŸ: {title[:50]}...")
                        
                        # En fazla 5 makale al
                        if len(source_articles) >= 5:
                            break
                        
                    except Exception as entry_error:
                        print(f"âš ï¸ RSS entry hatasÄ±: {entry_error}")
                        continue
                        
                if source_articles:
                    all_articles.extend(source_articles)
                    rss_source["article_count"] = len(source_articles)
                    rss_source["success_rate"] = min(100, rss_source.get("success_rate", 0) + 10)
                    print(f"âœ… {rss_source['name']}: {len(source_articles)} yeni makale bulundu")
                else:
                    rss_source["success_rate"] = max(0, rss_source.get("success_rate", 100) - 10)
                    print(f"âš ï¸ {rss_source['name']}: Yeni makale bulunamadÄ±")
                
                rss_source["last_checked"] = datetime.now().isoformat()
                
            except Exception as source_error:
                print(f"âŒ {rss_source['name']} RSS hatasÄ±: {source_error}")
                rss_source["success_rate"] = max(0, rss_source.get("success_rate", 100) - 30)
                rss_source["last_checked"] = datetime.now().isoformat()
        
        # GÃ¼ncellenmiÅŸ config'i kaydet
        try:
            save_json(NEWS_SOURCES_FILE, config)
        except Exception as save_error:
            print(f"âš ï¸ RSS kaynaklarÄ± kaydetme hatasÄ±: {save_error}")
        
        print(f"ğŸ“Š RSS ile toplam {len(all_articles)} yeni makale bulundu (Son 24 saat filtreli)")
        
        # Duplikat filtreleme uygula
        if all_articles:
            all_articles = filter_duplicate_articles(all_articles)
            print(f"ğŸ”„ Duplikat filtreleme sonrasÄ±: {len(all_articles)} benzersiz makale")
        
        # 24 saat iÃ§indeki makaleleri iÅŸaretle
        for article in all_articles:
            article['filtered_by_24h'] = True
            article['filter_applied_at'] = datetime.now().isoformat()
            article['method'] = 'rss'
        
        return all_articles
        
    except ImportError:
        print("âš ï¸ feedparser modÃ¼lÃ¼ bulunamadÄ±, RSS atlanÄ±yor")
        return []
    except Exception as e:
        print(f"âŒ RSS haber Ã§ekme genel hatasÄ±: {e}")
        return []

def fetch_articles_hybrid_mcp_rss():
    """Hibrit sistem: MCP + RSS fallback ile haber Ã§ekme"""
    try:
        print("ğŸ”„ Hibrit haber Ã§ekme sistemi baÅŸlatÄ±lÄ±yor (MCP + RSS)...")
        
        all_articles = []
        
        # 1. Ã–nce MCP ile dene
        try:
            print("ğŸ¤– MCP yÃ¶ntemi deneniyor...")
            mcp_articles = fetch_articles_with_mcp_only()
            
            if mcp_articles:
                all_articles.extend(mcp_articles)
                print(f"âœ… MCP ile {len(mcp_articles)} makale bulundu")
            else:
                print("âš ï¸ MCP ile makale bulunamadÄ±")
                
        except Exception as mcp_error:
            print(f"âŒ MCP hatasÄ±: {mcp_error}")
        
        # 2. EÄŸer MCP'den yeterli makale gelmezse RSS dene
        if len(all_articles) < 3:  # 3'ten az makale varsa RSS'yi de dene
            try:
                print("ğŸ“¡ RSS yÃ¶ntemi devreye giriyor...")
                rss_articles = fetch_articles_with_rss_only()
                
                if rss_articles:
                    # RSS makalelerini ekle (duplikat kontrolÃ¼ ile)
                    existing_urls = [article.get('url', '') for article in all_articles]
                    existing_hashes = [article.get('hash', '') for article in all_articles]
                    
                    new_rss_articles = []
                    for rss_article in rss_articles:
                        if (rss_article.get('url', '') not in existing_urls and 
                            rss_article.get('hash', '') not in existing_hashes):
                            new_rss_articles.append(rss_article)
                    
                    all_articles.extend(new_rss_articles)
                    print(f"âœ… RSS ile {len(new_rss_articles)} ek makale bulundu")
                else:
                    print("âš ï¸ RSS ile de makale bulunamadÄ±")
                    
            except Exception as rss_error:
                print(f"âŒ RSS hatasÄ±: {rss_error}")
        else:
            print(f"âœ… MCP'den yeterli makale var ({len(all_articles)}), RSS atlanÄ±yor")
        
        # 3. SonuÃ§larÄ± birleÅŸtir ve filtrele
        if all_articles:
            # Duplikat filtreleme
            all_articles = filter_duplicate_articles(all_articles)
            
            # Hibrit iÅŸareti ekle
            for article in all_articles:
                article['hybrid_method'] = True
                article['methods_used'] = 'MCP+RSS' if any('RSS' in a.get('source', '') for a in all_articles) else 'MCP'
            
            print(f"ğŸ¯ Hibrit sistem sonucu: {len(all_articles)} benzersiz makale")
            
            # Kaynak daÄŸÄ±lÄ±mÄ±nÄ± gÃ¶ster
            mcp_count = len([a for a in all_articles if 'MCP' in a.get('source', '')])
            rss_count = len([a for a in all_articles if 'RSS' in a.get('source', '')])
            print(f"ğŸ“Š Kaynak daÄŸÄ±lÄ±mÄ±: MCP={mcp_count}, RSS={rss_count}")
            
        return all_articles
        
    except Exception as e:
        print(f"âŒ Hibrit sistem hatasÄ±: {e}")
        return []

def fetch_articles_from_rss_feeds():
    """Eski RSS fonksiyonu - geriye uyumluluk iÃ§in"""
    return fetch_articles_with_rss_only()

def fetch_articles_with_simple_scraping():
    """Basit web scraping ile makale Ã§ek"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        # Scraping hedefleri
        scraping_targets = [
            {
                "url": "https://techcrunch.com/category/artificial-intelligence/",
                "name": "TechCrunch AI",
                "article_selector": "article.post-block",
                "title_selector": "h2.post-block__title a",
                "link_selector": "h2.post-block__title a",
                "excerpt_selector": ".post-block__content"
            },
            {
                "url": "https://www.theverge.com/ai-artificial-intelligence",
                "name": "The Verge AI",
                "article_selector": "article",
                "title_selector": "h2 a",
                "link_selector": "h2 a",
                "excerpt_selector": "p"
            }
        ]
        
        all_articles = []
        
        for target in scraping_targets:
            try:
                print(f"ğŸ” Web scraping: {target['name']}")
                
                response = requests.get(target['url'], headers=headers, timeout=15)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'html.parser')
                articles = soup.select(target['article_selector'])[:5]
                
                for article in articles:
                    try:
                        title_elem = article.select_one(target['title_selector'])
                        if not title_elem:
                            continue
                        
                        title = title_elem.get_text().strip()
                        link = title_elem.get('href', '')
                        
                        # Relative URL'leri absolute yap
                        if link.startswith('/'):
                            from urllib.parse import urljoin
                            link = urljoin(target['url'], link)
                        
                        # Excerpt al
                        excerpt = ""
                        excerpt_elem = article.select_one(target['excerpt_selector'])
                        if excerpt_elem:
                            excerpt = excerpt_elem.get_text().strip()[:500]
                        
                        # Hash oluÅŸtur
                        article_hash = hashlib.md5(title.encode()).hexdigest()
                        
                        all_articles.append({
                            "title": title,
                            "url": link,
                            "content": excerpt or title,
                            "excerpt": excerpt,
                            "hash": article_hash,
                            "source": f"Scraping - {target['name']}",
                            "fetch_date": datetime.now().isoformat(),
                            "is_new": True,
                            "already_posted": False
                        })
                        
                    except Exception as article_error:
                        print(f"âš ï¸ Makale parse hatasÄ±: {article_error}")
                        continue
                        
            except Exception as target_error:
                print(f"âš ï¸ Scraping hatasÄ± ({target['name']}): {target_error}")
                continue
        
        return all_articles
        
    except Exception as e:
        print(f"âŒ Web scraping genel hatasÄ±: {e}")
        return []

def fetch_articles_from_hackernews():
    """Hacker News API'den AI ile ilgili haberleri Ã§ek - GeliÅŸmiÅŸ iÃ§erik Ã§ekme ile"""
    try:
        # Hacker News API'den top stories al
        top_stories_url = "https://hacker-news.firebaseio.com/v0/topstories.json"
        response = requests.get(top_stories_url, timeout=10)
        story_ids = response.json()[:50]  # Ä°lk 50 hikaye
        
        ai_articles = []
        ai_keywords = ['ai', 'artificial intelligence', 'machine learning', 'deep learning', 'neural', 'gpt', 'llm', 'openai', 'anthropic', 'claude', 'chatgpt']
        
        for story_id in story_ids[:20]:  # Ä°lk 20'sini kontrol et
            try:
                story_url = f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
                story_response = requests.get(story_url, timeout=5)
                story = story_response.json()
                
                if not story or story.get('type') != 'story':
                    continue
                
                title = story.get('title', '')
                url = story.get('url', '')
                
                # AI ile ilgili mi kontrol et - Daha sÄ±kÄ± filtreleme
                title_lower = title.lower()
                is_ai_related = any(keyword in title_lower for keyword in ai_keywords)
                
                # AI ile ilgili olmayan konularÄ± filtrele
                non_ai_keywords = ['wood', 'dried', 'kiln', 'furniture', 'cooking', 'recipe', 'travel', 'music', 'art', 'painting', 'photography']
                has_non_ai_content = any(keyword in title_lower for keyword in non_ai_keywords)
                
                if not is_ai_related or not url or has_non_ai_content:
                    if has_non_ai_content:
                        terminal_log(f"âš ï¸ AI olmayan iÃ§erik filtrelendi: {title[:50]}...", "warning")
                    continue
                
                # GerÃ§ek makale iÃ§eriÄŸini Ã§ekmeye Ã§alÄ±ÅŸ
                article_content = title  # Fallback olarak baÅŸlÄ±k
                
                try:
                    # GeliÅŸmiÅŸ scraper ile iÃ§erik Ã§ek
                    content_result = advanced_web_scraper(url, wait_time=2, use_js=False)
                    
                    if content_result.get("success") and content_result.get("content"):
                        scraped_content = content_result["content"]
                        
                        # Ä°Ã§eriÄŸi temizle ve kÄ±salt
                        if len(scraped_content) > 500:
                            article_content = scraped_content[:500] + "..."
                        else:
                            article_content = scraped_content
                        
                        terminal_log(f"âœ… HN makale iÃ§eriÄŸi Ã§ekildi: {title[:50]}... ({len(scraped_content)} karakter)", "success")
                    else:
                        # MCP fallback dene
                        try:
                            mcp_result = mcp_firecrawl_scrape({
                                "url": url,
                                "formats": ["markdown"],
                                "onlyMainContent": True,
                                "waitFor": 1000
                            })
                            
                            if mcp_result.get("success") and mcp_result.get("content"):
                                mcp_content = mcp_result["content"]
                                if len(mcp_content) > 500:
                                    article_content = mcp_content[:500] + "..."
                                else:
                                    article_content = mcp_content
                                
                                terminal_log(f"âœ… HN makale iÃ§eriÄŸi MCP ile Ã§ekildi: {title[:50]}...", "success")
                            else:
                                terminal_log(f"âš ï¸ HN makale iÃ§eriÄŸi Ã§ekilemedi, baÅŸlÄ±k kullanÄ±lÄ±yor: {title[:50]}...", "warning")
                                
                        except Exception as mcp_error:
                            terminal_log(f"âš ï¸ HN MCP fallback hatasÄ±: {mcp_error}", "warning")
                            
                except Exception as content_error:
                    terminal_log(f"âš ï¸ HN iÃ§erik Ã§ekme hatasÄ±: {content_error}", "warning")
                
                # Hash oluÅŸtur
                article_hash = hashlib.md5(title.encode()).hexdigest()
                
                ai_articles.append({
                    "title": title,
                    "url": url,
                    "content": article_content,  # GerÃ§ek iÃ§erik veya baÅŸlÄ±k
                    "score": story.get('score', 0),
                    "hash": article_hash,
                    "source": "Hacker News",
                    "fetch_date": datetime.now().isoformat(),
                    "is_new": True,
                    "already_posted": False
                })
                
                if len(ai_articles) >= 5:  # En fazla 5 makale
                    break
                    
            except Exception as story_error:
                terminal_log(f"âš ï¸ HN story hatasÄ±: {story_error}", "warning")
                continue
        
        terminal_log(f"ğŸ“Š Hacker News'den {len(ai_articles)} AI makalesi bulundu", "info")
        return ai_articles
        
    except Exception as e:
        terminal_log(f"âŒ Hacker News API hatasÄ±: {e}", "error")
        return []

def fetch_articles_from_reddit():
    """Reddit'den AI subreddit'lerinden makale Ã§ek - GeliÅŸmiÅŸ iÃ§erik Ã§ekme ile"""
    try:
        # Reddit JSON API kullan (auth gerektirmez)
        subreddits = ['artificial', 'MachineLearning', 'deeplearning', 'singularity']
        all_articles = []
        
        for subreddit in subreddits:
            try:
                url = f"https://www.reddit.com/r/{subreddit}/hot.json?limit=10"
                headers = {'User-Agent': 'AI News Bot 1.0'}
                
                response = requests.get(url, headers=headers, timeout=10)
                data = response.json()
                
                posts = data.get('data', {}).get('children', [])
                
                for post in posts[:3]:  # Her subreddit'den 3 post
                    try:
                        post_data = post.get('data', {})
                        
                        title = post_data.get('title', '')
                        url = post_data.get('url', '')
                        selftext = post_data.get('selftext', '')
                        score = post_data.get('score', 0)
                        
                        # Sadece external link'leri al (reddit post'larÄ± deÄŸil)
                        if not url or 'reddit.com' in url or score < 10:
                            continue
                        
                        # Ä°Ã§erik oluÅŸtur - Ã–nce selftext, sonra gerÃ§ek makale iÃ§eriÄŸi
                        article_content = selftext[:500] if selftext else title
                        
                        # EÄŸer selftext yoksa veya Ã§ok kÄ±saysa, gerÃ§ek makale iÃ§eriÄŸini Ã§ekmeye Ã§alÄ±ÅŸ
                        if not selftext or len(selftext) < 100:
                            try:
                                # GeliÅŸmiÅŸ scraper ile iÃ§erik Ã§ek
                                content_result = advanced_web_scraper(url, wait_time=2, use_js=False)
                                
                                if content_result.get("success") and content_result.get("content"):
                                    scraped_content = content_result["content"]
                                    
                                    # Ä°Ã§eriÄŸi temizle ve kÄ±salt
                                    if len(scraped_content) > 500:
                                        article_content = scraped_content[:500] + "..."
                                    else:
                                        article_content = scraped_content
                                    
                                    terminal_log(f"âœ… Reddit makale iÃ§eriÄŸi Ã§ekildi: {title[:50]}... ({len(scraped_content)} karakter)", "success")
                                else:
                                    # MCP fallback dene
                                    try:
                                        mcp_result = mcp_firecrawl_scrape({
                                            "url": url,
                                            "formats": ["markdown"],
                                            "onlyMainContent": True,
                                            "waitFor": 1000
                                        })
                                        
                                        if mcp_result.get("success") and mcp_result.get("content"):
                                            mcp_content = mcp_result["content"]
                                            if len(mcp_content) > 500:
                                                article_content = mcp_content[:500] + "..."
                                            else:
                                                article_content = mcp_content
                                            
                                            terminal_log(f"âœ… Reddit makale iÃ§eriÄŸi MCP ile Ã§ekildi: {title[:50]}...", "success")
                                        else:
                                            terminal_log(f"âš ï¸ Reddit makale iÃ§eriÄŸi Ã§ekilemedi, baÅŸlÄ±k kullanÄ±lÄ±yor: {title[:50]}...", "warning")
                                            
                                    except Exception as mcp_error:
                                        terminal_log(f"âš ï¸ Reddit MCP fallback hatasÄ±: {mcp_error}", "warning")
                                        
                            except Exception as content_error:
                                terminal_log(f"âš ï¸ Reddit iÃ§erik Ã§ekme hatasÄ±: {content_error}", "warning")
                        
                        # Hash oluÅŸtur
                        article_hash = hashlib.md5(title.encode()).hexdigest()
                        
                        all_articles.append({
                            "title": title,
                            "url": url,
                            "content": article_content,
                            "score": score,
                            "hash": article_hash,
                            "source": f"Reddit - r/{subreddit}",
                            "fetch_date": datetime.now().isoformat(),
                            "is_new": True,
                            "already_posted": False
                        })
                        
                    except Exception as post_error:
                        terminal_log(f"âš ï¸ Reddit post hatasÄ±: {post_error}", "warning")
                        continue
                        
            except Exception as subreddit_error:
                terminal_log(f"âš ï¸ Reddit subreddit hatasÄ± ({subreddit}): {subreddit_error}", "warning")
                continue
        
        terminal_log(f"ğŸ“Š Reddit'den {len(all_articles)} AI makalesi bulundu", "info")
        return all_articles
        
    except Exception as e:
        terminal_log(f"âŒ Reddit API hatasÄ±: {e}", "error")
        return []

# ==========================================
# HABER Ã‡EKME YÃ–NTEMÄ° SEÃ‡Ä°CÄ°
# ==========================================

def get_news_fetching_method():
    """Ayarlardan haber Ã§ekme yÃ¶ntemini al"""
    try:
        # Automation settings'den kontrol et
        settings = load_automation_settings()
        method = settings.get('news_fetching_method', 'auto')
        
        # MCP config'den de kontrol et
        mcp_config = load_json(MCP_CONFIG_FILE) if os.path.exists(MCP_CONFIG_FILE) else {}
        mcp_enabled = mcp_config.get('mcp_enabled', False)
        
        return {
            'method': method,
            'mcp_enabled': mcp_enabled,
            'available_methods': ['auto', 'mcp_only', 'pythonanywhere_only', 'custom_sources_only']
        }
    except Exception as e:
        print(f"Haber Ã§ekme yÃ¶ntemi alma hatasÄ±: {e}")
        return {
            'method': 'auto',
            'mcp_enabled': False,
            'available_methods': ['auto', 'mcp_only', 'pythonanywhere_only', 'custom_sources_only']
        }

def fetch_latest_ai_articles_smart():
    """AkÄ±llÄ± haber Ã§ekme - Ayarlara gÃ¶re yÃ¶ntem seÃ§er"""
    try:
        method_info = get_news_fetching_method()
        method = method_info['method']
        mcp_enabled = method_info['mcp_enabled']
        
        print(f"ğŸ¯ Haber Ã§ekme yÃ¶ntemi: {method} (MCP: {'Aktif' if mcp_enabled else 'Pasif'})")
        
        if method == 'mcp_only' and mcp_enabled:
            # Sadece MCP kullan
            return fetch_latest_ai_articles_with_firecrawl()
            
        elif method == 'pythonanywhere_only':
            # Sadece PythonAnywhere uyumlu sistem kullan
            return fetch_latest_ai_articles_pythonanywhere()
            
        elif method == 'custom_sources_only':
            # Sadece Ã¶zel kaynaklarÄ± kullan
            return fetch_articles_from_custom_sources()
            
        else:  # method == 'auto'
            # Otomatik seÃ§im - Ã–ncelik sÄ±rasÄ±na gÃ¶re dene
            all_articles = []
            
            # 1. Ã–nce Ã¶zel kaynaklarÄ± dene
            try:
                custom_articles = fetch_articles_from_custom_sources()
                if custom_articles:
                    all_articles.extend(custom_articles)
                    print(f"âœ… Ã–zel kaynaklardan {len(custom_articles)} makale")
            except Exception as e:
                print(f"âš ï¸ Ã–zel kaynaklar hatasÄ±: {e}")
            
            # 2. PythonAnywhere sistemini dene
            try:
                pa_articles = fetch_latest_ai_articles_pythonanywhere()
                if pa_articles:
                    all_articles.extend(pa_articles)
                    print(f"âœ… PythonAnywhere sisteminden {len(pa_articles)} makale")
            except Exception as e:
                print(f"âš ï¸ PythonAnywhere sistemi hatasÄ±: {e}")
            
            # 3. MCP varsa onu da dene
            if mcp_enabled:
                try:
                    mcp_articles = fetch_latest_ai_articles_with_firecrawl()
                    if mcp_articles:
                        all_articles.extend(mcp_articles)
                        print(f"âœ… MCP'den {len(mcp_articles)} makale")
                except Exception as e:
                    print(f"âš ï¸ MCP hatasÄ±: {e}")
            
            # 4. Son Ã§are fallback
            if not all_articles:
                try:
                    fallback_articles = fetch_latest_ai_articles_fallback()
                    if fallback_articles:
                        all_articles.extend(fallback_articles)
                        print(f"âœ… Fallback'den {len(fallback_articles)} makale")
                except Exception as e:
                    print(f"âš ï¸ Fallback hatasÄ±: {e}")
            
            # Duplikat temizleme
            if all_articles:
                unique_articles = filter_duplicate_articles(all_articles)
                print(f"ğŸ“Š Toplam {len(unique_articles)} benzersiz makale bulundu")
                return unique_articles[:10]
            
            return []
        
    except Exception as e:
        print(f"âŒ AkÄ±llÄ± haber Ã§ekme hatasÄ±: {e}")
        # Son Ã§are fallback
        return fetch_latest_ai_articles_fallback()

def fetch_articles_with_mcp_only():
    """Sadece MCP yÃ¶ntemi ile haber kaynaklarÄ±ndan makale Ã§ekme - Son 24 saat filtreli"""
    try:
        print("ğŸ” MCP yÃ¶ntemi ile haber Ã§ekme baÅŸlatÄ±lÄ±yor (Son 24 saat)...")
        
        # BugÃ¼nÃ¼n tarih ve saatini al
        now = datetime.now()
        twenty_four_hours_ago = now - timedelta(hours=24)
        
        print(f"ğŸ“… BugÃ¼n: {now.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"â° 24 saat Ã¶ncesi: {twenty_four_hours_ago.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # Ã–nce mevcut yayÄ±nlanan makaleleri yÃ¼kle
        posted_articles = load_json(HISTORY_FILE)
        posted_urls = [article.get('url', '') for article in posted_articles]
        posted_hashes = [article.get('hash', '') for article in posted_articles]
        
        # Son 24 saat iÃ§inde paylaÅŸÄ±lan makaleleri de kontrol et
        recent_posted_urls = []
        recent_posted_hashes = []
        
        for article in posted_articles:
            posted_date_str = article.get('posted_date') or article.get('fetch_date')
            if posted_date_str:
                try:
                    # ISO format tarih parse et
                    posted_date = datetime.fromisoformat(posted_date_str.replace('Z', '+00:00').replace('+00:00', ''))
                    if posted_date >= twenty_four_hours_ago:
                        recent_posted_urls.append(article.get('url', ''))
                        recent_posted_hashes.append(article.get('hash', ''))
                except Exception as date_error:
                    print(f"âš ï¸ Tarih parse hatasÄ±: {date_error}")
                    continue
        
        print(f"ğŸ“Š Son 24 saatte paylaÅŸÄ±lan makale sayÄ±sÄ±: {len(recent_posted_urls)}")
        
        # Haber kaynaklarÄ±nÄ± yÃ¼kle
        config = load_news_sources()
        enabled_sources = [s for s in config.get("sources", []) if s.get("enabled", True)]
        
        if not enabled_sources:
            print("âš ï¸ Aktif haber kaynaÄŸÄ± bulunamadÄ±")
            return []
        
        print(f"ğŸ“° {len(enabled_sources)} haber kaynaÄŸÄ±ndan MCP ile makale Ã§ekiliyor...")
        
        all_articles = []
        
        for source in enabled_sources:
            try:
                print(f"ğŸ” MCP ile Ã§ekiliyor: {source['name']}")
                
                # GeliÅŸmiÅŸ scraper ile ana sayfa Ã§ek (MCP fallback)
                try:
                    scrape_result = advanced_web_scraper(source['url'], wait_time=5, use_js=True, return_html=True)
                    
                    if not scrape_result or 'html' not in scrape_result:
                        print(f"[MCP] GeliÅŸmiÅŸ scraper deneniyor (JS: True)...")
                        scrape_result = mcp_firecrawl_scrape({
                            "url": source['url'],
                            "formats": ["markdown", "links"],
                            "onlyMainContent": True,
                            "waitFor": 2000
                        })
                        
                        if not scrape_result.get("success", False):
                            print(f"âš ï¸ {source['name']} MCP ile Ã§ekilemedi")
                            source["success_rate"] = max(0, source.get("success_rate", 100) - 20)
                            continue
                        
                        # Markdown iÃ§eriÄŸinden makale linklerini Ã§Ä±kar
                        markdown_content = scrape_result.get("markdown", "")
                        print(f"[MCP] Firecrawl scrape baÅŸarÄ±lÄ±: {len(markdown_content)} karakter (firecrawl)")
                    else:
                        # HTML'den BeautifulSoup ile parse et
                        html_content = scrape_result['html']
                        soup = BeautifulSoup(html_content, 'html.parser')
                        
                                                 # TechCrunch iÃ§in Ã¶zel parsing
                        if 'techcrunch.com' in source['url']:
                            # .loop-card elementlerini bul
                            loop_cards = soup.select('.loop-card')
                            print(f"[MCP] TechCrunch: {len(loop_cards)} loop-card bulundu")
                            
                            article_urls = []
                            for card in loop_cards:
                                title_link = card.select_one('h3 a, h2 a, .loop-card__title a')
                                if title_link:
                                    href = title_link.get('href', '')
                                    title = title_link.get_text().strip()
                                    
                                    if href and title:
                                        if href.startswith('/'):
                                            href = 'https://techcrunch.com' + href
                                        article_urls.append(href)
                                        print(f"   ğŸ“° {title[:50]}... -> {href}")
                            
                            # TechCrunch kategorilerini de kontrol et
                            category_links = soup.find_all('a', href=True)
                            for link in category_links:
                                href = link.get('href', '')
                                text = link.get_text().strip()
                                
                                # AI, Apps, Robotics gibi kategorilerdeki makaleleri de al
                                if (href and 'techcrunch.com' in href and 
                                    ('2025' in href or '2024' in href) and
                                    text and len(text) > 15 and
                                    href not in article_urls):
                                    
                                    # AI ile ilgili kategorileri kontrol et
                                    ai_keywords = ['ai', 'artificial', 'machine learning', 'robotics', 'automation', 'chatgpt', 'openai']
                                    if any(keyword in text.lower() for keyword in ai_keywords):
                                        article_urls.append(href)
                                        print(f"   ğŸ¤– Kategori makalesi: {text[:50]}... -> {href}")
                            
                            # Markdown content simÃ¼lasyonu
                            markdown_content = '\n'.join([f"[Article]({url})" for url in article_urls])
                        else:
                            # DiÄŸer siteler iÃ§in genel parsing
                            all_links = soup.find_all('a', href=True)
                            article_urls = []
                            
                            for link in all_links:
                                href = link.get('href', '')
                                text = link.get_text().strip()
                                
                                if (href and text and len(text) > 15 and
                                    ('2025' in href or '2024' in href) and
                                    source['url'].split('/')[2] in href):
                                    
                                    if href.startswith('/'):
                                        base_url = f"https://{source['url'].split('/')[2]}"
                                        href = base_url + href
                                    
                                    article_urls.append(href)
                            
                            markdown_content = '\n'.join([f"[Article]({url})" for url in article_urls])
                        
                        print(f"[MCP] GeliÅŸmiÅŸ scraper baÅŸarÄ±lÄ±: {len(markdown_content)} karakter (selenium)")
                        
                except Exception as scraper_error:
                    print(f"âŒ Scraper hatasÄ±: {scraper_error}")
                    continue
                
                # Makale URL'lerini bul
                import re
                
                # Kaynak URL'sinin domain'ini al
                from urllib.parse import urlparse
                source_domain = urlparse(source['url']).netloc
                
                # Bu domain'e ait makale URL'lerini bul
                url_patterns = [
                    rf'https?://{re.escape(source_domain)}/[^\s\)\]]+',
                    rf'https?://www\.{re.escape(source_domain)}/[^\s\)\]]+',
                ]
                
                article_urls = []
                for pattern in url_patterns:
                    found_urls = re.findall(pattern, markdown_content)
                    article_urls.extend(found_urls)
                
                # URL'leri temizle ve filtrele (24 saat kontrolÃ¼ ile)
                clean_urls = []
                for url in article_urls:
                    url = url.rstrip(')')
                    
                    # Makale URL'si olup olmadÄ±ÄŸÄ±nÄ± kontrol et
                    if (url not in posted_urls and 
                        url not in recent_posted_urls and  # Son 24 saat kontrolÃ¼
                        url not in clean_urls and
                        len(url) > 30 and  # Ã‡ok kÄ±sa URL'leri filtrele
                        not any(skip in url.lower() for skip in ['category', 'tag', 'author', 'page', 'search'])):
                        
                        # URL'den makale tarihini Ã§Ä±karmaya Ã§alÄ±ÅŸ (TechCrunch, The Verge gibi siteler iÃ§in)
                        is_recent_article = check_article_url_date(url, twenty_four_hours_ago)
                        
                        # 24 saat filtresi yerine 48 saat (2 gÃ¼n) yapalÄ±m - daha fazla makale iÃ§in
                        forty_eight_hours_ago = now - timedelta(hours=48)
                        is_recent_48h = check_article_url_date(url, forty_eight_hours_ago)
                        
                        if is_recent_48h:  # 48 saat iÃ§indeki makaleleri al
                            clean_urls.append(url)
                        else:
                            print(f"â° Eski makale atlandÄ± (48h+): {url}")
                
                # Makale sayÄ±sÄ±nÄ± artÄ±r (5 -> 15)
                clean_urls = clean_urls[:15]
                print(f"ğŸ”— {source['name']}: {len(clean_urls)} makale URL'si bulundu")
                
                # Her makaleyi MCP ile Ã§ek
                source_articles = []
                for url in clean_urls:
                    try:
                        article_content = fetch_article_content_with_mcp_only(url)
                        
                        if article_content and len(article_content.get("content", "")) > 100:
                            title = article_content.get("title", "")
                            content = article_content.get("content", "")
                            publish_date = article_content.get("publish_date")
                            
                            # Makale yayÄ±n tarihini kontrol et
                            is_article_recent = True
                            if publish_date:
                                try:
                                    article_pub_date = datetime.fromisoformat(publish_date.replace('Z', ''))
                                    is_article_recent = article_pub_date >= twenty_four_hours_ago
                                    
                                    if not is_article_recent:
                                        print(f"â° Makale 24 saatten eski: {article_pub_date.strftime('%Y-%m-%d %H:%M')} - {title[:50]}...")
                                        continue
                                except Exception as date_error:
                                    print(f"âš ï¸ Makale tarih parse hatasÄ±: {date_error}")
                            
                            # Makale hash'i oluÅŸtur
                            article_hash = hashlib.md5(title.encode()).hexdigest()
                            
                            # Tekrar kontrolÃ¼ (hem genel hem de son 24 saat)
                            if (article_hash not in posted_hashes and 
                                article_hash not in recent_posted_hashes and
                                is_article_recent):
                                
                                source_articles.append({
                                    "title": title,
                                    "url": url,
                                    "content": content,
                                    "hash": article_hash,
                                    "fetch_date": datetime.now().isoformat(),
                                    "is_new": True,
                                    "already_posted": False,
                                    "source": f"MCP - {source['name']}",
                                    "source_id": source["id"],
                                    "article_date": publish_date or datetime.now().isoformat(),
                                    "is_within_24h": True
                                })
                                print(f"ğŸ†• MCP ile yeni makale (24h iÃ§inde): {title[:50]}...")
                            else:
                                if article_hash in recent_posted_hashes:
                                    print(f"â° Son 24 saatte paylaÅŸÄ±lmÄ±ÅŸ: {title[:50]}...")
                                elif not is_article_recent:
                                    print(f"ğŸ“… 24 saatten eski makale: {title[:50]}...")
                                else:
                                    print(f"âœ… Makale zaten paylaÅŸÄ±lmÄ±ÅŸ: {title[:50]}...")
                        else:
                            print(f"âš ï¸ Ä°Ã§erik yetersiz: {url}")
                            
                    except Exception as article_error:
                        print(f"âŒ Makale Ã§ekme hatasÄ± ({url}): {article_error}")
                        continue
                
                if source_articles:
                    all_articles.extend(source_articles)
                    source["article_count"] = len(source_articles)
                    source["success_rate"] = min(100, source.get("success_rate", 0) + 10)
                    print(f"âœ… {source['name']}: {len(source_articles)} yeni makale bulundu")
                else:
                    source["success_rate"] = max(0, source.get("success_rate", 100) - 10)
                    print(f"âš ï¸ {source['name']}: Yeni makale bulunamadÄ±")
                
                source["last_checked"] = datetime.now().isoformat()
                
            except Exception as source_error:
                print(f"âŒ {source['name']} kaynak hatasÄ±: {source_error}")
                source["success_rate"] = max(0, source.get("success_rate", 100) - 30)
                source["last_checked"] = datetime.now().isoformat()
        
        # GÃ¼ncellenmiÅŸ config'i kaydet
        try:
            save_json(NEWS_SOURCES_FILE, config)
        except Exception as save_error:
            print(f"âš ï¸ Haber kaynaklarÄ± kaydetme hatasÄ±: {save_error}")
        
        print(f"ğŸ“Š MCP ile toplam {len(all_articles)} yeni makale bulundu (Son 24 saat filtreli)")
        
        # Duplikat filtreleme uygula
        if all_articles:
            all_articles = filter_duplicate_articles(all_articles)
            print(f"ğŸ”„ Duplikat filtreleme sonrasÄ±: {len(all_articles)} benzersiz makale")
        
        # 24 saat iÃ§indeki makaleleri iÅŸaretle
        for article in all_articles:
            article['filtered_by_24h'] = True
            article['filter_applied_at'] = datetime.now().isoformat()
        
        return all_articles
        
    except Exception as e:
        print(f"âŒ MCP haber Ã§ekme genel hatasÄ±: {e}")
        return []

def check_article_url_date(url, cutoff_date):
    """URL'den makale tarihini Ã§Ä±karÄ±p son 24 saat iÃ§inde olup olmadÄ±ÄŸÄ±nÄ± kontrol et"""
    try:
        import re
        
        # TechCrunch URL formatÄ±: https://techcrunch.com/2025/01/09/article-name/
        techcrunch_pattern = r'/(\d{4})/(\d{2})/(\d{2})/'
        match = re.search(techcrunch_pattern, url)
        
        if match:
            year, month, day = match.groups()
            try:
                article_date = datetime(int(year), int(month), int(day))
                
                # Makale tarihi son 24 saat iÃ§inde mi?
                is_recent = article_date >= cutoff_date.replace(hour=0, minute=0, second=0, microsecond=0)
                
                if is_recent:
                    print(f"ğŸ“… GÃ¼ncel makale: {article_date.strftime('%Y-%m-%d')} - {url[:60]}...")
                else:
                    print(f"ğŸ“… Eski makale: {article_date.strftime('%Y-%m-%d')} - {url[:60]}...")
                
                return is_recent
                
            except ValueError as date_error:
                print(f"âš ï¸ Tarih parse hatasÄ±: {date_error}")
                return True  # Hata durumunda makaleyi dahil et
        
        # The Verge URL formatÄ±: https://www.theverge.com/2025/1/9/article-name
        verge_pattern = r'/(\d{4})/(\d{1,2})/(\d{1,2})/'
        match = re.search(verge_pattern, url)
        
        if match:
            year, month, day = match.groups()
            try:
                article_date = datetime(int(year), int(month), int(day))
                
                # Makale tarihi son 24 saat iÃ§inde mi?
                is_recent = article_date >= cutoff_date.replace(hour=0, minute=0, second=0, microsecond=0)
                
                if is_recent:
                    print(f"ğŸ“… GÃ¼ncel makale: {article_date.strftime('%Y-%m-%d')} - {url[:60]}...")
                else:
                    print(f"ğŸ“… Eski makale: {article_date.strftime('%Y-%m-%d')} - {url[:60]}...")
                
                return is_recent
                
            except ValueError as date_error:
                print(f"âš ï¸ Tarih parse hatasÄ±: {date_error}")
                return True  # Hata durumunda makaleyi dahil et
        
        # DiÄŸer siteler iÃ§in - URL'de tarih bulunamazsa gÃ¼ncel kabul et
        print(f"ğŸ“… Tarih tespit edilemedi, gÃ¼ncel kabul ediliyor: {url[:60]}...")
        return True
        
    except Exception as e:
        print(f"âš ï¸ URL tarih kontrolÃ¼ hatasÄ±: {e}")
        return True  # Hata durumunda makaleyi dahil et

def extract_article_date_from_content(content):
    """Makale iÃ§eriÄŸinden yayÄ±n tarihini Ã§Ä±karmaya Ã§alÄ±ÅŸ"""
    try:
        import re
        
        # Ã‡eÅŸitli tarih formatlarÄ±nÄ± ara
        date_patterns = [
            # ISO format: 2025-01-09T10:30:00
            r'(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2})',
            # Date format: January 9, 2025
            r'(January|February|March|April|May|June|July|August|September|October|November|December)\s+(\d{1,2}),?\s+(\d{4})',
            # Date format: 9 January 2025
            r'(\d{1,2})\s+(January|February|March|April|May|June|July|August|September|October|November|December)\s+(\d{4})',
            # Date format: 2025-01-09
            r'(\d{4}-\d{2}-\d{2})',
            # Date format: 01/09/2025
            r'(\d{2}/\d{2}/\d{4})',
            # Time ago format: "2 hours ago", "1 day ago"
            r'(\d+)\s+(hour|hours|day|days)\s+ago'
        ]
        
        now = datetime.now()
        
        for pattern in date_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            
            for match in matches:
                try:
                    if isinstance(match, tuple):
                        if len(match) == 1:  # ISO format veya basit tarih
                            date_str = match[0]
                            if 'T' in date_str:
                                # ISO format
                                return datetime.fromisoformat(date_str.replace('Z', ''))
                            else:
                                # Basit tarih formatÄ±
                                return datetime.strptime(date_str, '%Y-%m-%d')
                        
                        elif len(match) == 3:  # Month day, year format
                            if match[0].isdigit():  # Day month year
                                day, month_name, year = match
                                month_map = {
                                    'january': 1, 'february': 2, 'march': 3, 'april': 4,
                                    'may': 5, 'june': 6, 'july': 7, 'august': 8,
                                    'september': 9, 'october': 10, 'november': 11, 'december': 12
                                }
                                month = month_map.get(month_name.lower())
                                if month:
                                    return datetime(int(year), month, int(day))
                            else:  # Month day, year
                                month_name, day, year = match
                                month_map = {
                                    'january': 1, 'february': 2, 'march': 3, 'april': 4,
                                    'may': 5, 'june': 6, 'july': 7, 'august': 8,
                                    'september': 9, 'october': 10, 'november': 11, 'december': 12
                                }
                                month = month_map.get(month_name.lower())
                                if month:
                                    return datetime(int(year), month, int(day))
                        
                        elif len(match) == 2:  # Time ago format
                            amount, unit = match
                            amount = int(amount)
                            
                            if 'hour' in unit:
                                return now - timedelta(hours=amount)
                            elif 'day' in unit:
                                return now - timedelta(days=amount)
                    
                    else:  # Single string match
                        if '/' in match:  # MM/DD/YYYY format
                            return datetime.strptime(match, '%m/%d/%Y')
                        elif '-' in match:  # YYYY-MM-DD format
                            return datetime.strptime(match, '%Y-%m-%d')
                
                except (ValueError, TypeError) as parse_error:
                    continue
        
        return None
        
    except Exception as e:
        print(f"âš ï¸ Tarih Ã§Ä±karma hatasÄ±: {e}")
        return None

def fetch_article_content_with_mcp_only(url):
    """Sadece MCP ile makale iÃ§eriÄŸi Ã§ekme"""
    try:
        print(f"ğŸ” MCP ile makale Ã§ekiliyor: {url[:50]}...")
        
        # MCP scrape fonksiyonunu kullan
        scrape_result = mcp_firecrawl_scrape({
            "url": url,
            "formats": ["markdown"],
            "onlyMainContent": True,
            "waitFor": 3000,
            "removeBase64Images": True
        })
        
        if not scrape_result.get("success", False):
            print(f"âš ï¸ MCP ile Ã§ekilemedi: {url}")
            return None
        
        # Markdown iÃ§eriÄŸini al
        markdown_content = scrape_result.get("markdown", "")
        
        if not markdown_content or len(markdown_content) < 100:
            print(f"âš ï¸ MCP'den yetersiz iÃ§erik: {len(markdown_content) if markdown_content else 0} karakter")
            return None
        
        # BaÅŸlÄ±ÄŸÄ± Ã§Ä±kar (genellikle ilk # ile baÅŸlar)
        lines = markdown_content.split('\n')
        title = ""
        content_lines = []
        
        for line in lines:
            line = line.strip()
            if line.startswith('# ') and not title:
                title = line[2:].strip()
            elif line and not line.startswith('#') and len(line) > 20:
                content_lines.append(line)
        
        # Ä°Ã§eriÄŸi birleÅŸtir ve temizle
        content = '\n'.join(content_lines)
        
        # Gereksiz karakterleri temizle
        content = content.replace('*', '').replace('**', '').replace('_', '')
        content = ' '.join(content.split())  # Ã‡oklu boÅŸluklarÄ± tek boÅŸluÄŸa Ã§evir
        
        # Ä°Ã§eriÄŸi sÄ±nÄ±rla
        content = content[:2500]
        
        # Makale tarihini iÃ§erikten Ã§Ä±karmaya Ã§alÄ±ÅŸ
        article_publish_date = extract_article_date_from_content(markdown_content)
        
        print(f"âœ… MCP ile iÃ§erik Ã§ekildi: {len(content)} karakter")
        if article_publish_date:
            print(f"ğŸ“… Makale yayÄ±n tarihi: {article_publish_date.strftime('%Y-%m-%d %H:%M')}")
        
        return {
            "title": title or "BaÅŸlÄ±k bulunamadÄ±",
            "content": content,
            "source": "mcp_only",
            "publish_date": article_publish_date.isoformat() if article_publish_date else None
        }
        
    except Exception as e:
        print(f"âŒ MCP makale iÃ§eriÄŸi Ã§ekme hatasÄ± ({url}): {e}")
        return None

# GitHub MCP ModÃ¼lÃ¼
def fetch_trending_github_repos(language="python", time_period="daily", limit=10):
    """GitHub'dan trend olan repolarÄ± Ã§ek"""
    try:
        terminal_log("ğŸ” GitHub trending repolarÄ± Ã§ekiliyor...", "info")
        
        # GitHub API endpoint'i
        base_url = "https://api.github.com/search/repositories"
        
        # Tarih hesaplama
        if time_period == "daily":
            since_date = (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")
        elif time_period == "weekly":
            since_date = (datetime.now() - timedelta(days=7)).strftime("%Y-%m-%d")
        else:  # monthly
            since_date = (datetime.now() - timedelta(days=30)).strftime("%Y-%m-%d")
        
        # Arama parametreleri
        params = {
            "q": f"language:{language} created:>{since_date}",
            "sort": "stars",
            "order": "desc",
            "per_page": limit
        }
        
        headers = {
            "Accept": "application/vnd.github.v3+json",
            "User-Agent": "AI-Tweet-Bot/1.0"
        }
        
        # GitHub token varsa ekle
        github_token = os.environ.get('GITHUB_TOKEN')
        if github_token:
            headers["Authorization"] = f"token {github_token}"
        
        response = requests.get(base_url, params=params, headers=headers, timeout=30)
        response.raise_for_status()
        
        data = response.json()
        repos = []
        
        for repo in data.get("items", []):
            repo_data = {
                "id": repo["id"],
                "name": repo["name"],
                "full_name": repo["full_name"],
                "description": repo.get("description", ""),
                "url": repo["html_url"],
                "stars": repo["stargazers_count"],
                "forks": repo["forks_count"],
                "language": repo.get("language", ""),
                "created_at": repo["created_at"],
                "updated_at": repo["updated_at"],
                "owner": {
                    "login": repo["owner"]["login"],
                    "avatar_url": repo["owner"]["avatar_url"]
                },
                "topics": repo.get("topics", []),
                "license": repo.get("license", {}).get("name", "") if repo.get("license") else "",
                "open_issues": repo.get("open_issues_count", 0),
                "watchers": repo.get("watchers_count", 0)
            }
            repos.append(repo_data)
        
        terminal_log(f"âœ… {len(repos)} GitHub repo Ã§ekildi", "success")
        return repos
        
    except requests.exceptions.RequestException as e:
        terminal_log(f"âŒ GitHub API hatasÄ±: {e}", "error")
        return []
    except Exception as e:
        terminal_log(f"âŒ GitHub repo Ã§ekme hatasÄ±: {e}", "error")
        return []

def fetch_github_repo_details_with_mcp(repo_url):
    """MCP ile GitHub repo detaylarÄ±nÄ± Ã§ek"""
    try:
        terminal_log(f"ğŸ” GitHub repo detaylarÄ± Ã§ekiliyor: {repo_url}", "info")
        
        # MCP ile repo sayfasÄ±nÄ± Ã§ek
        scrape_result = mcp_firecrawl_scrape({
            "url": repo_url,
            "formats": ["markdown"],
            "onlyMainContent": True,
            "waitFor": 2000
        })
        
        if not scrape_result.get("success", False):
            terminal_log("âš ï¸ MCP ile repo Ã§ekilemedi, fallback yÃ¶nteme geÃ§iliyor...", "warning")
            return fetch_github_repo_details_fallback(repo_url)
        
        content = scrape_result.get("markdown", "")
        
        # README iÃ§eriÄŸini Ã§Ä±kar
        readme_content = ""
        if "README" in content:
            readme_start = content.find("README")
            if readme_start != -1:
                readme_content = content[readme_start:readme_start+2000]  # Ä°lk 2000 karakter
        
        terminal_log("âœ… GitHub repo detaylarÄ± MCP ile Ã§ekildi", "success")
        return {
            "success": True,
            "content": content,
            "readme": readme_content,
            "method": "mcp"
        }
        
    except Exception as e:
        terminal_log(f"âŒ MCP GitHub repo detay hatasÄ±: {e}", "error")
        return fetch_github_repo_details_fallback(repo_url)

def fetch_github_repo_details_fallback(repo_url):
    """Fallback yÃ¶ntemi ile GitHub repo detaylarÄ±nÄ± Ã§ek"""
    try:
        terminal_log(f"ğŸ”„ Fallback ile GitHub repo detaylarÄ± Ã§ekiliyor: {repo_url}", "info")
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
        }
        
        response = requests.get(repo_url, headers=headers, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # README iÃ§eriÄŸini Ã§Ä±kar
        readme_element = soup.find('article', class_='markdown-body')
        readme_content = ""
        if readme_element:
            readme_content = readme_element.get_text(strip=True)[:2000]
        
        # Repo aÃ§Ä±klamasÄ±nÄ± Ã§Ä±kar
        description_element = soup.find('p', class_='f4')
        description = ""
        if description_element:
            description = description_element.get_text(strip=True)
        
        content = f"Repository: {repo_url}\n"
        if description:
            content += f"Description: {description}\n"
        if readme_content:
            content += f"README: {readme_content}\n"
        
        terminal_log("âœ… GitHub repo detaylarÄ± fallback ile Ã§ekildi", "success")
        return {
            "success": True,
            "content": content,
            "readme": readme_content,
            "description": description,
            "method": "fallback"
        }
        
    except Exception as e:
        terminal_log(f"âŒ Fallback GitHub repo detay hatasÄ±: {e}", "error")
        return {
            "success": False,
            "error": str(e),
            "method": "fallback"
        }

def generate_github_tweet(repo_data, api_key):
    """GitHub repo iÃ§in AI tweet oluÅŸtur"""
    try:
        terminal_log(f"ğŸ¤– GitHub repo iÃ§in tweet oluÅŸturuluyor: {repo_data['name']}", "info")
        
        # Repo detaylarÄ±nÄ± Ã§ek
        repo_details = fetch_github_repo_details_with_mcp(repo_data["url"])
        
        # Tweet iÃ§in prompt hazÄ±rla
        prompt = f"""
        GitHub Repository Tweet OluÅŸtur:
        
        Repo Bilgileri:
        - Ä°sim: {repo_data['name']}
        - AÃ§Ä±klama: {repo_data['description']}
        - Dil: {repo_data['language']}
        - YÄ±ldÄ±z: {repo_data['stars']}
        - Fork: {repo_data['forks']}
        - Konular: {', '.join(repo_data['topics'][:5])}
        - Sahip: {repo_data['owner']['login']}
        - URL: {repo_data['url']}
        
        README Ä°Ã§eriÄŸi:
        {repo_details.get('readme', '')[:1000]}
        
        LÃ¼tfen bu GitHub repository iÃ§in:
        1. TÃ¼rkÃ§e bir tweet yazÄ±n (280 karakter sÄ±nÄ±rÄ±)
        2. Repo'nun Ã¶ne Ã§Ä±kan Ã¶zelliklerini vurgulayÄ±n
        3. GeliÅŸtiriciler iÃ§in neden ilginÃ§ olduÄŸunu aÃ§Ä±klayÄ±n
        4. Uygun hashtag'ler ekleyin (#GitHub #OpenSource #Programming)
        5. Emoji kullanarak gÃ¶rsel Ã§ekicilik katÄ±n
        
        Tweet formatÄ±:
        [Tweet metni]
        
        [Hashtag'ler]
        
        URL: {repo_data['url']}
        """
        
        # Gemini API ile tweet oluÅŸtur
        tweet_response = gemini_call(prompt, api_key, max_tokens=200)
        
        if not tweet_response:
            # Fallback tweet
            tweet_text = create_fallback_github_tweet(repo_data)
        else:
            tweet_text = tweet_response.strip()
        
        # Tweet'i temizle ve formatla
        if len(tweet_text) > 280:
            tweet_text = tweet_text[:277] + "..."
        
        terminal_log("âœ… GitHub tweet oluÅŸturuldu", "success")
        
        return {
            "success": True,
            "tweet": tweet_text,
            "repo_data": repo_data,
            "repo_details": repo_details
        }
        
    except Exception as e:
        terminal_log(f"âŒ GitHub tweet oluÅŸturma hatasÄ±: {e}", "error")
        
        # Fallback tweet
        tweet_text = create_fallback_github_tweet(repo_data)
        
        return {
            "success": False,
            "tweet": tweet_text,
            "repo_data": repo_data,
            "error": str(e)
        }

def create_fallback_github_tweet(repo_data):
    """GitHub repo iÃ§in basit fallback tweet"""
    try:
        name = repo_data.get('name', 'Unknown')
        description = repo_data.get('description', '')[:100]
        language = repo_data.get('language', '')
        stars = repo_data.get('stars', 0)
        url = repo_data.get('url', '')
        
        # Basit tweet formatÄ±
        tweet = f"ğŸš€ {name}\n"
        
        if description:
            tweet += f"{description}\n"
        
        if language:
            tweet += f"ğŸ’» {language} "
        
        if stars > 0:
            tweet += f"â­ {stars} stars"
        
        tweet += f"\n\n#GitHub #OpenSource #Programming"
        
        if language:
            tweet += f" #{language}"
        
        tweet += f"\n\n{url}"
        
        # 280 karakter sÄ±nÄ±rÄ±
        if len(tweet) > 280:
            tweet = tweet[:277] + "..."
        
        return tweet
        
    except Exception as e:
        return f"ğŸš€ Yeni GitHub projesi keÅŸfedildi!\n\n#GitHub #OpenSource #Programming\n\n{repo_data.get('url', '')}"

def fetch_github_articles_for_tweets():
    """Tweet iÃ§in GitHub repolarÄ±nÄ± Ã§ek ve iÅŸle"""
    try:
        terminal_log("ğŸ” GitHub repolarÄ± tweet iÃ§in Ã§ekiliyor...", "info")
        
        # FarklÄ± dillerde trend repolarÄ± Ã§ek
        languages = ["python", "javascript", "typescript", "go", "rust", "java"]
        all_repos = []
        
        for language in languages[:3]:  # Ä°lk 3 dil
            repos = fetch_trending_github_repos(language=language, time_period="weekly", limit=5)
            all_repos.extend(repos)
        
        if not all_repos:
            terminal_log("âŒ GitHub repo bulunamadÄ±", "warning")
            return []
        
        # Mevcut paylaÅŸÄ±lan repolarÄ± kontrol et
        posted_articles = load_json(HISTORY_FILE)
        posted_urls = [article.get('url', '') for article in posted_articles]
        
        # Yeni repolarÄ± filtrele
        new_repos = []
        for repo in all_repos:
            if repo['url'] not in posted_urls:
                new_repos.append(repo)
        
        terminal_log(f"âœ… {len(new_repos)} yeni GitHub repo bulundu", "success")
        
        # Tweet formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r
        github_articles = []
        for repo in new_repos[:5]:  # Ä°lk 5 repo
            article = {
                "title": f"{repo['name']} - {repo['description'][:100]}",
                "url": repo['url'],
                "content": f"GitHub Repository: {repo['full_name']}\n"
                          f"Description: {repo['description']}\n"
                          f"Language: {repo['language']}\n"
                          f"Stars: {repo['stars']}\n"
                          f"Forks: {repo['forks']}\n"
                          f"Topics: {', '.join(repo['topics'][:5])}",
                "source": "GitHub",
                "published_date": repo['created_at'],
                "hash": hashlib.md5(repo['url'].encode()).hexdigest(),
                "repo_data": repo
            }
            github_articles.append(article)
        
        terminal_log(f"âœ… {len(github_articles)} GitHub makalesi hazÄ±rlandÄ±", "success")
        return github_articles
        
    except Exception as e:
        terminal_log(f"âŒ GitHub makalesi Ã§ekme hatasÄ±: {e}", "error")
        return []

def save_github_repo_history(repo_data, tweet_result):
    """GitHub repo paylaÅŸÄ±m geÃ§miÅŸini kaydet"""
    try:
        # Mevcut geÃ§miÅŸi yÃ¼kle
        posted_articles = load_json(HISTORY_FILE)
        
        # Yeni kayÄ±t oluÅŸtur
        new_record = {
            "title": f"{repo_data['name']} - GitHub Repository",
            "url": repo_data['url'],
            "content": repo_data.get('description', ''),
            "source": "GitHub",
            "published_date": repo_data.get('created_at', datetime.now().isoformat()),
            "posted_date": datetime.now().isoformat(),
            "hash": hashlib.md5(repo_data['url'].encode()).hexdigest(),
            "tweet_id": tweet_result.get('tweet_id'),
            "tweet_url": tweet_result.get('tweet_url', ''),
            "repo_data": repo_data,
            "type": "github_repo"
        }
        
        # Listeye ekle
        posted_articles.append(new_record)
        
        # Kaydet
        save_json(HISTORY_FILE, posted_articles)
        
        terminal_log(f"âœ… GitHub repo geÃ§miÅŸi kaydedildi: {repo_data['name']}", "success")
        
    except Exception as e:
        terminal_log(f"âŒ GitHub repo geÃ§miÅŸi kaydetme hatasÄ±: {e}", "error")

def get_github_stats():
    """GitHub modÃ¼lÃ¼ istatistiklerini getir"""
    try:
        posted_articles = load_json(HISTORY_FILE)
        
        # GitHub repolarÄ±nÄ± filtrele
        github_repos = [article for article in posted_articles if article.get('type') == 'github_repo']
        
        # Ä°statistikleri hesapla
        total_repos = len(github_repos)
        
        # Dil daÄŸÄ±lÄ±mÄ±
        languages = {}
        for repo in github_repos:
            repo_data = repo.get('repo_data', {})
            lang = repo_data.get('language', 'Unknown')
            languages[lang] = languages.get(lang, 0) + 1
        
        # Son 7 gÃ¼nde paylaÅŸÄ±lan
        week_ago = datetime.now() - timedelta(days=7)
        recent_repos = [
            repo for repo in github_repos 
            if datetime.fromisoformat(repo.get('posted_date', '2020-01-01')) > week_ago
        ]
        
        return {
            "total_repos": total_repos,
            "recent_repos": len(recent_repos),
            "languages": languages,
            "last_repo": github_repos[-1] if github_repos else None
        }
        
    except Exception as e:
        terminal_log(f"âŒ GitHub istatistik hatasÄ±: {e}", "error")
        return {
            "total_repos": 0,
            "recent_repos": 0,
            "languages": {},
            "last_repo": None
        }
