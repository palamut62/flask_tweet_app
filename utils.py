import os
import json
import requests
from bs4 import BeautifulSoup
from fpdf import FPDF
import tweepy
from datetime import datetime, timedelta
import hashlib
from dotenv import load_dotenv
from PIL import Image
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import time
import re
from difflib import SequenceMatcher

# .env dosyasƒ±nƒ± y√ºkle
load_dotenv()

# Web scraping i√ßin alternatif k√ºt√ºphaneler (opsiyonel)
try:
    import requests_html  # type: ignore
    REQUESTS_HTML_AVAILABLE = True
except ImportError:
    REQUESTS_HTML_AVAILABLE = False
    requests_html = None  # type: ignore

try:
    from selenium import webdriver  # type: ignore
    from selenium.webdriver.chrome.options import Options  # type: ignore
    from selenium.webdriver.common.by import By  # type: ignore
    from selenium.webdriver.support.ui import WebDriverWait  # type: ignore
    from selenium.webdriver.support import expected_conditions as EC  # type: ignore
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False
    webdriver = None  # type: ignore
    Options = None  # type: ignore
    By = None  # type: ignore
    WebDriverWait = None  # type: ignore
    EC = None  # type: ignore

# .env dosyasƒ±nƒ± y√ºkle
load_dotenv()

# Firecrawl MCP fonksiyonlarƒ± i√ßin placeholder
def mcp_firecrawl_scrape(params):
    """Firecrawl MCP scrape fonksiyonu - Geli≈ümi≈ü alternatif sistemli"""
    try:
        print(f"[MCP] Firecrawl scrape √ßaƒürƒ±sƒ±: {params.get('url', 'unknown')}")
        
        url = params.get('url', '')
        if not url:
            return {"success": False, "error": "URL gerekli"}
        
        # JavaScript gerekli mi kontrol et
        use_js = any(domain in url.lower() for domain in [
            'techcrunch.com', 'theverge.com', 'wired.com', 
            'arstechnica.com', 'venturebeat.com'
        ])
        
        # √ñnce geli≈ümi≈ü scraper dene
        try:
            print(f"[MCP] Geli≈ümi≈ü scraper deneniyor (JS: {use_js})...")
            result = advanced_web_scraper(url, wait_time=3, use_js=use_js)
            
            if result.get("success") and result.get("content"):
                content = result.get("content", "")
                
                print(f"[MCP] Geli≈ümi≈ü scraper ba≈üarƒ±lƒ±: {len(content)} karakter ({result.get('method', 'unknown')})")
                
                return {
                    "success": True,
                    "content": content,
                    "markdown": content,
                    "source": f"advanced_scraper_{result.get('method', 'unknown')}",
                    "method": result.get('method', 'unknown')
                }
            else:
                print(f"[MCP] Geli≈ümi≈ü scraper ba≈üarƒ±sƒ±z: {result.get('error', 'Bilinmeyen hata')}")
                
        except Exception as advanced_error:
            print(f"[MCP] Geli≈ümi≈ü scraper hatasƒ±: {advanced_error}")
        
        # Fallback: Basit HTTP request
        try:
            print(f"[MCP] Basit fallback deneniyor...")
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive'
            }
            
            session = requests.Session()
            session.headers.update(headers)
            
            response = session.get(url, timeout=30, allow_redirects=True)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            content = extract_main_content(soup)
            
            if content and len(content) > 100:
                print(f"[MCP] Basit fallback ba≈üarƒ±lƒ±: {len(content)} karakter")
                
                return {
                    "success": True,
                    "content": content,
                    "markdown": content,
                    "source": "simple_fallback"
                }
            else:
                print(f"[MCP] Basit fallback yetersiz i√ßerik: {len(content) if content else 0} karakter")
                
        except Exception as fallback_error:
            print(f"[MCP] Basit fallback hatasƒ±: {fallback_error}")
        
        # Son √ßare: Sadece ba≈ülƒ±k √ßek
        try:
            print(f"[MCP] Son √ßare: Sadece ba≈ülƒ±k √ßekiliyor...")
            
            response = requests.get(url, timeout=15)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            title = ""
            title_selectors = ['title', 'h1', 'h2', '.title', '.headline']
            
            for selector in title_selectors:
                elem = soup.select_one(selector)
                if elem:
                    title = elem.get_text(strip=True)
                    if len(title) > 10:
                        break
            
            if title:
                print(f"[MCP] Son √ßare ba≈üarƒ±lƒ±: Ba≈ülƒ±k √ßekildi")
                return {
                    "success": True,
                    "content": title,
                    "markdown": title,
                    "source": "title_only"
                }
                
        except Exception as title_error:
            print(f"[MCP] Son √ßare hatasƒ±: {title_error}")
        
        return {"success": False, "error": "T√ºm y√∂ntemler ba≈üarƒ±sƒ±z"}
        
    except Exception as e:
        print(f"[MCP] Genel hata: {e}")
        return {"success": False, "error": str(e)}

HISTORY_FILE = "posted_articles.json"
HASHTAG_FILE = "hashtags.json"
ACCOUNT_FILE = "accounts.json"
SUMMARY_FILE = "summaries.json"
MCP_CONFIG_FILE = "mcp_config.json"

def fetch_latest_ai_articles_with_firecrawl():
    """Firecrawl MCP ile geli≈ümi≈ü haber √ßekme - Sadece son 4 makale"""
    try:
        # √ñnce mevcut yayƒ±nlanan makaleleri y√ºkle
        posted_articles = load_json(HISTORY_FILE)
        posted_urls = [article.get('url', '') for article in posted_articles]
        posted_hashes = [article.get('hash', '') for article in posted_articles]
        
        print("üîç TechCrunch AI kategorisinden Firecrawl MCP ile makale √ßekiliyor...")
        
        # Firecrawl MCP ile ana sayfa √ßek
        try:
            # Firecrawl MCP scrape fonksiyonunu kullan
            scrape_result = mcp_firecrawl_scrape({
                "url": "https://techcrunch.com/category/artificial-intelligence/",
                "formats": ["markdown", "links"],
                "onlyMainContent": True,
                "waitFor": 2000
            })
            
            if not scrape_result.get("success", False):
                print(f"‚ö†Ô∏è Firecrawl MCP hatasƒ±, fallback y√∂nteme ge√ßiliyor...")
                return fetch_latest_ai_articles_fallback()
            
            # Markdown i√ßeriƒüinden makale linklerini √ßƒ±kar
            markdown_content = scrape_result.get("markdown", "")
            
            # Basit regex ile TechCrunch makale URL'lerini bul
            import re
            current_year = datetime.now().year
            
            # T√ºm TechCrunch URL'lerini bul
            all_urls = re.findall(r'https://techcrunch\.com/[^\s\)\]]+', markdown_content)
            
            # G√ºncel yƒ±l ve √∂nceki yƒ±lƒ±n makalelerini filtrele
            article_urls = []
            for url in all_urls:
                # URL'yi temizle
                url = url.rstrip(')')
                
                # Yƒ±l kontrol√º
                year_check = f'/{current_year}/' in url or f'/{current_year-1}/' in url
                
                # Makale URL'si kontrol√º (tarih formatƒ± i√ßermeli)
                date_pattern = r'/\d{4}/\d{2}/\d{2}/'
                is_article = re.search(date_pattern, url)
                
                if (year_check and is_article and 
                    url not in posted_urls and 
                    url not in article_urls and
                    len(article_urls) < 4):  # Sadece son 4 makale
                    article_urls.append(url)
            
            print(f"üîó {len(article_urls)} makale URL'si bulundu")
            
        except Exception as firecrawl_error:
            print(f"‚ö†Ô∏è Firecrawl MCP hatasƒ±: {firecrawl_error}")
            print("üîÑ Fallback y√∂nteme ge√ßiliyor...")
            return fetch_latest_ai_articles_fallback()
        
        articles_data = []
        for url in article_urls:
            try:
                # Her makaleyi Firecrawl MCP ile √ßek
                article_content = fetch_article_content_with_firecrawl(url)
                
                if article_content and len(article_content.get("content", "")) > 100:
                    title = article_content.get("title", "")
                    content = article_content.get("content", "")
                    
                    # Makale hash'i olu≈ütur
                    article_hash = hashlib.md5(title.encode()).hexdigest()
                    
                    # Tekrar kontrol√º
                    if article_hash not in posted_hashes:
                        articles_data.append({
                            "title": title,
                            "url": url,
                            "content": content,
                            "hash": article_hash,
                            "fetch_date": datetime.now().isoformat(),
                            "is_new": True,
                            "already_posted": False,
                            "source": "firecrawl_mcp"
                        })
                        print(f"üÜï Firecrawl ile yeni makale: {title[:50]}...")
                    else:
                        print(f"‚úÖ Makale zaten payla≈üƒ±lmƒ±≈ü: {title[:50]}...")
                else:
                    print(f"‚ö†Ô∏è ƒ∞√ßerik yetersiz: {url}")
                    
            except Exception as article_error:
                print(f"‚ùå Makale √ßekme hatasƒ± ({url}): {article_error}")
                continue
        
        print(f"üìä Firecrawl MCP ile {len(articles_data)} yeni makale bulundu")
        
        # Duplikat filtreleme uygula
        if articles_data:
            articles_data = filter_duplicate_articles(articles_data)
        
        return articles_data
        
    except Exception as e:
        print(f"Firecrawl MCP haber √ßekme hatasƒ±: {e}")
        print("üîÑ Fallback y√∂nteme ge√ßiliyor...")
        return fetch_latest_ai_articles_fallback()

def fetch_latest_ai_articles():
    """Ana haber √ßekme fonksiyonu - Akƒ±llƒ± sistem ile"""
    try:
        # Yeni akƒ±llƒ± haber √ßekme sistemini kullan
        return fetch_latest_ai_articles_smart()
        
    except Exception as e:
        print(f"‚ùå Ana haber √ßekme hatasƒ±: {e}")
        print("üîÑ Son √ßare fallback deneniyor...")
        try:
            return fetch_latest_ai_articles_fallback()
        except Exception as fallback_error:
            print(f"‚ùå Fallback da ba≈üarƒ±sƒ±z: {fallback_error}")
            return []

def fetch_latest_ai_articles_fallback():
    """Fallback haber √ßekme y√∂ntemi - BeautifulSoup ile"""
    try:
        # √ñnce mevcut yayƒ±nlanan makaleleri y√ºkle
        posted_articles = load_json(HISTORY_FILE)
        posted_urls = [article.get('url', '') for article in posted_articles]
        posted_hashes = [article.get('hash', '') for article in posted_articles]
        
        headers = {'User-Agent': 'Mozilla/5.0'}
        html = requests.get("https://techcrunch.com/category/artificial-intelligence/", headers=headers).text
        soup = BeautifulSoup(html, "html.parser")
        article_links = soup.select("a.loop-card__title-link")[:4]  # Sadece son 4 makale
        
        print(f"üîç Fallback: TechCrunch AI kategorisinden son {len(article_links)} makale kontrol ediliyor...")
        
        articles_data = []
        for link_tag in article_links:
            title = link_tag.text.strip()
            url = link_tag['href']
            
            # Makale hash'i olu≈ütur (ba≈ülƒ±k bazlƒ±)
            article_hash = hashlib.md5(title.encode()).hexdigest()
            
            # Tekrar kontrol√º - URL ve hash bazlƒ±
            is_already_posted = url in posted_urls or article_hash in posted_hashes
            
            if is_already_posted:
                print(f"‚úÖ Makale zaten payla≈üƒ±lmƒ±≈ü, atlanƒ±yor: {title[:50]}...")
                continue
            
            # Makale i√ßeriƒüini geli≈ümi≈ü ≈üekilde √ßek
            content = fetch_article_content_advanced(url, headers)
            
            if content and len(content) > 100:  # Minimum i√ßerik kontrol√º
                articles_data.append({
                    "title": title, 
                    "url": url, 
                    "content": content,
                    "hash": article_hash,
                    "fetch_date": datetime.now().isoformat(),
                    "is_new": True,  # Yeni makale i≈üareti
                    "already_posted": False,
                    "source": "fallback"
                })
                print(f"üÜï Fallback ile yeni makale bulundu: {title[:50]}...")
            else:
                print(f"‚ö†Ô∏è ƒ∞√ßerik yetersiz, atlanƒ±yor: {title[:50]}...")
        
        print(f"üìä Fallback ile toplam {len(articles_data)} yeni makale bulundu")
        
        # Duplikat filtreleme uygula
        if articles_data:
            articles_data = filter_duplicate_articles(articles_data)
        
        return articles_data
        
    except Exception as e:
        print(f"Fallback haber √ßekme hatasƒ±: {e}")
        return []

def fetch_article_content_with_firecrawl(url):
    """Firecrawl MCP ile makale i√ßeriƒüi √ßekme"""
    try:
        print(f"üîç Firecrawl MCP ile makale √ßekiliyor: {url[:50]}...")
        
        # Firecrawl MCP scrape fonksiyonunu kullan
        scrape_result = mcp_firecrawl_scrape({
            "url": url,
            "formats": ["markdown"],
            "onlyMainContent": True,
            "waitFor": 3000,
            "removeBase64Images": True
        })
        
        if not scrape_result.get("success", False):
            print(f"‚ö†Ô∏è Firecrawl MCP ba≈üarƒ±sƒ±z, fallback deneniyor...")
            return fetch_article_content_advanced_fallback(url)
        
        # Markdown i√ßeriƒüini al
        markdown_content = scrape_result.get("markdown", "")
        
        if not markdown_content or len(markdown_content) < 100:
            print(f"‚ö†Ô∏è Firecrawl'dan yetersiz i√ßerik, fallback deneniyor...")
            return fetch_article_content_advanced_fallback(url)
        
        # Ba≈ülƒ±ƒüƒ± √ßƒ±kar (genellikle ilk # ile ba≈ülar)
        lines = markdown_content.split('\n')
        title = ""
        content_lines = []
        
        for line in lines:
            line = line.strip()
            if line.startswith('# ') and not title:
                title = line[2:].strip()
            elif line and not line.startswith('#') and len(line) > 20:
                content_lines.append(line)
        
        # ƒ∞√ßeriƒüi birle≈ütir ve temizle
        content = '\n'.join(content_lines)
        
        # Gereksiz karakterleri temizle
        content = content.replace('*', '').replace('**', '').replace('_', '')
        content = ' '.join(content.split())  # √áoklu bo≈üluklarƒ± tek bo≈üluƒüa √ßevir
        
        # ƒ∞√ßeriƒüi sƒ±nƒ±rla
        content = content[:2500]
        
        print(f"‚úÖ Firecrawl ile i√ßerik √ßekildi: {len(content)} karakter")
        
        return {
            "title": title or "Ba≈ülƒ±k bulunamadƒ±",
            "content": content,
            "source": "firecrawl_mcp"
        }
        
    except Exception as e:
        print(f"‚ùå Firecrawl MCP hatasƒ± ({url}): {e}")
        print("üîÑ Fallback y√∂nteme ge√ßiliyor...")
        return fetch_article_content_advanced_fallback(url)

def fetch_article_content_advanced_fallback(url):
    """Fallback makale i√ßeriƒüi √ßekme - BeautifulSoup ile"""
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        article_html = requests.get(url, headers=headers, timeout=10).text
        article_soup = BeautifulSoup(article_html, "html.parser")
        
        # Ba≈ülƒ±ƒüƒ± bul
        title = ""
        title_selectors = ["h1", "h1.entry-title", "h1.post-title", ".article-title h1"]
        for selector in title_selectors:
            title_elem = article_soup.select_one(selector)
            if title_elem:
                title = title_elem.text.strip()
                break
        
        # √áoklu selector deneme - daha kapsamlƒ± i√ßerik √ßekme
        content_selectors = [
            "div.article-content p",
            "div.entry-content p", 
            "div.post-content p",
            "article p",
            "div.content p",
            ".article-body p"
        ]
        
        content = ""
        for selector in content_selectors:
            paragraphs = article_soup.select(selector)
            if paragraphs:
                content = "\n".join([p.text.strip() for p in paragraphs if p.text.strip()])
                if len(content) > 200:  # Yeterli i√ßerik bulundu
                    break
        
        # Eƒüer hala i√ßerik bulunamadƒ±ysa, t√ºm p etiketlerini dene
        if not content:
            all_paragraphs = article_soup.find_all('p')
            content = "\n".join([p.text.strip() for p in all_paragraphs if len(p.text.strip()) > 50])
        
        content = content[:2000]  # ƒ∞√ßeriƒüi sƒ±nƒ±rla
        
        return {
            "title": title or "Ba≈ülƒ±k bulunamadƒ±",
            "content": content,
            "source": "fallback"
        }
        
    except Exception as e:
        print(f"Fallback makale i√ßeriƒüi √ßekme hatasƒ± ({url}): {e}")
        return None

def fetch_article_content_advanced(url, headers):
    """Geriye d√∂n√ºk uyumluluk i√ßin eski fonksiyon"""
    result = fetch_article_content_advanced_fallback(url)
    return result.get("content", "") if result else ""

def load_json(path):
    return json.load(open(path, 'r', encoding='utf-8')) if os.path.exists(path) else []

def save_json(path, data):
    with open(path, "w", encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)

def summarize_article(article_content, api_key):
    """LLM ile geli≈ümi≈ü makale √∂zetleme"""
    prompt = f"""A≈üaƒüƒ±daki AI/teknoloji haberini T√ºrk√ße olarak √∂zetle. √ñzet tweet formatƒ±nda, ilgi √ßekici ve bilgilendirici olsun:

Haber ƒ∞√ßeriƒüi:
{article_content[:1500]}

L√ºtfen:
- Maksimum 200 karakter
- Ana konuyu vurgula
- Teknik detaylarƒ± basitle≈ütir
- ƒ∞lgi √ßekici bir dil kullan

√ñzet:"""
    return gemini_call(prompt, api_key, max_tokens=100)

def score_article(article_content, api_key):
    prompt = f"""Bu AI/teknoloji haberinin √∂nemini 1-10 arasƒ±nda deƒüerlendir (sadece sayƒ±):

{article_content[:800]}

Deƒüerlendirme kriterleri:
- Yenilik derecesi
- Sekt√∂rel etki
- Geli≈ütiriciler i√ßin √∂nem
- Genel ilgi

Puan:"""
    result = gemini_call(prompt, api_key, max_tokens=5)
    try:
        return int(result.strip().split()[0])
    except:
        return 5

def categorize_article(article_content, api_key):
    prompt = f"""Bu haberin hedef kitlesini belirle:

{article_content[:500]}

Se√ßenekler: Developer, Investor, General
Cevap:"""
    return gemini_call(prompt, api_key, max_tokens=10).strip()

def gemini_call(prompt, api_key, max_tokens=100):
    """Google Gemini API √ßaƒürƒ±sƒ±"""
    if not api_key:
        safe_log("Gemini API anahtarƒ± bulunamadƒ±", "WARNING")
        return "API anahtarƒ± eksik"
    
    try:
        import google.generativeai as genai
        
        # API anahtarƒ±nƒ± yapƒ±landƒ±r
        genai.configure(api_key=api_key)
        
        # Modeli olu≈ütur
        model = genai.GenerativeModel('gemini-2.0-flash')
        
        safe_log("Gemini API √ßaƒürƒ±sƒ± yapƒ±lƒ±yor... Model: gemini-2.0-flash", "DEBUG")
        
        # Generation config
        generation_config = genai.types.GenerationConfig(
            max_output_tokens=max_tokens,
            temperature=0.7,
        )
        
        # API √ßaƒürƒ±sƒ±
        response = model.generate_content(
            prompt,
            generation_config=generation_config
        )
        
        safe_log("Gemini API Yanƒ±tƒ± alƒ±ndƒ±", "DEBUG")
        
        if response.text:
            content = response.text.strip()
            safe_log(f"ƒ∞√ßerik alƒ±ndƒ±: {len(content)} karakter", "DEBUG")
            return content
        else:
            safe_log("Gemini API yanƒ±tƒ±nda metin bulunamadƒ±", "DEBUG")
            return "API hatasƒ±"
            
    except Exception as e:
        safe_log(f"Gemini API √ßaƒürƒ± hatasƒ±: {str(e)}", "ERROR")
        return "API hatasƒ±"

def generate_smart_hashtags(title, content):
    """Makale i√ßeriƒüine g√∂re akƒ±llƒ± hashtag olu≈üturma - 5 pop√ºler hashtag"""
    combined_text = f"{title.lower()} {content.lower()}"
    hashtags = []
    
    # AI ve Machine Learning hashtag'leri
    if any(keyword in combined_text for keyword in ["artificial intelligence", "ai", "machine learning", "ml", "neural", "deep learning"]):
        hashtags.extend(["#ArtificialIntelligence", "#MachineLearning", "#DeepLearning", "#NeuralNetworks"])
    
    # Teknoloji ve yazƒ±lƒ±m hashtag'leri
    if any(keyword in combined_text for keyword in ["software", "programming", "code", "developer", "api"]):
        hashtags.extend(["#SoftwareDevelopment", "#Programming", "#Developer", "#API"])
    
    # Startup ve yatƒ±rƒ±m hashtag'leri
    if any(keyword in combined_text for keyword in ["startup", "funding", "investment", "venture", "billion", "million"]):
        hashtags.extend(["#Startup", "#Investment", "#VentureCapital", "#Funding", "#Business"])
    
    # ≈ûirket √∂zel hashtag'leri
    if "openai" in combined_text:
        hashtags.extend(["#OpenAI", "#ChatGPT", "#GPT"])
    if "google" in combined_text:
        hashtags.extend(["#Google", "#Alphabet", "#GoogleAI"])
    if "microsoft" in combined_text:
        hashtags.extend(["#Microsoft", "#Azure", "#Copilot"])
    if "meta" in combined_text:
        hashtags.extend(["#Meta", "#Facebook", "#MetaAI"])
    if "apple" in combined_text:
        hashtags.extend(["#Apple", "#iOS", "#AppleAI"])
    if "tesla" in combined_text:
        hashtags.extend(["#Tesla", "#ElonMusk", "#Autopilot"])
    if "nvidia" in combined_text:
        hashtags.extend(["#NVIDIA", "#GPU", "#CUDA"])
    if "anthropic" in combined_text:
        hashtags.extend(["#Anthropic", "#Claude"])
    
    # Teknoloji alanlarƒ±
    if any(keyword in combined_text for keyword in ["blockchain", "crypto", "bitcoin", "ethereum"]):
        hashtags.extend(["#Blockchain", "#Cryptocurrency", "#Web3", "#DeFi"])
    if any(keyword in combined_text for keyword in ["cloud", "aws", "azure", "gcp"]):
        hashtags.extend(["#CloudComputing", "#AWS", "#Azure", "#CloudNative"])
    if any(keyword in combined_text for keyword in ["cybersecurity", "security", "privacy", "encryption"]):
        hashtags.extend(["#Cybersecurity", "#DataPrivacy", "#InfoSec"])
    if any(keyword in combined_text for keyword in ["quantum", "quantum computing"]):
        hashtags.extend(["#QuantumComputing", "#Quantum", "#QuantumTech"])
    if any(keyword in combined_text for keyword in ["robotics", "robot", "automation"]):
        hashtags.extend(["#Robotics", "#Automation", "#RoboticProcess"])
    if any(keyword in combined_text for keyword in ["iot", "internet of things", "smart home"]):
        hashtags.extend(["#IoT", "#SmartHome", "#ConnectedDevices"])
    if any(keyword in combined_text for keyword in ["5g", "6g", "network", "connectivity"]):
        hashtags.extend(["#5G", "#Connectivity", "#Telecommunications"])
    if any(keyword in combined_text for keyword in ["ar", "vr", "augmented reality", "virtual reality", "metaverse"]):
        hashtags.extend(["#AR", "#VR", "#Metaverse", "#XR"])
    
    # Genel teknoloji hashtag'leri
    general_hashtags = ["#Innovation", "#Technology", "#DigitalTransformation", "#FutureTech", "#TechNews"]
    hashtags.extend(general_hashtags)
    
    # Tekrarlarƒ± kaldƒ±r ve 5 tane se√ß
    unique_hashtags = list(dict.fromkeys(hashtags))  # Sƒ±rayƒ± koruyarak tekrarlarƒ± kaldƒ±r
    
    # En alakalƒ± 5 hashtag se√ß
    selected_hashtags = unique_hashtags[:5]
    
    # Eƒüer 5'ten az varsa, genel hashtag'lerle tamamla
    if len(selected_hashtags) < 5:
        remaining_general = [h for h in general_hashtags if h not in selected_hashtags]
        selected_hashtags.extend(remaining_general[:5-len(selected_hashtags)])
    
    return selected_hashtags[:5]

def generate_smart_emojis(title, content):
    """Makale i√ßeriƒüine g√∂re akƒ±llƒ± emoji se√ßimi"""
    combined_text = f"{title.lower()} {content.lower()}"
    emojis = []
    
    # Konu bazlƒ± emojiler
    if any(keyword in combined_text for keyword in ["ai", "artificial intelligence", "robot", "machine learning"]):
        emojis.extend(["ü§ñ", "üß†", "‚ö°"])
    if any(keyword in combined_text for keyword in ["funding", "investment", "billion", "million", "money"]):
        emojis.extend(["üí∞", "üí∏", "üìà"])
    if any(keyword in combined_text for keyword in ["launch", "release", "unveil", "announce"]):
        emojis.extend(["üöÄ", "üéâ", "‚ú®"])
    if any(keyword in combined_text for keyword in ["research", "development", "breakthrough", "discovery"]):
        emojis.extend(["üî¨", "üí°", "üß™"])
    if any(keyword in combined_text for keyword in ["security", "privacy", "protection", "safe"]):
        emojis.extend(["üîí", "üõ°Ô∏è", "üîê"])
    if any(keyword in combined_text for keyword in ["acquisition", "merger", "partnership"]):
        emojis.extend(["ü§ù", "üîó", "üíº"])
    if any(keyword in combined_text for keyword in ["search", "query", "find", "discover"]):
        emojis.extend(["üîç", "üîé", "üìä"])
    if any(keyword in combined_text for keyword in ["mobile", "phone", "app", "smartphone"]):
        emojis.extend(["üì±", "üì≤", "üíª"])
    if any(keyword in combined_text for keyword in ["cloud", "server", "data", "storage"]):
        emojis.extend(["‚òÅÔ∏è", "üíæ", "üóÑÔ∏è"])
    if any(keyword in combined_text for keyword in ["game", "gaming", "entertainment"]):
        emojis.extend(["üéÆ", "üïπÔ∏è", "üéØ"])
    
    # Eƒüer emoji bulunamadƒ±ysa varsayƒ±lan emojiler
    if not emojis:
        emojis = ["üöÄ", "üíª", "üåü", "‚ö°", "üî•"]
    
    # En fazla 3 emoji se√ß
    return emojis[:3]

def generate_comprehensive_analysis(article_data, api_key):
    """Makale i√ßin kapsamlƒ± AI analizi - Ayrƒ± ayrƒ± √ßaƒürƒ±lar ile g√ºvenilir sonu√ß (ƒ∞ngilizce)"""
    title = article_data.get("title", "")
    content = article_data.get("content", "")
    
    print(f"üîç Kapsamlƒ± AI analizi ba≈ülatƒ±lƒ±yor...")
    
    # AI ile ilgili olmayan i√ßerikleri kontrol et
    title_lower = title.lower()
    content_lower = content.lower()
    
    # AI anahtar kelimeleri
    ai_keywords = ['ai', 'artificial intelligence', 'machine learning', 'deep learning', 'neural', 'gpt', 'llm', 'openai', 'anthropic', 'claude', 'chatgpt', 'algorithm', 'automation', 'robot', 'tech', 'software', 'data', 'computer']
    
    # AI ile ilgili olmayan anahtar kelimeler
    non_ai_keywords = ['wood', 'dried', 'kiln', 'furniture', 'cooking', 'recipe', 'travel', 'music', 'art', 'painting', 'photography', 'sports', 'fashion', 'food', 'health', 'medicine', 'politics', 'economy', 'finance', 'real estate']
    
    # AI ile ilgili mi kontrol et
    has_ai_content = any(keyword in title_lower or keyword in content_lower for keyword in ai_keywords)
    has_non_ai_content = any(keyword in title_lower or keyword in content_lower for keyword in non_ai_keywords)
    
    # Eƒüer AI ile ilgili deƒüilse veya AI olmayan i√ßerik varsa uyarƒ± ver
    if not has_ai_content or has_non_ai_content:
        print(f"‚ö†Ô∏è Bu i√ßerik AI/teknoloji ile ilgili g√∂r√ºnm√ºyor: {title[:50]}...")
        print(f"üîç AI i√ßerik: {has_ai_content}, AI olmayan i√ßerik: {has_non_ai_content}")
    
    analysis_result = {
        "innovation": "",
        "companies": [],
        "impact_level": 5,
        "audience": "General",
        "hashtags": [],
        "emojis": [],
        "tweet_text": ""
    }
    
    try:
        # 1. Main innovation/insight analysis (ENGLISH)
        innovation_prompt = f"""Briefly explain the main innovation or breakthrough in this AI/tech news (max 50 words, in English):\n\nTitle: {title}\nContent: {content[:800]}\n\nMain innovation:"""
        innovation = gemini_call(innovation_prompt, api_key, max_tokens=80)
        analysis_result["innovation"] = innovation.strip() if innovation != "API hatasƒ±" else "Technology innovation"
        
        # 2. Company analysis (ENGLISH)
        company_prompt = f"""List the main companies mentioned in this news (max 3, comma separated, in English):\n\nTitle: {title}\nContent: {content[:600]}\n\nCompanies:"""
        companies_text = gemini_call(company_prompt, api_key, max_tokens=50)
        if companies_text != "API hatasƒ±":
            companies = [c.strip() for c in companies_text.split(",") if c.strip()]
            analysis_result["companies"] = companies[:3]
        
        # 3. Impact level analysis (ENGLISH)
        impact_prompt = f"""Rate the impact of this news on the tech sector from 1 to 10 (just a number):\n\nTitle: {title}\nContent: {content[:600]}\n\nImpact score (1-10):"""
        impact_text = gemini_call(impact_prompt, api_key, max_tokens=10)
        try:
            impact_level = int(impact_text.strip().split()[0])
            if 1 <= impact_level <= 10:
                analysis_result["impact_level"] = impact_level
        except:
            analysis_result["impact_level"] = 5
        
        # 4. Audience analysis (ENGLISH)
        audience_prompt = f"""Determine the target audience for this news (Developer/Investor/General):\n\nTitle: {title}\nContent: {content[:500]}\n\nAudience:"""
        audience = gemini_call(audience_prompt, api_key, max_tokens=15)
        if audience != "API hatasƒ±" and audience.strip() in ["Developer", "Investor", "General"]:
            analysis_result["audience"] = audience.strip()
        
        # 5. Hashtag analysis (ENGLISH)
        hashtag_prompt = f"""Suggest the 3 most relevant hashtags for this news (in English, only hashtags, comma separated):\n\nTitle: {title}\nContent: {content[:800]}\n\nExample: #AI, #Technology, #Innovation\n\nHashtags:"""
        ai_hashtags_text = gemini_call(hashtag_prompt, api_key, max_tokens=50)
        ai_hashtags = []
        if ai_hashtags_text != "API hatasƒ±":
            clean_text = ai_hashtags_text.replace("Hashtags:", "").replace("Hashtag'ler:", "").replace("Hashtag'ler", "").strip()
            import re
            hashtag_matches = re.findall(r'#\w+', clean_text)
            if not hashtag_matches:
                words = re.findall(r'\b[A-Za-z][A-Za-z0-9]*\b', clean_text)
                for word in words[:3]:
                    if len(word) > 2:
                        ai_hashtags.append(f"#{word}")
            else:
                ai_hashtags = hashtag_matches[:3]
        # Smart hashtag system (ENGLISH)
        smart_hashtags = generate_smart_hashtags(title, content)
        combined_hashtags = []
        for tag in ai_hashtags[:3]:
            if tag not in combined_hashtags:
                combined_hashtags.append(tag)
        for tag in smart_hashtags:
            if tag not in combined_hashtags and len(combined_hashtags) < 3:
                combined_hashtags.append(tag)
        analysis_result["hashtags"] = combined_hashtags[:3]
        # 6. Emoji analysis (unchanged, universal)
        emoji_prompt = f"""Suggest the 3 most suitable emojis for this news (just emojis, no spaces):\n\nTitle: {title}\nContent: {content[:500]}\n\nEmojis:"""
        ai_emojis_text = gemini_call(emoji_prompt, api_key, max_tokens=20)
        ai_emojis = []
        if ai_emojis_text != "API hatasƒ±":
            import re
            emoji_pattern = re.compile(r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF\U00002702-\U000027B0\U000024C2-\U0001F251]+')
            found_emojis = emoji_pattern.findall(ai_emojis_text)
            for emoji in found_emojis:
                for single_emoji in emoji:
                    if single_emoji not in ai_emojis and len(ai_emojis) < 3:
                        ai_emojis.append(single_emoji)
        smart_emojis = generate_smart_emojis(title, content)
        combined_emojis = ai_emojis[:2]
        for emoji in smart_emojis:
            if emoji not in combined_emojis and len(combined_emojis) < 3:
                combined_emojis.append(emoji)
        analysis_result["emojis"] = combined_emojis[:3]
        print(f"‚úÖ Kapsamlƒ± analiz tamamlandƒ±:")
        print(f"üî¨ Innovation: {analysis_result['innovation'][:50]}...")
        print(f"üè¢ Companies: {', '.join(analysis_result['companies'])}")
        print(f"üéØ Audience: {analysis_result['audience']}")
        print(f"üè∑Ô∏è Hashtags: {' '.join(analysis_result['hashtags'])}")
        print(f"üòä Emojis: {''.join(analysis_result['emojis'])}")
        return analysis_result
    except Exception as e:
        print(f"‚ùå Kapsamlƒ± analiz hatasƒ±: {e}")
        return {
            "innovation": "AI/tech innovation",
            "companies": [],
            "impact_level": 5,
            "audience": "General",
            "hashtags": generate_smart_hashtags(title, content)[:3],
            "emojis": generate_smart_emojis(title, content)[:3],
            "tweet_text": ""
        }

def generate_ai_tweet_with_mcp_analysis(article_data, api_key):
    """MCP verisi ile geli≈ümi≈ü AI tweet olu≈üturma - Kapsamlƒ± analiz ile"""
    title = article_data.get("title", "")
    content = article_data.get("content", "")
    url = article_data.get("url", "")
    source = article_data.get("source", "unknown")
    
    # Twitter karakter limiti
    TWITTER_LIMIT = 280
    URL_LENGTH = 25  # "\n\nüîó " + URL kƒ±saltmasƒ± i√ßin
    
    print(f"ü§ñ AI ile tweet olu≈üturuluyor (kaynak: {source})...")
    
    try:
        # Kapsamlƒ± analiz yap
        analysis = generate_comprehensive_analysis(article_data, api_key)
        
        # Tweet metni olu≈ütur
        companies_text = ', '.join(analysis['companies'][:2]) if analysis['companies'] else ""
        
        tweet_prompt = f"""Create a compelling English tweet about this AI/tech breakthrough:

Title: {title[:120]}
Key Innovation: {analysis['innovation'][:120]}
Companies: {companies_text}

Requirements:
- Write in perfect English only
- Maximum 200 characters
- Make it clear, engaging and newsworthy
- Focus on WHAT changed and WHY it matters
- Use active voice and strong action verbs
- Include specific details when possible (numbers, capabilities)
- Make it accessible to general audience
- Sound exciting but credible
- Do NOT include hashtags, emojis, URLs, or impact levels (added separately)
- Do NOT mention impact, effect level, or rating in the tweet

Examples of good style:
- "OpenAI's new model achieves 95% accuracy in medical diagnosis"
- "Tesla's robot now performs complex assembly tasks autonomously"
- "Google's AI reduces data center energy consumption by 40%"

Tweet text:"""
        
        tweet_text = gemini_call(tweet_prompt, api_key, max_tokens=80)
        
        if tweet_text == "API hatasƒ±" or not tweet_text.strip():
            # Fallback tweet metni - daha anlamlƒ±
            if analysis['companies'] and analysis['innovation']:
                company = analysis['companies'][0]
                innovation = analysis['innovation'][:100]
                # Daha anlamlƒ± fallback tweet olu≈ütur
                if "launch" in innovation.lower():
                    tweet_text = f"{company} launches {innovation.lower().replace('launch', '').strip()}"
                elif "announce" in innovation.lower():
                    tweet_text = f"{company} announces {innovation.lower().replace('announce', '').strip()}"
                elif "develop" in innovation.lower():
                    tweet_text = f"{company} develops {innovation.lower().replace('develop', '').strip()}"
                else:
                    tweet_text = f"{company} unveils {innovation}"
            elif analysis['innovation']:
                tweet_text = f"Breaking: {analysis['innovation'][:150]}"
            else:
                tweet_text = f"AI breakthrough: {title[:120]}"
        
        # Tweet metnini temizle
        tweet_text = tweet_text.replace("Tweet:", "").replace("Tweet metni:", "").strip()
        
        # Gereksiz karakterleri temizle
        tweet_text = tweet_text.replace('"', '').replace("'", "'").strip()
        
        # Impact/etki bilgilerini temizle
        import re
        # "impact: orta", "etki: y√ºksek", "effect: medium" gibi ifadeleri kaldƒ±r
        tweet_text = re.sub(r'\b(impact|etki|effect)\s*:\s*\w+\b', '', tweet_text, flags=re.IGNORECASE)
        # "(impact: medium)", "[etki: y√ºksek]" gibi parantez i√ßindeki ifadeleri kaldƒ±r
        tweet_text = re.sub(r'[\(\[\{]\s*(impact|etki|effect)\s*:\s*\w+\s*[\)\]\}]', '', tweet_text, flags=re.IGNORECASE)
        # Fazla bo≈üluklarƒ± temizle
        tweet_text = re.sub(r'\s+', ' ', tweet_text).strip()
        
        # Hashtag ve emoji metinlerini olu≈ütur
        hashtag_text = " ".join(analysis['hashtags']).strip()
        emoji_text = "".join(analysis['emojis']).strip()
        url_part = f"\n\nüîó {url}"
        
        # Sabit kƒ±sƒ±mlarƒ±n uzunluƒüu
        fixed_parts_length = len(emoji_text) + len(hashtag_text) + len(url_part) + 2  # 2 bo≈üluk i√ßin
        available_chars = TWITTER_LIMIT - fixed_parts_length
        
        # Tweet metnini temizle ve kƒ±salt
        tweet_text = tweet_text.strip()
        
        # Eƒüer tweet metni √ßok uzunsa kƒ±salt
        if len(tweet_text) > available_chars:
            # "..." i√ßin 3 karakter ayƒ±r
            tweet_text = tweet_text[:available_chars-3] + "..."
        
        # Final tweet olu≈ütur - bo≈üluklarƒ± optimize et
        if emoji_text and tweet_text:
            # Emoji varsa emoji ile tweet arasƒ±nda tek bo≈üluk
            main_content = f"{emoji_text} {tweet_text}"
        else:
            # Emoji yoksa direkt tweet
            main_content = tweet_text
        
        if hashtag_text:
            # Hashtag varsa tek bo≈üluk ile ekle
            final_tweet = f"{main_content} {hashtag_text}{url_part}"
        else:
            # Hashtag yoksa direkt URL ekle
            final_tweet = f"{main_content}{url_part}"
        
        # Son g√ºvenlik kontrol√º - eƒüer hala uzunsa daha agresif kƒ±salt
        if len(final_tweet) > TWITTER_LIMIT:
            excess = len(final_tweet) - TWITTER_LIMIT
            # Tweet metninden fazlalƒ±ƒüƒ± √ßƒ±kar
            new_tweet_length = len(tweet_text) - excess - 3  # 3 "..." i√ßin
            if new_tweet_length > 10:  # Minimum 10 karakter bƒ±rak
                tweet_text = tweet_text[:new_tweet_length] + "..."
            else:
                # √áok kƒ±sa kalƒ±rsa hashtag'leri azalt
                hashtag_text = " ".join(analysis['hashtags'][:2])  # 2 hashtag
                fixed_parts_length = len(emoji_text) + len(hashtag_text) + len(url_part) + 1  # 1 bo≈üluk
                available_chars = TWITTER_LIMIT - fixed_parts_length
                tweet_text = tweet_text[:available_chars-3] + "..."
            
            # Yeniden olu≈ütur
            if emoji_text and tweet_text:
                main_content = f"{emoji_text} {tweet_text}"
            else:
                main_content = tweet_text
            
            if hashtag_text:
                final_tweet = f"{main_content} {hashtag_text}{url_part}"
            else:
                final_tweet = f"{main_content}{url_part}"
        
        print(f"‚úÖ AI analizi ile tweet olu≈üturuldu: {len(final_tweet)} karakter")
        print(f"üìù Tweet metni: {len(tweet_text)} karakter")
        print(f"üè∑Ô∏è AI Hashtag'ler: {hashtag_text} ({len(hashtag_text)} karakter)")
        print(f"üòä AI Emojiler: {emoji_text} ({len(emoji_text)} karakter)")
        print(f"üîó URL kƒ±smƒ±: {len(url_part)} karakter")
        print(f"üéØ Hedef Kitle: {analysis['audience']}")
        print(f"üìä Impact Score: 8 (varsayƒ±lan)")
        
        # Dictionary formatƒ±nda d√∂nd√ºr
        return {
            "tweet": final_tweet,
            "impact_score": 8,  # Varsayƒ±lan y√ºksek skor
            "analysis": analysis,
            "source": "mcp_analysis"
        }
        
    except Exception as e:
        print(f"‚ùå AI tweet olu≈üturma hatasƒ±: {e}")
        print("üîÑ Fallback y√∂nteme ge√ßiliyor...")
        fallback_tweet = generate_ai_tweet_with_content_fallback(article_data, api_key)
        return {
            "tweet": fallback_tweet,
            "impact_score": 6,  # Orta skor
            "analysis": {"audience": "General", "companies": [], "hashtags": [], "emojis": []},
            "source": "fallback"
        }

def generate_ai_tweet_with_content(article_data, api_key):
    """Ana tweet olu≈üturma fonksiyonu - MCP analizi √∂ncelikli"""
    try:
        # √ñnce MCP analizi ile dene
        tweet_data = generate_ai_tweet_with_mcp_analysis(article_data, api_key)
        
        # Eƒüer ba≈üarƒ±sƒ±zsa fallback kullan
        if not tweet_data or not tweet_data.get('tweet') or len(tweet_data.get('tweet', '')) < 50:
            print("üîÑ MCP analizi yetersiz, fallback y√∂ntemi deneniyor...")
            fallback_tweet = generate_ai_tweet_with_content_fallback(article_data, api_key)
            return {
                "tweet": fallback_tweet,
                "impact_score": 6,
                "analysis": {"audience": "General", "companies": [], "hashtags": [], "emojis": []},
                "source": "fallback"
            }
        
        return tweet_data
        
    except Exception as e:
        print(f"Ana tweet olu≈üturma hatasƒ±: {e}")
        fallback_tweet = generate_ai_tweet_with_content_fallback(article_data, api_key)
        return {
            "tweet": fallback_tweet,
            "impact_score": 6,
            "analysis": {"audience": "General", "companies": [], "hashtags": [], "emojis": []},
            "source": "fallback"
        }

def generate_ai_tweet_with_content_fallback(article_data, api_key):
    """Fallback tweet olu≈üturma - Eski y√∂ntem"""
    title = article_data.get("title", "")
    content = article_data.get("content", "")
    url = article_data.get("url", "")
    
    # Twitter karakter limiti (URL i√ßin 23 karakter ayrƒ±lƒ±r)
    TWITTER_LIMIT = 280
    URL_LENGTH = 25  # "\n\nüîó " + URL kƒ±saltmasƒ± i√ßin
    
    # Akƒ±llƒ± hashtag ve emoji olu≈ütur
    smart_hashtags = generate_smart_hashtags(title, content)
    smart_emojis = generate_smart_emojis(title, content)
    
    hashtag_text = " ".join(smart_hashtags).strip()
    emoji_text = "".join(smart_emojis).strip()
    
    # Hashtag ve emoji i√ßin yer ayƒ±r
    hashtag_emoji_length = len(hashtag_text) + len(emoji_text) + 2  # 2 bo≈üluk i√ßin
    MAX_CONTENT_LENGTH = TWITTER_LIMIT - URL_LENGTH - hashtag_emoji_length
    
    # ƒ∞ngilizce tweet i√ßin geli≈ümi≈ü prompt
    prompt = f"""Create a compelling English tweet about this AI/tech breakthrough:

Article Title: {title}
Article Content: {content[:1000]}

Requirements:
- Write in perfect English only
- Maximum {MAX_CONTENT_LENGTH} characters
- Make it clear, engaging and newsworthy
- Focus on WHAT changed and WHY it matters
- Use active voice and strong action verbs
- Include specific details when possible (numbers, capabilities, improvements)
- Make it accessible to general audience
- Sound exciting but credible
- Avoid jargon and technical terms
- Do NOT include hashtags, emojis, URLs, or impact levels (added separately)
- Do NOT mention impact, effect level, or rating in the tweet

Examples of good style:
- "OpenAI's new model achieves 95% accuracy in medical diagnosis"
- "Tesla's robot now performs complex assembly tasks autonomously"
- "Google's AI reduces data center energy consumption by 40%"
- "Meta's VR headset delivers 4K resolution at half the price"

Tweet text (max {MAX_CONTENT_LENGTH} chars):"""

    try:
        tweet_text = gemini_call(prompt, api_key, max_tokens=150)
        
        if tweet_text and len(tweet_text.strip()) > 10:
            # Tweet metnini temizle
            import re
            # Impact/etki bilgilerini temizle
            tweet_text = re.sub(r'\b(impact|etki|effect)\s*:\s*\w+\b', '', tweet_text, flags=re.IGNORECASE)
            tweet_text = re.sub(r'[\(\[\{]\s*(impact|etki|effect)\s*:\s*\w+\s*[\)\]\}]', '', tweet_text, flags=re.IGNORECASE)
            tweet_text = re.sub(r'\s+', ' ', tweet_text).strip()
            
            # Karakter limiti kontrol√º
            if len(tweet_text.strip()) > MAX_CONTENT_LENGTH:
                tweet_text = tweet_text.strip()[:MAX_CONTENT_LENGTH-3] + "..."
            
            # Emoji, tweet metni, hashtag'ler ve URL'yi birle≈ütir - bo≈üluklarƒ± optimize et
            parts = []
            if emoji_text:
                parts.append(emoji_text)
            if tweet_text.strip():
                parts.append(tweet_text.strip())
            if hashtag_text:
                parts.append(hashtag_text)
            
            main_content = " ".join(parts)
            final_tweet = f"{main_content}\n\nüîó {url}"
            
            # Final karakter kontrol√º
            if len(final_tweet) > TWITTER_LIMIT:
                # Tekrar kƒ±salt
                excess = len(final_tweet) - TWITTER_LIMIT
                tweet_text = tweet_text.strip()[:-(excess + 3)] + "..."
                
                # Yeniden birle≈ütir - bo≈üluklarƒ± optimize et
                parts = []
                if emoji_text:
                    parts.append(emoji_text)
                if tweet_text:
                    parts.append(tweet_text)
                if hashtag_text:
                    parts.append(hashtag_text)
                
                main_content = " ".join(parts)
                final_tweet = f"{main_content}\n\nüîó {url}"
            
            print(f"[FALLBACK] Tweet olu≈üturuldu: {len(final_tweet)} karakter (limit: {TWITTER_LIMIT})")
            print(f"[FALLBACK] Hashtag'ler: {hashtag_text}")
            print(f"[FALLBACK] Emojiler: {emoji_text}")
            
            return final_tweet
        else:
            print("[FALLBACK] API yanƒ±tƒ± yetersiz, basit fallback tweet olu≈üturuluyor...")
            return create_fallback_tweet(title, content, url)
            
    except Exception as e:
        print(f"Fallback tweet olu≈üturma hatasƒ±: {e}")
        print("[FALLBACK] API hatasƒ±, basit fallback tweet olu≈üturuluyor...")
        return create_fallback_tweet(title, content, url)

def create_fallback_tweet(title, content, url=""):
    """API hatasƒ± durumunda fallback tweet olu≈ütur - Akƒ±llƒ± hashtag ve emoji ile"""
    try:
        # Twitter karakter limiti
        TWITTER_LIMIT = 280
        URL_LENGTH = 25  # "\n\nüîó " + URL i√ßin
        
        # Akƒ±llƒ± hashtag ve emoji olu≈ütur
        smart_hashtags = generate_smart_hashtags(title, content)
        smart_emojis = generate_smart_emojis(title, content)
        
        hashtag_text = " ".join(smart_hashtags).strip()
        emoji_text = "".join(smart_emojis).strip()
        
        # Hashtag ve emoji i√ßin yer ayƒ±r
        hashtag_emoji_length = len(hashtag_text) + len(emoji_text) + 2  # 2 bo≈üluk i√ßin
        MAX_CONTENT_LENGTH = TWITTER_LIMIT - URL_LENGTH - hashtag_emoji_length
        
        # Ba≈ülƒ±ƒüƒ± temizle
        clean_title = title.strip()
        
        # ƒ∞√ßerikten anahtar kelimeler ve √∂nemli bilgiler √ßƒ±kar
        content_lower = content.lower()
        title_lower = title.lower()
        combined_text = f"{title_lower} {content_lower}"
        
        # Sayƒ±sal bilgileri √ßƒ±kar
        import re
        numbers = re.findall(r'\$?(\d+(?:\.\d+)?)\s*(billion|million|%|percent)', combined_text, re.IGNORECASE)
        
        # ≈ûirket isimlerini tespit et
        companies = []
        company_names = ["OpenAI", "Google", "Microsoft", "Meta", "Apple", "Amazon", "Tesla", "Nvidia", "Anthropic", "Perplexity", "Cursor", "DeviantArt", "AMD", "Intel"]
        for company in company_names:
            if company.lower() in combined_text:
                companies.append(company)
        
        # Ana tweet metni olu≈ütur - daha anlamlƒ± ƒ∞ngilizce
        tweet_parts = []
        
        # ≈ûirket ve eylem bazlƒ± tweet olu≈ütur
        if companies:
            main_company = companies[0]
            
            # Eyleme g√∂re anlamlƒ± c√ºmle olu≈ütur
            if "acquisition" in combined_text or "acquire" in combined_text:
                if "billion" in combined_text:
                    tweet_parts.append(f"{main_company} completes major acquisition")
                else:
                    tweet_parts.append(f"{main_company} acquires strategic company")
            elif "funding" in combined_text or "investment" in combined_text:
                if numbers:
                    largest_num = max(numbers, key=lambda x: float(x[0]))
                    if largest_num[1].lower() == 'billion':
                        tweet_parts.append(f"{main_company} raises ${largest_num[0]}B in funding")
                    elif largest_num[1].lower() == 'million':
                        tweet_parts.append(f"{main_company} secures ${largest_num[0]}M investment")
                    else:
                        tweet_parts.append(f"{main_company} secures major funding")
                else:
                    tweet_parts.append(f"{main_company} secures new funding round")
            elif "launch" in combined_text or "release" in combined_text:
                if "ai" in combined_text or "artificial intelligence" in combined_text:
                    tweet_parts.append(f"{main_company} launches new AI technology")
                elif "robot" in combined_text:
                    tweet_parts.append(f"{main_company} unveils advanced robotics")
                else:
                    tweet_parts.append(f"{main_company} releases breakthrough innovation")
            elif "partnership" in combined_text or "partner" in combined_text:
                tweet_parts.append(f"{main_company} forms strategic partnership")
            elif "breakthrough" in combined_text or "innovation" in combined_text:
                tweet_parts.append(f"{main_company} achieves major breakthrough")
            else:
                # Ba≈ülƒ±ƒüƒ± kullan ama ≈üirket adƒ±nƒ± √∂ne √ßƒ±kar
                clean_title_short = clean_title.replace(main_company, "").strip()
                if clean_title_short:
                    tweet_parts.append(f"{main_company}: {clean_title_short[:80]}")
                else:
                    tweet_parts.append(f"{main_company} makes major announcement")
        else:
            # ≈ûirket yoksa ba≈ülƒ±ƒüƒ± kullan
            if "ai" in combined_text or "artificial intelligence" in combined_text:
                tweet_parts.append(f"AI breakthrough: {clean_title[:100]}")
            elif "robot" in combined_text:
                tweet_parts.append(f"Robotics advance: {clean_title[:100]}")
            else:
                tweet_parts.append(f"Tech news: {clean_title[:120]}")
        
        # Sayƒ±sal bilgi ekle (eƒüer hen√ºz eklenmemi≈üse)
        if numbers and not any("$" in part for part in tweet_parts):
            largest_num = max(numbers, key=lambda x: float(x[0]))
            if largest_num[1].lower() == 'billion':
                tweet_parts.append(f"(${largest_num[0]}B)")
            elif largest_num[1].lower() == 'million':
                tweet_parts.append(f"({largest_num[0]}M)")
            elif largest_num[1].lower() in ['%', 'percent']:
                tweet_parts.append(f"({largest_num[0]}% improvement)")
        
        # Tweet'i birle≈ütir
        main_text = " ".join(tweet_parts)
        
        # Karakter limiti kontrol√º
        if len(main_text) > MAX_CONTENT_LENGTH:
            # √áok uzunsa kƒ±salt
            main_text = main_text[:MAX_CONTENT_LENGTH-3] + "..."
        
        # Emoji, tweet metni, hashtag'ler ve URL'yi birle≈ütir
        if url:
            fallback_tweet = f"{emoji_text} {main_text} {hashtag_text}\n\nüîó {url}"
        else:
            fallback_tweet = f"{emoji_text} {main_text} {hashtag_text}"
        
        # Final karakter kontrol√º
        if len(fallback_tweet) > TWITTER_LIMIT:
            # Tekrar kƒ±salt
            excess = len(fallback_tweet) - TWITTER_LIMIT
            main_text = main_text[:-(excess + 3)] + "..."
            if url:
                fallback_tweet = f"{emoji_text} {main_text} {hashtag_text}\n\nüîó {url}"
            else:
                fallback_tweet = f"{emoji_text} {main_text} {hashtag_text}"
        
        print(f"[FALLBACK] Tweet olu≈üturuldu: {len(fallback_tweet)} karakter (limit: {TWITTER_LIMIT})")
        print(f"[FALLBACK] Hashtag'ler: {hashtag_text}")
        print(f"[FALLBACK] Emojiler: {emoji_text}")
        
        return fallback_tweet
        
    except Exception as e:
        print(f"Fallback tweet olu≈üturma hatasƒ±: {e}")
        # En basit fallback - akƒ±llƒ± hashtag ve emoji ile
        try:
            simple_hashtags = generate_smart_hashtags(title, "")[:3]  # 3 hashtag
            simple_emojis = generate_smart_emojis(title, "")[:2]  # 2 emoji
            
            hashtag_text = " ".join(simple_hashtags)
            emoji_text = "".join(simple_emojis)
            
            # Karakter hesaplama
            url_length = len(f"\n\nüîó {url}") if url else 0
            available_chars = TWITTER_LIMIT - url_length - len(hashtag_text) - len(emoji_text) - 2
            
            # Ba≈ülƒ±ƒüƒ± kƒ±salt
            if len(title) > available_chars:
                title_text = title[:available_chars-3] + "..."
            else:
                title_text = title
            
            simple_tweet = f"{emoji_text} {title_text} {hashtag_text}"
            if url:
                simple_tweet += f"\n\nüîó {url}"
            
            return simple_tweet
            
        except:
            # En son √ßare - basit tweet
            simple_text = f"ü§ñ {title[:200]}... #AI #Innovation #Technology"
            if url:
                simple_tweet = f"{simple_text}\n\nüîó {url}"
            else:
                simple_tweet = simple_text
            
            # Karakter limiti kontrol√º
            if len(simple_tweet) > TWITTER_LIMIT:
                available = TWITTER_LIMIT - len("\n\nüîó ") - len(url) - len(" #AI #Innovation #Technology") - 3
                simple_text = f"ü§ñ {title[:available]}... #AI #Innovation #Technology"
                simple_tweet = f"{simple_text}\n\nüîó {url}" if url else simple_text
            
            return simple_tweet

def setup_twitter_api():
    import tweepy
    import os
    # V1.1 API ile oturum a√ß
    auth = tweepy.OAuth1UserHandler(
        os.environ['TWITTER_API_KEY'],
        os.environ['TWITTER_API_SECRET'],
        os.environ['TWITTER_ACCESS_TOKEN'],
        os.environ['TWITTER_ACCESS_TOKEN_SECRET']
    )
    api = tweepy.API(auth)
    return api

def post_tweet(tweet_text, article_title=""):
    """X platformunda tweet payla≈üma ve Gmail bildirimi - Twitter API v2 kullanarak"""
    try:
        # Twitter API v2 kullan
        tweet_result = post_text_tweet_v2(tweet_text)
        
        # Ba≈üarƒ±sƒ±z olursa hata d√∂nd√ºr
        if not tweet_result.get("success"):
            safe_log(f"Tweet payla≈üƒ±m hatasƒ±: {tweet_result.get('error', 'Bilinmeyen hata')}", "ERROR")
            return tweet_result
        
        tweet_id = tweet_result.get("tweet_id")
        tweet_url = tweet_result.get("url")
        
        # Gmail bildirimi g√∂nder (Telegram yerine)
        email_sent = False
        try:
            gmail_result = send_gmail_notification(
                message=tweet_text,
                tweet_url=tweet_url,
                article_title=article_title
            )
            if gmail_result.get("success"):
                safe_log(f"Gmail bildirimi g√∂nderildi: {gmail_result.get('email')}", "INFO")
                email_sent = True
            else:
                safe_log(f"Gmail bildirimi g√∂nderilemedi: {gmail_result.get('reason', 'unknown')}", "WARNING")
        except Exception as gmail_error:
            safe_log(f"Gmail bildirim hatasƒ±: {gmail_error}", "ERROR")
        
        # Fallback: Telegram bildirimi (eƒüer Gmail ba≈üarƒ±sƒ±z olursa)
        telegram_sent = False
        if not email_sent:
            try:
                telegram_result = send_telegram_notification(
                    message=tweet_text,
                    tweet_url=tweet_url,
                    article_title=article_title
                )
                if telegram_result.get("success"):
                    safe_log("Fallback Telegram bildirimi g√∂nderildi", "INFO")
                    telegram_sent = True
                else:
                    safe_log(f"Fallback Telegram bildirimi de ba≈üarƒ±sƒ±z: {telegram_result.get('reason', 'unknown')}", "WARNING")
            except Exception as telegram_error:
                safe_log(f"Fallback Telegram bildirim hatasƒ±: {telegram_error}", "ERROR")
        
        return {
            "success": True,
            "tweet_id": tweet_id,
            "url": tweet_url,
            "email_sent": email_sent,
            "telegram_sent": telegram_sent
        }
        
    except Exception as e:
        safe_log(f"Tweet payla≈üƒ±m hatasƒ±: {str(e)}", "ERROR")
        return {"success": False, "error": f"Tweet payla≈üƒ±m hatasƒ±: {str(e)}"}

def mark_article_as_posted(article_data, tweet_result):
    """Makaleyi payla≈üƒ±ldƒ± olarak i≈üaretle - API ve manuel payla≈üƒ±mlarƒ± destekler"""
    try:
        posted_articles = load_json(HISTORY_FILE)
        
        # Manuel payla≈üƒ±m kontrol√º
        is_manual_post = tweet_result.get("manual_post", False)
        
        posted_article = {
            "title": article_data.get("title", ""),
            "url": article_data.get("url", ""),
            "hash": article_data.get("hash", ""),
            "posted_date": datetime.now().isoformat(),
            "tweet_id": tweet_result.get("tweet_id", ""),
            "tweet_url": tweet_result.get("url", ""),
            "manual_post": is_manual_post,
            "post_method": "manuel" if is_manual_post else "api"
        }
        
        # Manuel payla≈üƒ±m i√ßin ek bilgiler
        if is_manual_post:
            posted_article["tweet_text"] = article_data.get("tweet_text", "")
            posted_article["manual_posted_at"] = tweet_result.get("posted_at", datetime.now().isoformat())
        
        posted_articles.append(posted_article)
        save_json(HISTORY_FILE, posted_articles)
        
        safe_log(f"Makale kaydedildi: {article_data.get('title', '')[:50]}... (Y√∂ntem: {posted_article['post_method']})", "INFO")
        
        return True
    except Exception as e:
        safe_log(f"Makale kaydetme hatasƒ±: {e}", "ERROR")
        return False

def check_duplicate_articles():
    """Tekrarlanan makaleleri temizle"""
    try:
        posted_articles = load_json(HISTORY_FILE)
        
        # Son 30 g√ºnl√ºk makaleleri tut
        cutoff_date = datetime.now() - timedelta(days=30)
        
        filtered_articles = []
        seen_hashes = set()
        
        for article in posted_articles:
            try:
                posted_date = datetime.fromisoformat(article.get("posted_date", ""))
                article_hash = article.get("hash", "")
                
                if posted_date > cutoff_date and article_hash not in seen_hashes:
                    filtered_articles.append(article)
                    seen_hashes.add(article_hash)
            except:
                continue
        
        save_json(HISTORY_FILE, filtered_articles)
        return len(posted_articles) - len(filtered_articles)
        
    except Exception as e:
        print(f"Tekrar temizleme hatasƒ±: {e}")
        return 0

def generate_ai_digest(summaries_with_links, api_key):
    """Eski fonksiyon - geriye d√∂n√ºk uyumluluk i√ßin"""
    if not summaries_with_links:
        return "√ñzet bulunamadƒ±"
    
    # ƒ∞lk makaleyi kullanarak tweet olu≈ütur
    first_summary = summaries_with_links[0]
    article_data = {
        "title": "AI Digest",
        "content": first_summary.get("summary", ""),
        "url": first_summary.get("url", "")
    }
    
    return generate_ai_tweet_with_content(article_data, api_key)

def create_pdf(summaries, filename="daily_digest.pdf"):
    pdf = FPDF()
    pdf.add_page()
    pdf.set_font("Arial", size=12)
    pdf.cell(200, 10, txt="AI Tweet Digest", ln=True, align='C')
    for s in summaries:
        pdf.multi_cell(0, 10, f"‚Ä¢ {s}")
    pdf.output(filename)
    return filename

def get_posted_articles_summary():
    """Payla≈üƒ±lmƒ±≈ü makalelerin √∂zetini d√∂nd√ºr - geli≈ümi≈ü istatistiklerle"""
    try:
        from datetime import datetime, date, timedelta
        posted_articles = load_json(HISTORY_FILE)
        today = date.today()
        
        # Son 7 g√ºnl√ºk makaleleri al
        cutoff_date = datetime.now() - timedelta(days=7)
        recent_articles = []
        today_articles = []
        week_articles = []
        
        # En y√ºksek skorlu makale
        highest_scored = None
        highest_score = 0
        
        # En son payla≈üƒ±lan makale
        latest_posted = None
        latest_date = None
        
        for article in posted_articles:
            try:
                posted_date_str = article.get("posted_date", "")
                if posted_date_str:
                    posted_date = datetime.fromisoformat(posted_date_str.replace('Z', '+00:00'))
                    
                    # Bug√ºnk√º makaleler
                    if posted_date.date() == today:
                        today_articles.append(article)
                    
                    # Son 7 g√ºnl√ºk makaleler
                    if posted_date > cutoff_date:
                        recent_articles.append(article)
                        week_articles.append(article)
                    
                    # En son payla≈üƒ±lan
                    if latest_date is None or posted_date > latest_date:
                        latest_date = posted_date
                        latest_posted = article
                
                # En y√ºksek skorlu makale (eƒüer skor varsa)
                score = article.get("score", 0)
                if isinstance(score, (int, float)) and score > highest_score:
                    highest_score = score
                    highest_scored = article
                    
            except Exception as e:
                print(f"[DEBUG] Makale parse hatasƒ±: {e}")
                continue
        
        # Ba≈üarƒ± oranƒ± hesapla (basit: payla≈üƒ±lan / toplam)
        success_rate = (len(posted_articles) / max(len(posted_articles) + 1, 1)) * 100
        
        # Kategori daƒüƒ±lƒ±mƒ± (eƒüer varsa)
        category_distribution = {}
        for article in posted_articles:
            category = article.get("category", "Genel")
            category_distribution[category] = category_distribution.get(category, 0) + 1
        
        return {
            "total_posted": len(posted_articles),
            "recent_posted": len(recent_articles),
            "recent_articles": recent_articles[-5:],  # Son 5 makale
            "today_articles": len(today_articles),
            "week_articles": len(week_articles),
            "highest_scored": highest_scored,
            "latest_posted": latest_posted,
            "success_rate": success_rate,
            "category_distribution": category_distribution,
            "average_score": highest_score / max(len(posted_articles), 1) if highest_score > 0 else 0,
            "last_check_time": datetime.now().strftime("%H:%M") if posted_articles else "Hen√ºz yok"
        }
        
    except Exception as e:
        print(f"Payla≈üƒ±lmƒ±≈ü makale √∂zeti hatasƒ±: {e}")
        return {
            "total_posted": 0, 
            "recent_posted": 0, 
            "recent_articles": [],
            "today_articles": 0,
            "week_articles": 0,
            "highest_scored": None,
            "latest_posted": None,
            "success_rate": 0,
            "category_distribution": {},
            "average_score": 0,
            "last_check_time": "Hen√ºz yok"
        }

def create_automatic_backup():
    """G√ºnl√ºk otomatik yedekleme olu≈ütur"""
    try:
        import shutil
        from datetime import datetime
        
        # Bug√ºn√ºn tarihi
        today = datetime.now().strftime('%Y%m%d')
        backup_dir = f"daily_backup_{today}"
        
        # Eƒüer bug√ºnk√º yedekleme zaten varsa atla
        if os.path.exists(backup_dir):
            return {"success": True, "message": "Bug√ºnk√º yedekleme zaten mevcut", "backup_dir": backup_dir}
        
        os.makedirs(backup_dir, exist_ok=True)
        
        files_to_backup = [
            "posted_articles.json",
            "pending_tweets.json", 
            "automation_settings.json",
            "news_sources.json"
        ]
        
        backup_count = 0
        for file_path in files_to_backup:
            if os.path.exists(file_path):
                backup_path = os.path.join(backup_dir, file_path)
                shutil.copy2(file_path, backup_path)
                backup_count += 1
        
        return {
            "success": True,
            "message": f"‚úÖ G√ºnl√ºk yedekleme olu≈üturuldu: {backup_count} dosya",
            "backup_dir": backup_dir
        }
        
    except Exception as e:
        return {
            "success": False,
            "message": f"‚ùå Yedekleme hatasƒ±: {str(e)}"
        }

def reset_all_data():
    """T√ºm uygulama verilerini sƒ±fƒ±rla - Otomatik yedekleme ile"""
    try:
        import shutil
        from datetime import datetime
        
        # Yedekleme klas√∂r√º olu≈ütur
        backup_dir = f"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        os.makedirs(backup_dir, exist_ok=True)
        
        files_to_reset = [
            "posted_articles.json",
            "pending_tweets.json", 
            "summaries.json",
            "hashtags.json",
            "accounts.json",
            "automation_settings.json",
            "news_sources.json"
        ]
        
        # √ñnce yedekle
        backup_count = 0
        for file_path in files_to_reset:
            if os.path.exists(file_path):
                backup_path = os.path.join(backup_dir, file_path)
                shutil.copy2(file_path, backup_path)
                backup_count += 1
                print(f"üì¶ {file_path} yedeklendi -> {backup_path}")
        
        # Sonra sƒ±fƒ±rla
        reset_count = 0
        for file_path in files_to_reset:
            if os.path.exists(file_path):
                save_json(file_path, [])
                reset_count += 1
                print(f"‚úÖ {file_path} sƒ±fƒ±rlandƒ±")
            else:
                # Dosya yoksa bo≈ü olu≈ütur
                save_json(file_path, [])
                print(f"üÜï {file_path} olu≈üturuldu")
        
        return {
            "success": True,
            "message": f"‚úÖ {reset_count} dosya sƒ±fƒ±rlandƒ±, {backup_count} dosya yedeklendi ({backup_dir})",
            "reset_files": files_to_reset,
            "backup_dir": backup_dir
        }
        
    except Exception as e:
        return {
            "success": False,
            "message": f"‚ùå Sƒ±fƒ±rlama hatasƒ±: {str(e)}",
            "reset_files": []
        }

def clear_pending_tweets():
    """Sadece bekleyen tweet'leri temizle"""
    try:
        pending_tweets = load_json("pending_tweets.json")
        
        # Sadece posted olanlarƒ± tut, pending olanlarƒ± sil
        posted_tweets = [t for t in pending_tweets if t.get("status") == "posted"]
        
        save_json("pending_tweets.json", posted_tweets)
        
        cleared_count = len(pending_tweets) - len(posted_tweets)
        
        return {
            "success": True,
            "message": f"‚úÖ {cleared_count} bekleyen tweet temizlendi",
            "cleared_count": cleared_count
        }
        
    except Exception as e:
        return {
            "success": False,
            "message": f"‚ùå Temizleme hatasƒ±: {str(e)}",
            "cleared_count": 0
        }

def get_data_statistics():
    """Veri istatistiklerini d√∂nd√ºr - bug√ºnk√º analizler dahil"""
    try:
        from datetime import datetime, date, timedelta
        today = date.today()
        stats = {}
        
        # Payla≈üƒ±lan makaleler
        posted_articles = load_json("posted_articles.json")
        stats["posted_articles"] = len(posted_articles)
        stats["total_articles"] = len(posted_articles)  # Template uyumluluƒüu i√ßin
        
        # Bug√ºnk√º payla≈üƒ±lan makaleler (silinmemi≈ü olanlar)
        today_articles = []
        active_articles = []
        deleted_articles = []
        
        for article in posted_articles:
            # Silinmi≈ü/aktif ayrƒ±mƒ±
            if article.get("deleted", False):
                deleted_articles.append(article)
            else:
                active_articles.append(article)
            
            # Bug√ºnk√º makaleler (sadece aktif olanlar)
            if article.get("posted_date") and not article.get("deleted", False):
                try:
                    # ISO format tarihini parse et
                    posted_date_str = article["posted_date"]
                    # Z suffix'ini kaldƒ±r ve parse et
                    if posted_date_str.endswith('Z'):
                        posted_date_str = posted_date_str[:-1] + '+00:00'
                    
                    posted_date = datetime.fromisoformat(posted_date_str)
                    article_date = posted_date.date()
                    
                    if article_date == today:
                        today_articles.append(article)
                        
                except (ValueError, TypeError) as e:
                    print(f"[DEBUG] Tarih parse hatasƒ±: {e} - {article.get('posted_date')}")
                    continue
        
        # D√ºnk√º makaleler de hesapla
        yesterday = today - timedelta(days=1)
        yesterday_articles = []
        
        for article in active_articles:
            if article.get("posted_date"):
                try:
                    posted_date_str = article["posted_date"]
                    if posted_date_str.endswith('Z'):
                        posted_date_str = posted_date_str[:-1] + '+00:00'
                    
                    posted_date = datetime.fromisoformat(posted_date_str)
                    article_date = posted_date.date()
                    
                    if article_date == yesterday:
                        yesterday_articles.append(article)
                        
                except (ValueError, TypeError):
                    continue
        
        stats["today_articles"] = len(today_articles)
        stats["yesterday_articles"] = len(yesterday_articles)
        stats["active_articles"] = len(active_articles)
        stats["deleted_articles"] = len(deleted_articles)
        
        # Bekleyen tweet'ler
        pending_tweets = load_json("pending_tweets.json")
        pending_count = len([t for t in pending_tweets if t.get("status") == "pending"])
        posted_count = len([t for t in pending_tweets if t.get("status") == "posted"])
        stats["pending_tweets"] = pending_count
        stats["posted_tweets_in_pending"] = posted_count
        
        # Bug√ºnk√º bekleyen tweet'ler
        today_pending = []
        for tweet in pending_tweets:
            # created_date veya created_at alanƒ±nƒ± kontrol et
            date_field = tweet.get("created_date") or tweet.get("created_at")
            if date_field:
                try:
                    created_date = datetime.fromisoformat(date_field.replace('Z', '+00:00'))
                    # Status kontrol√º - eƒüer status yoksa pending kabul et
                    tweet_status = tweet.get("status", "pending")
                    if created_date.date() == today and tweet_status == "pending":
                        today_pending.append(tweet)
                except (ValueError, TypeError):
                    continue
        
        stats["today_pending"] = len(today_pending)
        
        # Debug i√ßin log ekle
        print(f"[DEBUG] Bug√ºnk√º pending tweet'ler: {len(today_pending)} / {len(pending_tweets)} toplam")
        
        # √ñzetler
        summaries = load_json("summaries.json")
        stats["summaries"] = len(summaries)
        
        # Hashtag'ler
        hashtags = load_json("hashtags.json")
        stats["hashtags"] = len(hashtags)
        
        # Hesaplar
        accounts = load_json("accounts.json")
        stats["accounts"] = len(accounts)
        
        # Bug√ºnk√º toplam aktivite
        stats["today_total_activity"] = stats["today_articles"] + stats["today_pending"]
        
        # Dosya boyutlarƒ±nƒ± hesapla
        import os
        
        def format_file_size(size_bytes):
            """Dosya boyutunu okunabilir formata √ßevir"""
            if size_bytes == 0:
                return "0 B"
            size_names = ["B", "KB", "MB", "GB"]
            i = 0
            while size_bytes >= 1024 and i < len(size_names) - 1:
                size_bytes /= 1024.0
                i += 1
            return f"{size_bytes:.1f} {size_names[i]}"
        
        def get_file_size(filename):
            """Dosya boyutunu g√ºvenli ≈üekilde al"""
            try:
                if os.path.exists(filename):
                    size = os.path.getsize(filename)
                    return format_file_size(size)
                else:
                    return "Dosya yok"
            except Exception as e:
                return f"Hata: {str(e)}"
        
        # Dosya boyutlarƒ±nƒ± hesapla
        stats["articles_file_size"] = get_file_size("posted_articles.json")
        stats["pending_file_size"] = get_file_size("pending_tweets.json")
        stats["settings_file_size"] = get_file_size("automation_settings.json")
        
        print(f"[DEBUG] ƒ∞statistikler: Bug√ºn {stats['today_articles']} payla≈üƒ±m, {stats['today_pending']} bekleyen (Payla≈üƒ±m tarihine g√∂re)")
        
        return stats
        
    except Exception as e:
        print(f"ƒ∞statistik hatasƒ±: {e}")
        return {
            "posted_articles": 0,
            "total_articles": 0,
            "today_articles": 0,
            "pending_tweets": 0,
            "today_pending": 0,
            "posted_tweets_in_pending": 0,
            "summaries": 0,
            "hashtags": 0,
            "accounts": 0,
            "today_total_activity": 0,
            "articles_file_size": "N/A",
            "pending_file_size": "N/A",
            "settings_file_size": "N/A"
        }

def load_automation_settings():
    """Otomatikle≈ütirme ayarlarƒ±nƒ± y√ºkle"""
    try:
        settings_data = load_json("automation_settings.json")
        
        # Eƒüer liste ise (eski format), bo≈ü dict d√∂nd√ºr
        if isinstance(settings_data, list):
            settings = {}
        else:
            settings = settings_data
        
        # Varsayƒ±lan ayarlar
        default_settings = {
            "auto_mode": False,
            "min_score": 5,
            "check_interval_hours": 3,
            "max_articles_per_run": 10,
            "auto_post_enabled": False,
            "require_manual_approval": True,
            "working_hours_only": False,
            "working_hours_start": "09:00",
            "working_hours_end": "18:00",
            "weekend_enabled": True,
            "last_updated": datetime.now().isoformat()
        }
        
        # Eksik ayarlarƒ± varsayƒ±lanlarla doldur
        for key, value in default_settings.items():
            if key not in settings:
                settings[key] = value
        
        return settings
        
    except Exception as e:
        print(f"Ayarlar y√ºkleme hatasƒ±: {e}")
        # Varsayƒ±lan ayarlarƒ± d√∂nd√ºr
        return {
            "auto_mode": False,
            "min_score": 5,
            "check_interval_hours": 3,
            "max_articles_per_run": 10,
            "auto_post_enabled": False,
            "require_manual_approval": True,
            "working_hours_only": False,
            "working_hours_start": "09:00",
            "working_hours_end": "18:00",
            "weekend_enabled": True,
            "last_updated": datetime.now().isoformat()
        }

def save_automation_settings(settings):
    """Otomatikle≈ütirme ayarlarƒ±nƒ± kaydet"""
    try:
        settings["last_updated"] = datetime.now().isoformat()
        save_json("automation_settings.json", settings)
        return {"success": True, "message": "‚úÖ Ayarlar ba≈üarƒ±yla kaydedildi"}
    except Exception as e:
        return {"success": False, "message": f"‚ùå Ayarlar kaydedilemedi: {e}"}

def get_automation_status():
    """Otomatikle≈ütirme durumunu kontrol et"""
    try:
        settings = load_automation_settings()
        
        # √áalƒ±≈üma saatleri kontrol√º
        if settings.get("working_hours_only", False):
            from datetime import datetime, time
            now = datetime.now()
            start_time = datetime.strptime(settings.get("working_hours_start", "09:00"), "%H:%M").time()
            end_time = datetime.strptime(settings.get("working_hours_end", "18:00"), "%H:%M").time()
            
            current_time = now.time()
            is_working_hours = start_time <= current_time <= end_time
            
            # Hafta sonu kontrol√º
            is_weekend = now.weekday() >= 5  # 5=Cumartesi, 6=Pazar
            weekend_allowed = settings.get("weekend_enabled", True)
            
            if is_weekend and not weekend_allowed:
                return {
                    "active": False,
                    "auto_mode": False,
                    "check_interval_hours": settings.get("check_interval_hours", 2),
                    "min_score_threshold": settings.get("min_score_threshold", 5),
                    "reason": "Hafta sonu √ßalƒ±≈üma devre dƒ±≈üƒ±",
                    "settings": settings
                }
            
            if not is_working_hours:
                return {
                    "active": False,
                    "auto_mode": False,
                    "check_interval_hours": settings.get("check_interval_hours", 2),
                    "min_score_threshold": settings.get("min_score_threshold", 5),
                    "reason": f"√áalƒ±≈üma saatleri dƒ±≈üƒ±nda ({start_time}-{end_time})",
                    "settings": settings
                }
        
        return {
            "active": settings.get("auto_mode", False),
            "auto_mode": settings.get("auto_mode", False),  # Template uyumluluƒüu i√ßin
            "check_interval_hours": settings.get("check_interval_hours", 2),
            "min_score_threshold": settings.get("min_score_threshold", 5),
            "reason": "Aktif" if settings.get("auto_mode", False) else "Manuel mod",
            "settings": settings
        }
        
    except Exception as e:
        return {
            "active": False,
            "auto_mode": False,
            "check_interval_hours": 2,
            "min_score_threshold": 5,
            "reason": f"Hata: {e}",
            "settings": {}
        }

def update_scheduler_settings():
    """Scheduler ayarlarƒ±nƒ± g√ºncelle (scheduler.py i√ßin)"""
    try:
        settings = load_automation_settings()
        
        # Scheduler i√ßin ayarlar dosyasƒ± olu≈ütur
        scheduler_config = {
            "auto_mode": settings.get("auto_mode", False),
            "min_score": settings.get("min_score", 5),
            "check_interval_hours": settings.get("check_interval_hours", 3),
            "max_articles_per_run": settings.get("max_articles_per_run", 10),
            "auto_post_enabled": settings.get("auto_post_enabled", False),
            "last_updated": datetime.now().isoformat()
        }
        
        save_json("scheduler_config.json", scheduler_config)
        return {"success": True, "message": "Scheduler ayarlarƒ± g√ºncellendi"}
        
    except Exception as e:
        return {"success": False, "message": f"Scheduler ayarlarƒ± g√ºncellenemedi: {e}"}

def validate_automation_settings(settings):
    """Otomatikle≈ütirme ayarlarƒ±nƒ± doƒürula"""
    errors = []
    
    # Minimum skor kontrol√º
    min_score = settings.get("min_score", 5)
    if not isinstance(min_score, int) or min_score < 1 or min_score > 10:
        errors.append("Minimum skor 1-10 arasƒ±nda olmalƒ±")
    
    # Kontrol aralƒ±ƒüƒ± kontrol√º
    interval = settings.get("check_interval_hours", 3)
    if not isinstance(interval, (int, float)) or interval < 0.5 or interval > 24:
        errors.append("Kontrol aralƒ±ƒüƒ± 0.5-24 saat arasƒ±nda olmalƒ±")
    
    # Maksimum makale sayƒ±sƒ± kontrol√º
    max_articles = settings.get("max_articles_per_run", 10)
    if not isinstance(max_articles, int) or max_articles < 1 or max_articles > 50:
        errors.append("Maksimum makale sayƒ±sƒ± 1-50 arasƒ±nda olmalƒ±")
    
    # √áalƒ±≈üma saatleri kontrol√º
    try:
        start_time = settings.get("working_hours_start", "09:00")
        end_time = settings.get("working_hours_end", "18:00")
        datetime.strptime(start_time, "%H:%M")
        datetime.strptime(end_time, "%H:%M")
    except ValueError:
        errors.append("√áalƒ±≈üma saatleri HH:MM formatƒ±nda olmalƒ±")
    

    
    return errors

def send_telegram_notification(message, tweet_url="", article_title=""):
    """Telegram bot'a bildirim g√∂nder - Bot token env'den, Chat ID settings'den"""
    try:
        # Bot token'ƒ± environment variable'dan √ßek
        bot_token = os.getenv("TELEGRAM_BOT_TOKEN", "").strip()
        
        # Chat ID'yi settings'den √ßek
        settings = load_automation_settings()
        chat_id = settings.get("telegram_chat_id", "").strip()
        
        # Eƒüer bot token env'de yoksa settings'den dene (geriye d√∂n√ºk uyumluluk)
        if not bot_token:
            bot_token = settings.get("telegram_bot_token", "").strip()
        
        # Telegram bildirimleri kapalƒ± mƒ± kontrol et
        if not settings.get("telegram_notifications", True):  # Varsayƒ±lan True
            print("[DEBUG] Telegram bildirimleri kapalƒ±")
            return {"success": False, "reason": "disabled"}
        
        if not bot_token:
            safe_log("Telegram bot token eksik. .env dosyasƒ±nda TELEGRAM_BOT_TOKEN ayarlayƒ±n.", "WARNING")
            return {"success": False, "reason": "missing_bot_token"}
            
        if not chat_id:
            print("[INFO] Chat ID eksik, otomatik algƒ±lama deneniyor...")
            # Otomatik Chat ID algƒ±lamayƒ± dene
            auto_result = auto_detect_and_save_chat_id()
            
            if auto_result["success"]:
                chat_id = auto_result["chat_id"]
                print(f"[SUCCESS] Chat ID otomatik algƒ±landƒ±: {chat_id}")
            else:
                print(f"[WARNING] Chat ID otomatik algƒ±lanamadƒ±: {auto_result.get('error', 'Bilinmeyen hata')}")
                print("[INFO] Bot'a @tweet62_bot adresinden bir mesaj g√∂nderin ve tekrar deneyin.")
                return {"success": False, "reason": "missing_chat_id", "auto_detect_error": auto_result.get('error')}
        
        # Telegram mesajƒ±nƒ± hazƒ±rla
        telegram_message = f"ü§ñ **Yeni Tweet Payla≈üƒ±ldƒ±!**\n\n"
        
        if article_title:
            telegram_message += f"üì∞ **Makale:** {article_title}\n\n"
        
        telegram_message += f"üí¨ **Tweet:** {message}\n\n"
        
        if tweet_url:
            telegram_message += f"üîó **Link:** {tweet_url}\n\n"
        
        telegram_message += f"‚è∞ **Zaman:** {datetime.now().strftime('%d.%m.%Y %H:%M')}"
        
        # Telegram API'ye g√∂nder
        url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
        
        payload = {
            "chat_id": str(chat_id),
            "text": telegram_message,
            "parse_mode": "Markdown",
            "disable_web_page_preview": False
        }
        
        response = requests.post(url, json=payload, timeout=10)
        
        if response.status_code == 200:
            print(f"[SUCCESS] Telegram bildirimi g√∂nderildi: {chat_id}")
            return {"success": True, "message_id": response.json().get("result", {}).get("message_id")}
        else:
            print(f"[ERROR] Telegram API hatasƒ±: {response.status_code} - {response.text}")
            return {"success": False, "error": f"API Error: {response.status_code}"}
            
    except Exception as e:
        print(f"[ERROR] Telegram bildirim hatasƒ±: {e}")
        return {"success": False, "error": str(e)}

def test_telegram_connection():
    """Telegram bot baƒülantƒ±sƒ±nƒ± test et - Bot token env'den, Chat ID settings'den"""
    try:
        # Bot token'ƒ± environment variable'dan √ßek
        bot_token = os.getenv("TELEGRAM_BOT_TOKEN", "").strip()
        
        # Chat ID'yi settings'den √ßek
        settings = load_automation_settings()
        chat_id = settings.get("telegram_chat_id", "").strip()
        
        # Eƒüer bot token env'de yoksa settings'den dene (geriye d√∂n√ºk uyumluluk)
        if not bot_token:
            bot_token = settings.get("telegram_bot_token", "").strip()
        
        if not bot_token:
            return {
                "success": False, 
                "error": "Bot token eksik. .env dosyasƒ±nda TELEGRAM_BOT_TOKEN ayarlayƒ±n."
            }
            
        if not chat_id:
            print("[INFO] Chat ID eksik, otomatik algƒ±lama deneniyor...")
            # Otomatik Chat ID algƒ±lamayƒ± dene
            auto_result = auto_detect_and_save_chat_id()
            
            if auto_result["success"]:
                chat_id = auto_result["chat_id"]
                print(f"[SUCCESS] Chat ID otomatik algƒ±landƒ±: {chat_id}")
            else:
                return {
                    "success": False, 
                    "error": f"Chat ID eksik ve otomatik algƒ±lanamadƒ±: {auto_result.get('error', 'Bilinmeyen hata')}. Bot'a @tweet62_bot adresinden bir mesaj g√∂nderin."
                }
        
        # Bot bilgilerini al
        url = f"https://api.telegram.org/bot{bot_token}/getMe"
        response = requests.get(url, timeout=10)
        
        if response.status_code != 200:
            return {"success": False, "error": f"Bot token ge√ßersiz: {response.status_code}"}
        
        bot_info = response.json().get("result", {})
        
        # Test mesajƒ± g√∂nder
        test_message = f"üß™ **Test Mesajƒ±**\n\nBot ba≈üarƒ±yla baƒülandƒ±!\n\n‚è∞ {datetime.now().strftime('%d.%m.%Y %H:%M')}"
        
        send_url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
        payload = {
            "chat_id": str(chat_id),
            "text": test_message,
            "parse_mode": "Markdown"
        }
        
        send_response = requests.post(send_url, json=payload, timeout=10)
        
        if send_response.status_code == 200:
            return {
                "success": True, 
                "bot_name": bot_info.get("first_name", "Unknown"),
                "bot_username": bot_info.get("username", "Unknown")
            }
        else:
            error_detail = send_response.text
            return {"success": False, "error": f"Mesaj g√∂nderilemedi: {send_response.status_code} - {error_detail}"}
            
    except Exception as e:
        return {"success": False, "error": str(e)}

def check_telegram_configuration():
    """Telegram konfig√ºrasyonunu kontrol et - Bot token env'den, Chat ID settings'den"""
    try:
        # Bot token environment variable'dan
        bot_token = os.getenv("TELEGRAM_BOT_TOKEN", "").strip()
        
        # Chat ID settings'den
        settings = load_automation_settings()
        chat_id = settings.get("telegram_chat_id", "").strip()
        
        # Geriye d√∂n√ºk uyumluluk i√ßin settings'den bot token kontrol et
        settings_bot_token = settings.get("telegram_bot_token", "").strip()
        
        status = {
            "bot_token_env": bool(bot_token),
            "bot_token_settings": bool(settings_bot_token),
            "chat_id_set": bool(chat_id),
            "ready": bool((bot_token or settings_bot_token) and chat_id)
        }
        
        if status["ready"]:
            if status["bot_token_env"]:
                status["message"] = "‚úÖ Telegram yapƒ±landƒ±rmasƒ± tamamlanmƒ±≈ü (Bot token: ENV, Chat ID: Ayarlar)"
            else:
                status["message"] = "‚úÖ Telegram yapƒ±landƒ±rmasƒ± tamamlanmƒ±≈ü (Bot token: Ayarlar, Chat ID: Ayarlar)"
            status["status"] = "ready"
        elif status["bot_token_env"] or status["bot_token_settings"]:
            if not status["chat_id_set"]:
                status["message"] = "‚ö†Ô∏è Bot token var, Chat ID eksik - 'Chat ID Bul' butonu ile ayarlayƒ±n"
                status["status"] = "partial"
            else:
                status["message"] = "‚úÖ Telegram yapƒ±landƒ±rmasƒ± tamamlanmƒ±≈ü"
                status["status"] = "ready"
        else:
            status["message"] = "‚ùå Bot token eksik - .env dosyasƒ±nda TELEGRAM_BOT_TOKEN ayarlayƒ±n"
            status["status"] = "missing"
            
        return status
        
    except Exception as e:
        return {
            "bot_token_env": False,
            "bot_token_settings": False,
            "chat_id_set": False,
            "ready": False,
            "message": f"‚ùå Kontrol hatasƒ±: {e}",
            "status": "error"
        }

def get_telegram_chat_id(bot_token=None):
    """Bot'a mesaj g√∂nderen kullanƒ±cƒ±larƒ±n chat ID'lerini al - Environment variable'lardan token √ßeker"""
    try:
        # Eƒüer bot_token parametre olarak verilmemi≈üse env'den √ßek
        if not bot_token:
            bot_token = os.getenv("TELEGRAM_BOT_TOKEN", "").strip()
            
            # Eƒüer env'de yoksa settings'den dene
            if not bot_token:
                settings = load_automation_settings()
                bot_token = settings.get("telegram_bot_token", "").strip()
        
        if not bot_token:
            return {
                "success": False, 
                "error": "Bot token eksik. .env dosyasƒ±nda TELEGRAM_BOT_TOKEN ayarlayƒ±n."
            }
        
        url = f"https://api.telegram.org/bot{bot_token}/getUpdates"
        response = requests.get(url, timeout=10)
        
        if response.status_code != 200:
            return {"success": False, "error": "Bot token ge√ßersiz"}
        
        data = response.json()
        updates = data.get("result", [])
        
        chat_ids = []
        for update in updates[-10:]:  # Son 10 mesaj
            message = update.get("message", {})
            chat = message.get("chat", {})
            if chat.get("id"):
                chat_info = {
                    "chat_id": chat.get("id"),
                    "type": chat.get("type"),
                    "title": chat.get("title") or f"{chat.get('first_name', '')} {chat.get('last_name', '')}".strip()
                }
                if chat_info not in chat_ids:
                    chat_ids.append(chat_info)
        
        return {"success": True, "chat_ids": chat_ids}
        
    except Exception as e:
        return {"success": False, "error": str(e)}

def save_telegram_chat_id(chat_id):
    """Chat ID'yi otomatik olarak ayarlara kaydet"""
    try:
        settings = load_automation_settings()
        settings["telegram_chat_id"] = str(chat_id).strip()
        
        save_result = save_automation_settings(settings)
        
        if save_result["success"]:
            print(f"[SUCCESS] Chat ID otomatik kaydedildi: {chat_id}")
            return {"success": True, "message": f"‚úÖ Chat ID kaydedildi: {chat_id}"}
        else:
            print(f"[ERROR] Chat ID kaydetme hatasƒ±: {save_result['message']}")
            return {"success": False, "error": f"Kaydetme hatasƒ±: {save_result['message']}"}
            
    except Exception as e:
        print(f"[ERROR] Chat ID kaydetme hatasƒ±: {e}")
        return {"success": False, "error": str(e)}

def auto_detect_and_save_chat_id():
    """Otomatik chat ID tespit et ve kaydet"""
    try:
        # Mevcut chat ID'yi kontrol et
        settings = load_automation_settings()
        current_chat_id = settings.get("telegram_chat_id", "").strip()
        
        if current_chat_id:
            return {
                "success": True, 
                "message": f"Chat ID zaten ayarlanmƒ±≈ü: {current_chat_id}",
                "chat_id": current_chat_id,
                "auto_detected": False
            }
        
        # Chat ID'leri bul
        result = get_telegram_chat_id()
        
        if not result["success"]:
            return {
                "success": False,
                "error": result["error"],
                "auto_detected": False
            }
        
        chat_ids = result.get("chat_ids", [])
        
        if not chat_ids:
            return {
                "success": False,
                "error": "Chat ID bulunamadƒ±. Bot'a √∂nce bir mesaj g√∂nderin.",
                "auto_detected": False
            }
        
        # ƒ∞lk chat ID'yi otomatik se√ß (genellikle en son mesaj)
        selected_chat = chat_ids[0]
        chat_id = selected_chat["chat_id"]
        
        # Chat ID'yi kaydet
        save_result = save_telegram_chat_id(chat_id)
        
        if save_result["success"]:
            return {
                "success": True,
                "message": f"‚úÖ Chat ID otomatik tespit edildi ve kaydedildi: {chat_id}",
                "chat_id": chat_id,
                "chat_info": selected_chat,
                "auto_detected": True,
                "all_chats": chat_ids
            }
        else:
            return {
                "success": False,
                "error": save_result["error"],
                "auto_detected": False
            }
            
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "auto_detected": False
        }

def load_mcp_config():
    """MCP konfig√ºrasyonunu y√ºkle"""
    try:
        if os.path.exists(MCP_CONFIG_FILE):
            config = load_json(MCP_CONFIG_FILE)
        else:
            # Varsayƒ±lan konfig√ºrasyon
            config = {
                "mcp_enabled": False,
                "firecrawl_mcp": {
                    "enabled": False,
                    "server_url": "http://localhost:3000",
                    "api_key": "",
                    "timeout": 30,
                    "retry_count": 3,
                    "fallback_enabled": True
                },
                "content_extraction": {
                    "max_content_length": 2500,
                    "min_content_length": 100,
                    "wait_time": 3000,
                    "remove_base64_images": True,
                    "only_main_content": True
                },
                "ai_analysis": {
                    "enabled": True,
                    "max_tokens": 300,
                    "temperature": 0.7,
                    "model": "deepseek/deepseek-chat-v3-0324:free",
                    "fallback_enabled": True
                },
                "last_updated": datetime.now().isoformat()
            }
            save_json(MCP_CONFIG_FILE, config)
        
        return config
        
    except Exception as e:
        print(f"MCP konfig√ºrasyon y√ºkleme hatasƒ±: {e}")
        return {
            "mcp_enabled": False,
            "firecrawl_mcp": {"enabled": False, "fallback_enabled": True},
            "ai_analysis": {"enabled": True, "fallback_enabled": True}
        }

def save_mcp_config(config):
    """MCP konfig√ºrasyonunu kaydet"""
    try:
        config["last_updated"] = datetime.now().isoformat()
        save_json(MCP_CONFIG_FILE, config)
        return {"success": True, "message": "‚úÖ MCP konfig√ºrasyonu kaydedildi"}
    except Exception as e:
        return {"success": False, "message": f"‚ùå MCP konfig√ºrasyonu kaydedilemedi: {e}"}

def get_mcp_status():
    """MCP durumunu kontrol et"""
    try:
        config = load_mcp_config()
        
        status = {
            "mcp_enabled": config.get("mcp_enabled", False),
            "firecrawl_enabled": config.get("firecrawl_mcp", {}).get("enabled", False),
            "ai_analysis_enabled": config.get("ai_analysis", {}).get("enabled", True),
            "fallback_available": True,
            "ready": False
        }
        
        # MCP hazƒ±r mƒ± kontrol et
        if status["mcp_enabled"] and status["firecrawl_enabled"]:
            # Firecrawl MCP server baƒülantƒ±sƒ±nƒ± test et
            try:
                server_url = config.get("firecrawl_mcp", {}).get("server_url", "")
                if server_url:
                    # Basit ping testi (ger√ßek implementasyonda MCP server'a ping atƒ±lacak)
                    status["ready"] = True
                    status["message"] = "‚úÖ MCP Firecrawl aktif ve hazƒ±r"
                else:
                    status["message"] = "‚ö†Ô∏è MCP server URL'si eksik"
            except:
                status["message"] = "‚ùå MCP server baƒülantƒ±sƒ± ba≈üarƒ±sƒ±z"
        elif status["ai_analysis_enabled"]:
            status["message"] = "‚úÖ AI analizi aktif (MCP olmadan)"
        else:
            status["message"] = "‚ö†Ô∏è MCP ve AI analizi devre dƒ±≈üƒ±"
        
        return status
        
    except Exception as e:
        return {
            "mcp_enabled": False,
            "firecrawl_enabled": False,
            "ai_analysis_enabled": True,
            "fallback_available": True,
            "ready": False,
            "message": f"‚ùå MCP durum kontrol√º hatasƒ±: {e}"
        }

def test_mcp_connection():
    """MCP baƒülantƒ±sƒ±nƒ± test et"""
    try:
        config = load_mcp_config()
        
        if not config.get("mcp_enabled", False):
            return {
                "success": False,
                "message": "MCP devre dƒ±≈üƒ±",
                "details": "MCP konfig√ºrasyondan etkinle≈ütirilmeli"
            }
        
        firecrawl_config = config.get("firecrawl_mcp", {})
        
        if not firecrawl_config.get("enabled", False):
            return {
                "success": False,
                "message": "Firecrawl MCP devre dƒ±≈üƒ±",
                "details": "Firecrawl MCP konfig√ºrasyondan etkinle≈ütirilmeli"
            }
        
        server_url = firecrawl_config.get("server_url", "")
        
        if not server_url:
            return {
                "success": False,
                "message": "MCP server URL'si eksik",
                "details": "Konfig√ºrasyonda server_url ayarlanmalƒ±"
            }
        
        # Ger√ßek MCP implementasyonunda burada MCP server'a test √ßaƒürƒ±sƒ± yapƒ±lacak
        # ≈ûimdilik sim√ºle ediyoruz
        print(f"[TEST] MCP server test ediliyor: {server_url}")
        
        # Test URL'si ile basit scrape denemesi
        test_result = mcp_firecrawl_scrape({
            "url": "https://example.com",
            "formats": ["markdown"],
            "onlyMainContent": True
        })
        
        if test_result.get("success", False):
            return {
                "success": True,
                "message": "‚úÖ MCP Firecrawl baƒülantƒ±sƒ± ba≈üarƒ±lƒ±",
                "details": f"Server: {server_url}"
            }
        else:
            return {
                "success": False,
                "message": "‚ùå MCP Firecrawl test ba≈üarƒ±sƒ±z",
                "details": test_result.get("reason", "Bilinmeyen hata")
            }
        
    except Exception as e:
        return {
            "success": False,
            "message": f"‚ùå MCP test hatasƒ±: {e}",
            "details": "Baƒülantƒ± testi sƒ±rasƒ±nda hata olu≈ütu"
        }

def gemini_ocr_image(image_path):
    import google.generativeai as genai
    import os
    from PIL import Image

    api_key = os.environ.get('GOOGLE_API_KEY')
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel('gemini-2.0-flash')

    # Resmi PIL ile a√ß
    image = Image.open(image_path)
    prompt = "Bu g√∂rseldeki t√ºm metni OCR ile √ßƒ±kar ve sadece metni d√∂nd√ºr."
    response = model.generate_content([prompt, image])
    return response.text.strip()

def setup_twitter_v2_client():
    """Tweepy v2 API Client ile kimlik doƒürulama (sadece metinli tweet i√ßin)"""
    import tweepy
    import os
    # v2 API Client
    client = tweepy.Client(
        bearer_token=os.environ.get('TWITTER_BEARER_TOKEN'),
        consumer_key=os.environ.get('TWITTER_API_KEY'),
        consumer_secret=os.environ.get('TWITTER_API_SECRET'),
        access_token=os.environ.get('TWITTER_ACCESS_TOKEN'),
        access_token_secret=os.environ.get('TWITTER_ACCESS_TOKEN_SECRET')
    )
    return client



def post_text_tweet_v2(tweet_text):
    """Sadece metinli tweet atmak i√ßin Tweepy v2 API kullanƒ±mƒ± - Rate limit kontrol√º ile"""
    try:
        import tweepy
        
        # Rate limit kontrol√º yap
        rate_check = check_rate_limit("tweets")
        if not rate_check.get("allowed", True):
            wait_minutes = int(rate_check.get("wait_time", 0) / 60) + 1
            error_msg = f"Twitter API rate limit a≈üƒ±ldƒ±. {wait_minutes} dakika sonra tekrar deneyin. ({rate_check.get('requests_made', 0)}/{rate_check.get('limit', 300)} istek kullanƒ±ldƒ±)"
            safe_log(error_msg, "WARNING")
            return {"success": False, "error": error_msg, "rate_limited": True, "wait_minutes": wait_minutes}
        
        client = setup_twitter_v2_client()
        TWITTER_LIMIT = 280
        if len(tweet_text) > TWITTER_LIMIT:
            tweet_text = tweet_text[:TWITTER_LIMIT-3] + "..."
        safe_log(f"Tweet uzunluƒüu: {len(tweet_text)}", "DEBUG")
        
        response = client.create_tweet(text=tweet_text)
        
        # Rate limit kullanƒ±mƒ±nƒ± g√ºncelle
        update_rate_limit_usage("tweets")
        
        if hasattr(response, 'data') and response.data and 'id' in response.data:
            tweet_id = response.data['id']
            tweet_url = f"https://twitter.com/user/status/{tweet_id}"
            safe_log(f"Tweet ba≈üarƒ±yla g√∂nderildi: {tweet_url}", "INFO")
            return {"success": True, "tweet_id": tweet_id, "url": tweet_url}
        else:
            safe_log(f"Tweet g√∂nderilemedi: {response}", "ERROR")
            return {"success": False, "error": "Tweet g√∂nderilemedi"}
            
    except tweepy.TooManyRequests as rate_limit_error:
        # API'den gelen rate limit bilgilerini g√ºncelle
        try:
            # Rate limit durumunu g√ºncelle
            status = load_rate_limit_status()
            current_time = time.time()
            if "tweets" not in status:
                status["tweets"] = {}
            status["tweets"]["requests"] = TWITTER_RATE_LIMITS["tweets"]["limit"]  # Limit dolu
            status["tweets"]["reset_time"] = current_time + 3600  # 1 saat sonra reset (Free plan i√ßin g√ºvenli)
            save_rate_limit_status(status)
        except:
            pass
        
        wait_minutes = 15  # Twitter API rate limit genelde 15 dakika
        error_msg = f"Twitter API rate limit a≈üƒ±ldƒ±: {rate_limit_error}\n{wait_minutes} dakika sonra tekrar deneyin."
        safe_log(error_msg, "WARNING")
        return {"success": False, "error": error_msg, "rate_limited": True, "wait_minutes": wait_minutes}
        
    except tweepy.Unauthorized as auth_error:
        safe_log(f"Twitter API yetkilendirme hatasƒ±: {auth_error}", "ERROR")
        return {"success": False, "error": f"Twitter API yetkilendirme hatasƒ±: {auth_error}"}
        
    except tweepy.Forbidden as forbidden_error:
        safe_log(f"Twitter API yasak i≈ülem: {forbidden_error}", "ERROR")
        return {"success": False, "error": f"Twitter API yasak i≈ülem: {forbidden_error}"}
        
    except Exception as e:
        safe_log(f"Tweet payla≈üƒ±m genel hatasƒ±: {e}", "ERROR")
        return {"success": False, "error": str(e)}

def fetch_url_content_with_mcp(url):
    """MCP ile URL i√ßeriƒüi √ßekme - Tweet olu≈üturma i√ßin"""
    try:
        print(f"üîç MCP ile URL i√ßeriƒüi √ßekiliyor: {url}")
        
        # Firecrawl MCP scrape fonksiyonunu kullan
        scrape_result = mcp_firecrawl_scrape({
            "url": url,
            "formats": ["markdown"],
            "onlyMainContent": True,
            "waitFor": 3000,
            "removeBase64Images": True
        })
        
        if not scrape_result.get("success", False):
            print(f"‚ö†Ô∏è MCP ba≈üarƒ±sƒ±z, fallback deneniyor...")
            return fetch_url_content_fallback(url)
        
        # Markdown i√ßeriƒüini al
        markdown_content = scrape_result.get("markdown", "")
        
        if not markdown_content or len(markdown_content) < 100:
            print(f"‚ö†Ô∏è MCP'den yetersiz i√ßerik, fallback deneniyor...")
            return fetch_url_content_fallback(url)
        
        # Ba≈ülƒ±ƒüƒ± √ßƒ±kar (genellikle ilk # ile ba≈ülar)
        lines = markdown_content.split('\n')
        title = ""
        content_lines = []
        
        for line in lines:
            line = line.strip()
            if line.startswith('# ') and not title:
                title = line[2:].strip()
            elif line and not line.startswith('#') and len(line) > 20:
                content_lines.append(line)
        
        # ƒ∞√ßeriƒüi birle≈ütir ve temizle
        content = '\n'.join(content_lines)
        
        # Gereksiz karakterleri temizle
        content = content.replace('*', '').replace('**', '').replace('_', '')
        content = ' '.join(content.split())  # √áoklu bo≈üluklarƒ± tek bo≈üluƒüa √ßevir
        
        # ƒ∞√ßeriƒüi sƒ±nƒ±rla
        content = content[:2000]
        
        print(f"‚úÖ MCP ile URL i√ßeriƒüi √ßekildi: {len(content)} karakter")
        
        return {
            "title": title or "Ba≈ülƒ±k bulunamadƒ±",
            "content": content,
            "url": url,
            "source": "mcp"
        }
        
    except Exception as e:
        print(f"‚ùå MCP URL √ßekme hatasƒ± ({url}): {e}")
        print("üîÑ Fallback y√∂nteme ge√ßiliyor...")
        return fetch_url_content_fallback(url)

def fetch_url_content_fallback(url):
    """Fallback URL i√ßeriƒüi √ßekme - BeautifulSoup ile"""
    try:
        print(f"üîÑ Fallback ile URL i√ßeriƒüi √ßekiliyor: {url}")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, "html.parser")
        
        # Ba≈ülƒ±ƒüƒ± bul
        title = ""
        title_selectors = ["h1", "h1.entry-title", "h1.post-title", ".article-title h1", "title"]
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                title = title_elem.text.strip()
                break
        
        # ƒ∞√ßeriƒüi bul - √ßoklu selector deneme
        content_selectors = [
            "article p",
            "div.article-content p",
            "div.entry-content p", 
            "div.post-content p",
            "div.content p",
            ".article-body p",
            "main p",
            ".post p"
        ]
        
        content = ""
        for selector in content_selectors:
            paragraphs = soup.select(selector)
            if paragraphs:
                content = "\n".join([p.text.strip() for p in paragraphs if p.text.strip() and len(p.text.strip()) > 20])
                if len(content) > 200:  # Yeterli i√ßerik bulundu
                    break
        
        # Eƒüer hala i√ßerik bulunamadƒ±ysa, t√ºm p etiketlerini dene
        if not content:
            all_paragraphs = soup.find_all('p')
            content = "\n".join([p.text.strip() for p in all_paragraphs if len(p.text.strip()) > 30])
        
        # ƒ∞√ßeriƒüi temizle ve sƒ±nƒ±rla
        content = ' '.join(content.split())  # √áoklu bo≈üluklarƒ± temizle
        content = content[:2000]  # ƒ∞√ßeriƒüi sƒ±nƒ±rla
        
        print(f"‚úÖ Fallback ile URL i√ßeriƒüi √ßekildi: {len(content)} karakter")
        
        return {
            "title": title or "Ba≈ülƒ±k bulunamadƒ±",
            "content": content,
            "url": url,
            "source": "fallback"
        }
        
    except Exception as e:
        print(f"‚ùå Fallback URL √ßekme hatasƒ± ({url}): {e}")
        return {
            "title": "ƒ∞√ßerik √ßekilemedi",
            "content": f"URL: {url} - ƒ∞√ßerik √ßekilemedi: {str(e)}",
            "url": url,
            "source": "error"
        }

# =============================================================================
# GMAIL E-POSTA Bƒ∞LDƒ∞Rƒ∞M Sƒ∞STEMƒ∞
# =============================================================================

def send_gmail_notification(message, tweet_url="", article_title=""):
    """Gmail SMTP ile e-posta bildirimi g√∂nder"""
    try:
        import smtplib
        from email.mime.text import MIMEText
        from email.mime.multipart import MIMEMultipart
        from datetime import datetime
        import os
        
        # Gmail SMTP ayarlarƒ±
        gmail_email = os.getenv("GMAIL_EMAIL", "").strip()
        gmail_password = os.getenv("GMAIL_APP_PASSWORD", "").strip()
        
        # Ayarlarƒ± kontrol et
        settings = load_automation_settings()
        email_notifications = settings.get("email_notifications", True)
        
        if not email_notifications:
            print("[DEBUG] E-posta bildirimleri kapalƒ±")
            return {"success": False, "reason": "disabled"}
        
        if not gmail_email:
            print("[WARNING] Gmail e-posta adresi eksik. .env dosyasƒ±nda GMAIL_EMAIL ayarlayƒ±n.")
            return {"success": False, "reason": "missing_email"}
            
        if not gmail_password:
            safe_log("Gmail uygulama ≈üifresi eksik. .env dosyasƒ±nda GMAIL_APP_PASSWORD ayarlayƒ±n.", "WARNING")
            return {"success": False, "reason": "missing_password"}
        
        # E-posta i√ßeriƒüini hazƒ±rla
        subject = "ü§ñ AI Tweet Bot - Yeni Tweet Payla≈üƒ±ldƒ±!"
        
        # HTML e-posta i√ßeriƒüi
        html_content = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <meta charset="UTF-8">
            <style>
                body {{ font-family: Arial, sans-serif; line-height: 1.6; color: #333; }}
                .container {{ max-width: 600px; margin: 0 auto; padding: 20px; }}
                .header {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 10px 10px 0 0; text-align: center; }}
                .content {{ background: #f9f9f9; padding: 20px; border-radius: 0 0 10px 10px; }}
                .tweet-box {{ background: white; border-left: 4px solid #1da1f2; padding: 15px; margin: 15px 0; border-radius: 5px; }}
                .article-box {{ background: white; border-left: 4px solid #28a745; padding: 15px; margin: 15px 0; border-radius: 5px; }}
                .footer {{ text-align: center; margin-top: 20px; color: #666; font-size: 12px; }}
                .btn {{ display: inline-block; background: #1da1f2; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px; margin: 10px 5px; }}
                .stats {{ background: #e3f2fd; padding: 10px; border-radius: 5px; margin: 10px 0; }}
            </style>
        </head>
        <body>
            <div class="container">
                <div class="header">
                    <h1>ü§ñ AI Tweet Bot</h1>
                    <p>Yeni Tweet Ba≈üarƒ±yla Payla≈üƒ±ldƒ±!</p>
                </div>
                
                <div class="content">
                    <div class="tweet-box">
                        <h3>üí¨ Payla≈üƒ±lan Tweet:</h3>
                        <p><strong>{message}</strong></p>
                        {f'<a href="{tweet_url}" class="btn">üîó Tweet&apos;i G√∂r√ºnt√ºle</a>' if tweet_url else ''}
                    </div>
                    
                    {f'''
                    <div class="article-box">
                        <h3>üì∞ Kaynak Makale:</h3>
                        <p><strong>{article_title}</strong></p>
                    </div>
                    ''' if article_title else ''}
                    
                    <div class="stats">
                        <h3>üìä Sistem Bilgileri:</h3>
                        <p><strong>‚è∞ Zaman:</strong> {datetime.now().strftime('%d.%m.%Y %H:%M:%S')}</p>
                        <p><strong>ü§ñ Bot:</strong> AI Tweet Bot v2.0</p>
                        <p><strong>üîÑ Durum:</strong> Otomatik payla≈üƒ±m aktif</p>
                    </div>
                </div>
                
                <div class="footer">
                    <p>Bu e-posta AI Tweet Bot tarafƒ±ndan otomatik olarak g√∂nderilmi≈ütir.</p>
                    <p>Bildirimleri kapatmak i√ßin uygulama ayarlarƒ±ndan e-posta bildirimlerini devre dƒ±≈üƒ± bƒ±rakabilirsiniz.</p>
                </div>
            </div>
        </body>
        </html>
        """
        
        # Metin versiyonu (fallback)
        text_content = f"""
ü§ñ AI Tweet Bot - Yeni Tweet Payla≈üƒ±ldƒ±!

üí¨ Tweet: {message}

{f'üì∞ Makale: {article_title}' if article_title else ''}

{f'üîó Tweet Linki: {tweet_url}' if tweet_url else ''}

‚è∞ Zaman: {datetime.now().strftime('%d.%m.%Y %H:%M:%S')}

Bu e-posta AI Tweet Bot tarafƒ±ndan otomatik olarak g√∂nderilmi≈ütir.
        """
        
        # E-posta mesajƒ±nƒ± olu≈ütur
        msg = MIMEMultipart('alternative')
        msg['Subject'] = subject
        msg['From'] = gmail_email
        msg['To'] = gmail_email  # Kendine g√∂nder
        
        # Metin ve HTML par√ßalarƒ±nƒ± ekle
        text_part = MIMEText(text_content, 'plain', 'utf-8')
        html_part = MIMEText(html_content, 'html', 'utf-8')
        
        msg.attach(text_part)
        msg.attach(html_part)
        
        # Gmail SMTP ile g√∂nder
        with smtplib.SMTP('smtp.gmail.com', 587) as server:
            server.starttls()
            server.login(gmail_email, gmail_password)
            server.send_message(msg)
        
        print(f"[SUCCESS] Gmail bildirimi g√∂nderildi: {gmail_email}")
        return {"success": True, "email": gmail_email}
        
    except Exception as e:
        print(f"[ERROR] Gmail bildirim hatasƒ±: {e}")
        return {"success": False, "error": str(e)}

def test_gmail_connection():
    """Gmail SMTP baƒülantƒ±sƒ±nƒ± test et"""
    try:
        import smtplib
        from email.mime.text import MIMEText
        from datetime import datetime
        import os
        
        gmail_email = os.getenv("GMAIL_EMAIL", "").strip()
        gmail_password = os.getenv("GMAIL_APP_PASSWORD", "").strip()
        
        if not gmail_email:
            return {
                "success": False, 
                "error": "Gmail e-posta adresi eksik. .env dosyasƒ±nda GMAIL_EMAIL ayarlayƒ±n."
            }
            
        if not gmail_password:
            return {
                "success": False, 
                "error": "Gmail uygulama ≈üifresi eksik. .env dosyasƒ±nda GMAIL_APP_PASSWORD ayarlayƒ±n."
            }
        
        # Test e-postasƒ± g√∂nder
        subject = "üß™ AI Tweet Bot - Test E-postasƒ±"
        body = f"""
üß™ Test E-postasƒ±

Gmail SMTP baƒülantƒ±sƒ± ba≈üarƒ±yla test edildi!

‚è∞ Test Zamanƒ±: {datetime.now().strftime('%d.%m.%Y %H:%M:%S')}
üìß E-posta: {gmail_email}
ü§ñ AI Tweet Bot v2.0

Bu bir test e-postasƒ±dƒ±r.
        """
        
        msg = MIMEText(body, 'plain', 'utf-8')
        msg['Subject'] = subject
        msg['From'] = gmail_email
        msg['To'] = gmail_email
        
        with smtplib.SMTP('smtp.gmail.com', 587) as server:
            server.starttls()
            server.login(gmail_email, gmail_password)
            server.send_message(msg)
        
        return {
            "success": True, 
            "message": f"‚úÖ Test e-postasƒ± ba≈üarƒ±yla g√∂nderildi: {gmail_email}"
        }
        
    except Exception as e:
        return {"success": False, "error": str(e)}

def check_gmail_configuration():
    """Gmail konfig√ºrasyonunu kontrol et"""
    try:
        import os
        
        gmail_email = os.getenv("GMAIL_EMAIL", "").strip()
        gmail_password = os.getenv("GMAIL_APP_PASSWORD", "").strip()
        
        # Ayarlarƒ± kontrol et
        settings = load_automation_settings()
        email_notifications = settings.get("email_notifications", True)
        
        status = {
            "email_set": bool(gmail_email and "@" in gmail_email),
            "password_set": bool(gmail_password),
            "notifications_enabled": email_notifications,
            "ready": False
        }
        
        if status["email_set"] and status["password_set"] and status["notifications_enabled"]:
            status["ready"] = True
            status["message"] = "‚úÖ Gmail yapƒ±landƒ±rmasƒ± tamamlanmƒ±≈ü ve aktif"
            status["status"] = "ready"
        elif status["email_set"] and status["password_set"]:
            status["message"] = "‚ö†Ô∏è Gmail yapƒ±landƒ±rƒ±lmƒ±≈ü ama bildirimler kapalƒ±"
            status["status"] = "disabled"
        elif status["email_set"]:
            status["message"] = "‚ö†Ô∏è E-posta var, uygulama ≈üifresi eksik"
            status["status"] = "partial"
        else:
            status["message"] = "‚ùå Gmail yapƒ±landƒ±rmasƒ± eksik"
            status["status"] = "missing"
            
        return status
        
    except Exception as e:
        return {
            "email_set": False,
            "password_set": False,
            "notifications_enabled": False,
            "ready": False,
            "message": f"‚ùå Kontrol hatasƒ±: {e}",
            "status": "error"
        }

# ==========================================
# √ñZEL HABER KAYNAKLARI Y√ñNETƒ∞Mƒ∞
# ==========================================

NEWS_SOURCES_FILE = "news_sources.json"

def load_news_sources():
    """Haber kaynaklarƒ±nƒ± y√ºkle"""
    try:
        if os.path.exists(NEWS_SOURCES_FILE):
            return load_json(NEWS_SOURCES_FILE)
        else:
            # Varsayƒ±lan konfig√ºrasyon
            default_config = {
                "sources": [
                    {
                        "id": "techcrunch_ai",
                        "name": "TechCrunch AI",
                        "url": "https://techcrunch.com/category/artificial-intelligence/",
                        "description": "TechCrunch Yapay Zeka haberleri",
                        "enabled": True,
                        "selector_type": "rss_like",
                        "article_selectors": {
                            "container": "article.post-block",
                            "title": "h2.post-block__title a",
                            "link": "h2.post-block__title a",
                            "date": "time.river-byline__time",
                            "excerpt": ".post-block__content"
                        },
                        "added_date": datetime.now().isoformat(),
                        "last_checked": None,
                        "article_count": 0,
                        "success_rate": 100
                    }
                ],
                "settings": {
                    "max_sources": 10,
                    "check_all_sources": True,
                    "source_timeout": 30,
                    "articles_per_source": 5,
                    "last_updated": datetime.now().isoformat()
                }
            }
            save_json(NEWS_SOURCES_FILE, default_config)
            return default_config
    except Exception as e:
        print(f"Haber kaynaklarƒ± y√ºkleme hatasƒ±: {e}")
        return {"sources": [], "settings": {}}

def save_news_sources(config):
    """Haber kaynaklarƒ±nƒ± kaydet"""
    try:
        config["settings"]["last_updated"] = datetime.now().isoformat()
        save_json(NEWS_SOURCES_FILE, config)
        return {"success": True, "message": "‚úÖ Haber kaynaklarƒ± kaydedildi"}
    except Exception as e:
        return {"success": False, "message": f"‚ùå Kaydetme hatasƒ±: {e}"}

def test_selectors_for_url(url):
    """URL i√ßin selector'larƒ± test et ve otomatik tespit et - Geli≈ümi≈ü sistem"""
    try:
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        
        print(f"üß™ URL test ediliyor: {url}")
        
        # Geli≈ümi≈ü scraper ile sayfayƒ± √ßek
        scrape_result = advanced_web_scraper(url, wait_time=3, use_js=True)
        
        if not scrape_result.get("success"):
            # Fallback: Basit HTTP request
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            scrape_method = "simple_request"
        else:
            soup = BeautifulSoup(scrape_result.get("html", ""), 'html.parser')
            scrape_method = scrape_result.get("method", "advanced")
        
        print(f"‚úÖ Sayfa √ßekildi ({scrape_method})")
        
        # Otomatik selector tespiti
        print("üîç Otomatik selector tespiti ba≈ülatƒ±lƒ±yor...")
        selectors, container_count = auto_detect_selectors(soup, url)
        
        # Tespit edilen selector'larƒ± doƒürula
        print("‚úÖ Selector'lar doƒürulanƒ±yor...")
        is_valid, validation_msg = validate_selectors(soup, selectors)
        
        if is_valid:
            # √ñrnek makaleleri √ßek
            print("üì∞ √ñrnek makaleler √ßekiliyor...")
            containers = soup.select(selectors["container"])
            sample_articles = []
            
            for i, container in enumerate(containers[:5]):  # ƒ∞lk 5 makale
                try:
                    print(f"üîç Makale {i+1} i≈üleniyor...")
                    
                    # Ba≈ülƒ±k ve link √ßek
                    title_elem = container.select_one(selectors["title"])
                    link_elem = container.select_one(selectors["link"])
                    
                    if title_elem and link_elem:
                        title = title_elem.get_text(strip=True)
                        link = link_elem.get('href', '')
                        
                        # Relative URL'leri absolute yap
                        if link.startswith('/'):
                            from urllib.parse import urljoin
                            link = urljoin(url, link)
                        
                        # Tarih √ßek (opsiyonel)
                        date_text = ""
                        if selectors.get("date"):
                            date_elem = container.select_one(selectors["date"])
                            if date_elem:
                                date_text = date_elem.get_text(strip=True)
                        
                        # √ñzet √ßek (opsiyonel)
                        excerpt_text = ""
                        if selectors.get("excerpt"):
                            excerpt_elem = container.select_one(selectors["excerpt"])
                            if excerpt_elem:
                                excerpt_text = excerpt_elem.get_text(strip=True)[:200]
                        
                        if title and link and len(title) > 10:
                            sample_articles.append({
                                "title": title,
                                "url": link,
                                "date": date_text,
                                "excerpt": excerpt_text
                            })
                            print(f"‚úÖ Makale {i+1}: {title[:50]}...")
                        
                except Exception as article_error:
                    print(f"‚ö†Ô∏è Makale {i+1} hatasƒ±: {article_error}")
                    continue
            
            # Site t√ºr√º tespiti
            site_info = detect_site_type(url, soup)
            
            # Ba≈üarƒ± mesajƒ±
            success_msg = f"‚úÖ {container_count} konteyner bulundu, {len(sample_articles)} √∂rnek makale √ßekildi"
            
            return {
                "success": True,
                "message": success_msg,
                "selectors": selectors,
                "container_count": container_count,
                "sample_articles": sample_articles,
                "validation_message": validation_msg,
                "scrape_method": scrape_method,
                "site_info": site_info,
                "test_details": {
                    "total_containers": container_count,
                    "successful_articles": len(sample_articles),
                    "selector_quality": "high" if len(sample_articles) >= 3 else "medium" if len(sample_articles) >= 1 else "low"
                }
            }
        else:
            return {
                "success": False,
                "message": f"‚ùå Selector tespiti ba≈üarƒ±sƒ±z: {validation_msg}",
                "selectors": selectors,
                "container_count": container_count,
                "scrape_method": scrape_method,
                "test_details": {
                    "error": validation_msg,
                    "total_containers": container_count
                }
            }
            
    except Exception as e:
        print(f"‚ùå URL test hatasƒ±: {e}")
        return {
            "success": False,
            "message": f"‚ùå URL test hatasƒ±: {str(e)}",
            "test_details": {
                "error": str(e)
            }
        }

def detect_site_type(url, soup):
    """Site t√ºr√ºn√º ve CMS'ini tespit et"""
    try:
        site_info = {
            "cms": "unknown",
            "type": "unknown",
            "features": []
        }
        
        # WordPress tespiti
        wp_indicators = [
            'wp-content', 'wp-includes', 'wp-json',
            'class*="wp-"', 'id*="wp-"'
        ]
        
        for indicator in wp_indicators:
            if indicator in str(soup).lower():
                site_info["cms"] = "wordpress"
                site_info["features"].append("WordPress")
                break
        
        # Site t√ºr√º tespiti
        if any(domain in url.lower() for domain in ['techcrunch', 'theverge', 'wired', 'arstechnica']):
            site_info["type"] = "tech_news"
            site_info["features"].append("Tech News Site")
        elif any(keyword in str(soup).lower() for keyword in ['news', 'article', 'story', 'journalism']):
            site_info["type"] = "news"
            site_info["features"].append("News Site")
        elif any(keyword in str(soup).lower() for keyword in ['blog', 'post', 'author']):
            site_info["type"] = "blog"
            site_info["features"].append("Blog")
        
        # JavaScript framework tespiti
        if 'react' in str(soup).lower():
            site_info["features"].append("React")
        if 'vue' in str(soup).lower():
            site_info["features"].append("Vue.js")
        if 'angular' in str(soup).lower():
            site_info["features"].append("Angular")
        
        return site_info
        
    except Exception as e:
        return {"cms": "unknown", "type": "unknown", "features": [], "error": str(e)}

def test_manual_selectors_for_url(url, selectors):
    """Manuel selector'larƒ± test et"""
    try:
        safe_log(f"Manuel selector test ediliyor: {url}", "INFO")
        
        # Selector'larƒ± temizle
        article_container = selectors.get('article_container', '').strip()
        title_selector = selectors.get('title_selector', '').strip()
        link_selector = selectors.get('link_selector', '').strip()
        date_selector = selectors.get('date_selector', '').strip()
        summary_selector = selectors.get('summary_selector', '').strip()
        base_url = selectors.get('base_url', '').strip()
        
        if not base_url:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            base_url = f"{parsed.scheme}://{parsed.netloc}"
        
        # Sayfayƒ± √ßek
        try:
            response = requests.get(url, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }, timeout=15)
            
            if response.status_code != 200:
                return {
                    'success': False,
                    'error': f'Sayfa y√ºklenemedi (HTTP {response.status_code})',
                    'details': {
                        'status_code': response.status_code,
                        'url': url
                    }
                }
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
        except Exception as e:
            return {
                'success': False,
                'error': f'Sayfa √ßekme hatasƒ±: {str(e)}',
                'details': {
                    'error_type': type(e).__name__,
                    'url': url
                }
            }
        
        # Makale konteynerlerini bul
        try:
            containers = soup.select(article_container)
            
            if not containers:
                return {
                    'success': False,
                    'error': f'Makale konteyneri bulunamadƒ±',
                    'details': {
                        'article_container': article_container,
                        'page_title': soup.title.get_text() if soup.title else 'Ba≈ülƒ±k yok',
                        'total_elements': len(soup.find_all())
                    }
                }
            
            safe_log(f"Bulunan konteyner sayƒ±sƒ±: {len(containers)}", "INFO")
            
        except Exception as e:
            return {
                'success': False,
                'error': f'Konteyner selector hatasƒ±: {str(e)}',
                'details': {
                    'article_container': article_container,
                    'error_type': type(e).__name__
                }
            }
        
        # Her konteynerden makale bilgilerini √ßek
        articles = []
        test_details = {
            'container_count': len(containers),
            'successful_extractions': 0,
            'failed_extractions': 0
        }
        
        for i, container in enumerate(containers[:5]):  # ƒ∞lk 5 konteyner
            try:
                article_data = {}
                extraction_success = True
                
                # Ba≈ülƒ±k √ßek
                try:
                    title_elem = container.select_one(title_selector)
                    if title_elem:
                        article_data['title'] = title_elem.get_text(strip=True)
                    else:
                        article_data['title'] = None
                        extraction_success = False
                except Exception as e:
                    article_data['title'] = None
                    extraction_success = False
                    safe_log(f"Ba≈ülƒ±k √ßekme hatasƒ± (konteyner {i+1}): {e}", "WARNING")
                
                # Link √ßek
                try:
                    link_elem = container.select_one(link_selector)
                    if link_elem:
                        href = link_elem.get('href', '')
                        if href:
                            # Relative link kontrol√º
                            if href.startswith('/'):
                                from urllib.parse import urljoin
                                href = urljoin(base_url, href)
                            elif not href.startswith(('http://', 'https://')):
                                href = base_url + '/' + href.lstrip('/')
                            article_data['url'] = href
                        else:
                            article_data['url'] = None
                            extraction_success = False
                    else:
                        article_data['url'] = None
                        extraction_success = False
                except Exception as e:
                    article_data['url'] = None
                    extraction_success = False
                    safe_log(f"Link √ßekme hatasƒ± (konteyner {i+1}): {e}", "WARNING")
                
                # Tarih √ßek (opsiyonel)
                if date_selector:
                    try:
                        date_elem = container.select_one(date_selector)
                        if date_elem:
                            article_data['date'] = date_elem.get_text(strip=True)
                        else:
                            article_data['date'] = None
                    except Exception as e:
                        article_data['date'] = None
                        safe_log(f"Tarih √ßekme hatasƒ± (konteyner {i+1}): {e}", "WARNING")
                
                # √ñzet √ßek (opsiyonel)
                if summary_selector:
                    try:
                        summary_elem = container.select_one(summary_selector)
                        if summary_elem:
                            summary_text = summary_elem.get_text(strip=True)
                            article_data['summary'] = summary_text[:200] + '...' if len(summary_text) > 200 else summary_text
                        else:
                            article_data['summary'] = None
                    except Exception as e:
                        article_data['summary'] = None
                        safe_log(f"√ñzet √ßekme hatasƒ± (konteyner {i+1}): {e}", "WARNING")
                
                # Ba≈üarƒ± kontrol√º
                if extraction_success and article_data.get('title') and article_data.get('url'):
                    articles.append(article_data)
                    test_details['successful_extractions'] += 1
                else:
                    test_details['failed_extractions'] += 1
                    safe_log(f"Konteyner {i+1} ba≈üarƒ±sƒ±z: title={bool(article_data.get('title'))}, url={bool(article_data.get('url'))}", "WARNING")
                
            except Exception as e:
                test_details['failed_extractions'] += 1
                safe_log(f"Konteyner {i+1} i≈üleme hatasƒ±: {e}", "ERROR")
                continue
        
        # Sonu√ß deƒüerlendirmesi
        if not articles:
            return {
                'success': False,
                'error': 'Hi√ßbir makale √ßekilemedi',
                'details': {
                    **test_details,
                    'selectors_used': {
                        'article_container': article_container,
                        'title_selector': title_selector,
                        'link_selector': link_selector,
                        'date_selector': date_selector,
                        'summary_selector': summary_selector
                    }
                }
            }
        
        success_rate = (test_details['successful_extractions'] / test_details['container_count']) * 100
        
        return {
            'success': True,
            'message': f'Manuel selector test ba≈üarƒ±lƒ±',
            'article_count': len(articles),
            'articles': articles,
            'test_details': test_details,
            'success_rate': round(success_rate, 1),
            'selectors_used': {
                'article_container': article_container,
                'title_selector': title_selector,
                'link_selector': link_selector,
                'date_selector': date_selector,
                'summary_selector': summary_selector,
                'base_url': base_url
            }
        }
        
    except Exception as e:
        safe_log(f"Manuel selector test hatasƒ±: {e}", "ERROR")
        return {
            'success': False,
            'error': f'Test hatasƒ±: {str(e)}',
            'details': {
                'error_type': type(e).__name__,
                'url': url
            }
        }

def add_news_source_with_validation(name, url, description="", auto_detect=True, manual_selectors=None):
    """Doƒürulama ile yeni haber kaynaƒüƒ± ekle"""
    try:
        # Konsol log i√ßin import (terminal kaldƒ±rƒ±ldƒ±)
        try:
            from app import terminal_log
        except ImportError:
            # Eƒüer app.py'den import edilemezse normal print kullan
            def terminal_log(msg, level='info'):
                import time
                timestamp = time.strftime('%H:%M:%S')
                print(f"[{timestamp}] [{level.upper()}] {msg}")
        
        terminal_log(f"üîç Kaynak ekleme ba≈ülatƒ±ldƒ± - Name: {name}, URL: {url}, Auto: {auto_detect}", "debug")
        
        config = load_news_sources()
        
        # URL'yi temizle ve doƒürula
        url = url.strip()
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        
        # Aynƒ± URL var mƒ± kontrol et
        for source in config["sources"]:
            if source["url"] == url:
                return {
                    "success": False,
                    "message": f"‚ùå Bu URL zaten mevcut: {source['name']}"
                }
        
        # Maksimum kaynak sayƒ±sƒ±nƒ± kontrol et
        max_sources = config["settings"].get("max_sources", 10)
        if len(config["sources"]) >= max_sources:
            return {"success": False, "message": f"‚ùå Maksimum {max_sources} kaynak eklenebilir"}
        
        # URL'yi test et
        if auto_detect:
            terminal_log(f"üîç {name} kaynaƒüƒ± otomatik tespit ile test ediliyor...", "info")
            test_result = test_selectors_for_url(url)
            
            if not test_result['success']:
                terminal_log(f"‚ùå {name} otomatik tespit ba≈üarƒ±sƒ±z: {test_result['message']}", "error")
                return {
                    "success": False,
                    "message": f"‚ùå Otomatik tespit ba≈üarƒ±sƒ±z: {test_result['message']}",
                    "test_details": test_result
                }
            
            selectors = test_result['selectors']
            selector_type = "auto_detected"
            terminal_log(f"‚úÖ {name}: {test_result['container_count']} konteyner bulundu", "success")
            
        else:
            # Manuel selector'larƒ± test et
            if not manual_selectors:
                terminal_log(f"‚ùå {name} manuel mod i√ßin selector'lar eksik", "error")
                return {
                    "success": False,
                    "message": "‚ùå Manuel mod i√ßin selector'lar gerekli"
                }
            
            terminal_log(f"üîç {name} kaynaƒüƒ± manuel selector'larla test ediliyor...", "info")
            test_result = test_manual_selectors_for_url(url, manual_selectors)
            
            if not test_result['success']:
                terminal_log(f"‚ùå {name} manuel selector test ba≈üarƒ±sƒ±z: {test_result.get('error', 'Bilinmeyen hata')}", "error")
                return {
                    "success": False,
                    "message": f"‚ùå Manuel selector test ba≈üarƒ±sƒ±z: {test_result.get('error', 'Bilinmeyen hata')}",
                    "test_details": test_result
                }
            
            # Manuel selector'larƒ± uygun formata √ßevir
            selectors = {
                "container": manual_selectors['article_container'],
                "title": manual_selectors['title_selector'],
                "link": manual_selectors['link_selector'],
                "date": manual_selectors.get('date_selector', ''),
                "excerpt": manual_selectors.get('summary_selector', '')
            }
            selector_type = "manual"
            terminal_log(f"‚úÖ {name}: {test_result['article_count']} makale bulundu", "success")
        
        # Benzersiz ID olu≈ütur
        import random
        source_id = f"custom_{len(config['sources']) + 1}_{random.randint(1000000000, 9999999999)}"
        
        # Yeni kaynak olu≈ütur
        new_source = {
            "id": source_id,
            "name": name.strip(),
            "url": url,
            "description": description.strip(),
            "enabled": True,
            "selector_type": selector_type,
            "article_selectors": selectors,
            "added_date": datetime.now().isoformat(),
            "last_checked": datetime.now().isoformat(),
            "article_count": 0,
            "success_rate": 0
        }
        
        # Kaynaƒüƒ± ekle
        config["sources"].append(new_source)
        config["settings"]["last_updated"] = datetime.now().isoformat()
        
        # Kaydet
        result = save_news_sources(config)
        
        if result["success"]:
            terminal_log(f"‚úÖ '{name}' kaynaƒüƒ± ba≈üarƒ±yla eklendi", "success")
            response = {
                "success": True,
                "message": f"‚úÖ '{name}' kaynaƒüƒ± ba≈üarƒ±yla eklendi",
                "source": new_source
            }
            
            # Test sonu√ßlarƒ±nƒ± da ekle
            if auto_detect and 'test_result' in locals():
                response['test_details'] = test_result
            
            return response
        else:
            terminal_log(f"‚ùå '{name}' kaynaƒüƒ± kaydedilemedi: {result.get('message', 'Bilinmeyen hata')}", "error")
            return result
        
    except Exception as e:
        terminal_log(f"‚ùå Kaynak ekleme hatasƒ±: {str(e)}", "error")
        return {
            "success": False,
            "message": f"‚ùå Kaynak ekleme hatasƒ±: {str(e)}"
        }

def add_news_source(name, url, description=""):
    """Yeni haber kaynaƒüƒ± ekle (otomatik doƒürulama ile)"""
    return add_news_source_with_validation(name, url, description, auto_detect=True)

def remove_news_source(source_id):
    """Haber kaynaƒüƒ±nƒ± kaldƒ±r"""
    try:
        config = load_news_sources()
        
        # Kaynaƒüƒ± bul ve kaldƒ±r
        original_count = len(config["sources"])
        config["sources"] = [s for s in config["sources"] if s["id"] != source_id]
        
        if len(config["sources"]) == original_count:
            return {"success": False, "message": "‚ùå Kaynak bulunamadƒ±"}
        
        result = save_news_sources(config)
        if result["success"]:
            return {"success": True, "message": "‚úÖ Kaynak ba≈üarƒ±yla kaldƒ±rƒ±ldƒ±"}
        else:
            return result
            
    except Exception as e:
        return {"success": False, "message": f"‚ùå Kaynak kaldƒ±rma hatasƒ±: {e}"}

def toggle_news_source(source_id, enabled=None):
    """Haber kaynaƒüƒ±nƒ± aktif/pasif yap - RSS dahil"""
    try:
        config = load_news_sources()
        
        # Normal kaynaklarda ara
        for source in config["sources"]:
            if source["id"] == source_id:
                if enabled is None:
                    source["enabled"] = not source["enabled"]
                else:
                    source["enabled"] = bool(enabled)
                
                result = save_news_sources(config)
                if result["success"]:
                    status = "aktif" if source["enabled"] else "pasif"
                    return {"success": True, "message": f"‚úÖ '{source['name']}' kaynaƒüƒ± {status} yapƒ±ldƒ±"}
                else:
                    return result
        
        # RSS kaynaklarƒ±nda ara
        for rss_source in config.get("rss_sources", []):
            if rss_source["id"] == source_id:
                if enabled is None:
                    rss_source["enabled"] = not rss_source["enabled"]
                else:
                    rss_source["enabled"] = bool(enabled)
                
                result = save_news_sources(config)
                if result["success"]:
                    status = "aktif" if rss_source["enabled"] else "pasif"
                    return {"success": True, "message": f"‚úÖ '{rss_source['name']}' RSS kaynaƒüƒ± {status} yapƒ±ldƒ±"}
                else:
                    return result
        
        return {"success": False, "message": "‚ùå Kaynak bulunamadƒ±"}
        
    except Exception as e:
        return {"success": False, "message": f"‚ùå Durum deƒüi≈ütirme hatasƒ±: {e}"}

def remove_rss_source(source_id):
    """RSS kaynaƒüƒ±nƒ± kaldƒ±r"""
    try:
        config = load_news_sources()
        
        # RSS kaynaƒüƒ±nƒ± bul ve kaldƒ±r
        original_count = len(config.get("rss_sources", []))
        config["rss_sources"] = [s for s in config.get("rss_sources", []) if s["id"] != source_id]
        
        if len(config.get("rss_sources", [])) == original_count:
            return {"success": False, "message": "‚ùå RSS kaynaƒüƒ± bulunamadƒ±"}
        
        result = save_news_sources(config)
        if result["success"]:
            return {"success": True, "message": "‚úÖ RSS kaynaƒüƒ± ba≈üarƒ±yla kaldƒ±rƒ±ldƒ±"}
        else:
            return result
            
    except Exception as e:
        return {"success": False, "message": f"‚ùå RSS kaynaƒüƒ± kaldƒ±rma hatasƒ±: {e}"}

def add_rss_source(name, url, description=""):
    """Yeni RSS kaynaƒüƒ± ekle"""
    try:
        config = load_news_sources()
        
        # RSS kaynaklarƒ± listesi yoksa olu≈ütur
        if "rss_sources" not in config:
            config["rss_sources"] = []
        
        # URL'yi temizle ve doƒürula
        url = url.strip()
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        
        # Aynƒ± URL var mƒ± kontrol et
        for rss_source in config["rss_sources"]:
            if rss_source["url"] == url:
                return {
                    "success": False,
                    "message": f"‚ùå Bu RSS URL'si zaten mevcut: {rss_source['name']}"
                }
        
        # Benzersiz ID olu≈ütur
        import random
        source_id = f"rss_{len(config['rss_sources']) + 1}_{random.randint(1000000000, 9999999999)}"
        
        # Yeni RSS kaynaƒüƒ± olu≈ütur
        new_rss_source = {
            "id": source_id,
            "name": name.strip(),
            "url": url,
            "description": description.strip(),
            "enabled": True,
            "type": "rss",
            "added_date": datetime.now().isoformat(),
            "last_checked": None,
            "article_count": 0,
            "success_rate": 100
        }
        
        # RSS kaynaƒüƒ±nƒ± ekle
        config["rss_sources"].append(new_rss_source)
        config["settings"]["last_updated"] = datetime.now().isoformat()
        
        # Kaydet
        result = save_news_sources(config)
        
        if result["success"]:
            return {
                "success": True,
                "message": f"‚úÖ '{name}' RSS kaynaƒüƒ± ba≈üarƒ±yla eklendi",
                "source": new_rss_source
            }
        else:
            return result
        
    except Exception as e:
        return {
            "success": False,
            "message": f"‚ùå RSS kaynaƒüƒ± ekleme hatasƒ±: {str(e)}"
        }

def fetch_articles_from_custom_sources():
    """√ñzel haber kaynaklarƒ±ndan makale √ßek"""
    try:
        config = load_news_sources()
        all_articles = []
        
        enabled_sources = [s for s in config["sources"] if s.get("enabled", True)]
        
        if not enabled_sources:
            print("‚ö†Ô∏è Aktif haber kaynaƒüƒ± bulunamadƒ±")
            return []
        
        # Konsol log i√ßin import (terminal kaldƒ±rƒ±ldƒ±)
        try:
            from app import terminal_log
        except ImportError:
            # Eƒüer app.py'den import edilemezse normal print kullan
            def terminal_log(msg, level='info'):
                import time
                timestamp = time.strftime('%H:%M:%S')
                print(f"[{timestamp}] [{level.upper()}] {msg}")
        
        terminal_log(f"üîç {len(enabled_sources)} haber kaynaƒüƒ±ndan makale √ßekiliyor...", "info")
        
        for source in enabled_sources:
            try:
                terminal_log(f"üì∞ {source['name']} kaynaƒüƒ± kontrol ediliyor...", "info")
                
                # Kaynak URL'sini √ßek
                articles = fetch_articles_from_single_source(source)
                
                if articles:
                    all_articles.extend(articles)
                    source["article_count"] = len(articles)
                    source["success_rate"] = min(100, source.get("success_rate", 0) + 10)
                    terminal_log(f"‚úÖ {source['name']}: {len(articles)} makale bulundu", "success")
                else:
                    source["success_rate"] = max(0, source.get("success_rate", 100) - 20)
                    terminal_log(f"‚ö†Ô∏è {source['name']}: Makale bulunamadƒ±", "warning")
                
                source["last_checked"] = datetime.now().isoformat()
                
            except Exception as e:
                terminal_log(f"‚ùå {source['name']} hatasƒ±: {e}", "error")
                source["success_rate"] = max(0, source.get("success_rate", 100) - 30)
                source["last_checked"] = datetime.now().isoformat()
        
        # G√ºncellenmi≈ü istatistikleri kaydet
        save_news_sources(config)
        
        terminal_log(f"üìä Toplam {len(all_articles)} makale √ßekildi", "info")
        
        # Duplikat filtreleme uygula
        if all_articles:
            filtered_articles = filter_duplicate_articles(all_articles)
            terminal_log(f"‚úÖ Duplikat filtreleme sonrasƒ± {len(filtered_articles)} benzersiz makale", "success")
            return filtered_articles
        
        return all_articles
        
    except Exception as e:
        print(f"‚ùå √ñzel kaynaklardan makale √ßekme hatasƒ±: {e}")
        return []

def auto_detect_selectors(soup, url):
    """Sayfadan otomatik olarak en iyi selector'larƒ± tespit et - Geli≈ümi≈ü AI destekli"""
    
    print(f"üîç Otomatik selector tespiti ba≈ülatƒ±lƒ±yor: {url}")
    
    # Site t√ºr√ºne g√∂re √∂zel pattern'ler
    site_patterns = {
        'techcrunch.com': {
            'containers': [".loop-card", "li.wp-block-post", "article.wp-block-post"],
            'titles': ["h3", "h2", "h3 a", "h2 a", ".loop-card__title a"],
            'dates': ["time", "[datetime]", ".loop-card__time"]
        },
        'theverge.com': {
            'containers': [
                "article[data-testid='story-card']", 
                ".duet--content-cards--content-card",
                ".c-entry-box", 
                ".c-compact-river__entry",
                "[data-testid*='story']",
                "[data-testid*='card']",
                ".group-hover",
                "article"
            ],
            'titles': [
                "h2 a", 
                "h3 a", 
                ".c-entry-box__title a", 
                "[data-testid*='headline'] a",
                ".font-polysans a",
                "a[href*='/2025/']",
                "a[href*='/2024/']"
            ],
            'dates': [
                ".c-byline__item time", 
                "time", 
                "[datetime]",
                ".text-gray-31"
            ]
        },
        'wired.com': {
            'containers': ["article", ".archive-item-component", ".card-component"],
            'titles': ["h3 a", "h2 a", ".card-component__title a"],
            'dates': ["time", ".publish-date"]
        }
    }
    
    # URL'den site t√ºr√ºn√º tespit et
    site_type = None
    for domain in site_patterns:
        if domain in url.lower():
            site_type = domain
            break
    
    # Yaygƒ±n makale konteyner pattern'leri (√∂ncelik sƒ±rasƒ±na g√∂re)
    container_patterns = []
    
    # Site √∂zel pattern'leri √∂nce ekle
    if site_type and site_type in site_patterns:
        container_patterns.extend(site_patterns[site_type]['containers'])
        print(f"‚úÖ Site √∂zel pattern'ler eklendi: {site_type}")
    
    # Genel pattern'leri ekle
    container_patterns.extend([
        # Modern WordPress
        "li.wp-block-post", "article.wp-block-post", ".loop-card",
        
        # Modern haber siteleri
        "article[data-testid*='story']", "article[data-testid*='card']",
        ".story-card", ".news-item", ".article-card",
        
        # Yaygƒ±n CMS pattern'leri
        ".post-item", ".entry-item", ".news-entry",
        ".content-item", ".article-item",
        
        # Genel yapƒ±lar
        "article.post", "article", ".article", ".post",
        ".entry", ".story", ".news",
        
        # Class i√ßeren genel pattern'ler
        "[class*='article']", "[class*='post']", 
        "[class*='story']", "[class*='news']",
        "[class*='item']", "[class*='card']"
    ])
    
    best_selectors = {
        "container": "article",
        "title": "h2 a",
        "link": "h2 a", 
        "date": "time",
        "excerpt": "p"
    }
    
    max_containers = 0
    best_score = 0
    
    print(f"üîç {len(container_patterns)} farklƒ± pattern test ediliyor...")
    
    # Her pattern'i test et
    for i, pattern in enumerate(container_patterns):
        try:
            containers = soup.select(pattern)
            container_count = len(containers)
            
            # ƒ∞deal aralƒ±k: 2-100 konteyner
            if 2 <= container_count <= 100:
                
                # Kalite skoru hesapla
                quality_score = calculate_selector_quality(containers, pattern, url)
                
                print(f"üìä Pattern {i+1}: {pattern} -> {container_count} konteyner, skor: {quality_score}")
                
                if quality_score > best_score:
                    best_score = quality_score
                    max_containers = container_count
                    best_selectors["container"] = pattern
                    
                    # Bu konteyner i√ßin en iyi title selector'ƒ±nƒ± bul
                    if containers:
                        sample_container = containers[0]
                        
                        # Site √∂zel title pattern'leri √∂nce dene
                        title_patterns = []
                        if site_type and site_type in site_patterns:
                            title_patterns.extend(site_patterns[site_type]['titles'])
                        
                        # Genel title pattern'leri ekle
                        title_patterns.extend([
                            "h3 a", "h2 a", "h1 a", "h4 a",
                            ".title a", ".headline a", ".entry-title a",
                            "[class*='title'] a", "[class*='headline'] a",
                            "a[href*='article']", "a[href*='post']",
                            "a[href*='/20']",  # Tarih i√ßeren URL'ler
                            "a"  # Son √ßare
                        ])
                        
                        for title_pattern in title_patterns:
                            title_elem = sample_container.select_one(title_pattern)
                            if title_elem and title_elem.get_text(strip=True) and len(title_elem.get_text(strip=True)) > 10:
                                best_selectors["title"] = title_pattern
                                best_selectors["link"] = title_pattern
                                print(f"‚úÖ Title pattern bulundu: {title_pattern}")
                                break
                        
                        # Date selector
                        date_patterns = []
                        if site_type and site_type in site_patterns:
                            date_patterns.extend(site_patterns[site_type]['dates'])
                        
                        date_patterns.extend([
                            "time", ".date", ".published", ".publish-date",
                            "[class*='date']", "[class*='time']",
                            "[datetime]", ".meta-date"
                        ])
                        
                        for date_pattern in date_patterns:
                            if sample_container.select_one(date_pattern):
                                best_selectors["date"] = date_pattern
                                print(f"‚úÖ Date pattern bulundu: {date_pattern}")
                                break
                        
                        # Excerpt selector
                        excerpt_patterns = [
                            ".excerpt", ".summary", ".description",
                            ".entry-summary", ".post-excerpt",
                            "p", ".content p"
                        ]
                        
                        for excerpt_pattern in excerpt_patterns:
                            excerpt_elem = sample_container.select_one(excerpt_pattern)
                            if excerpt_elem and len(excerpt_elem.get_text(strip=True)) > 20:
                                best_selectors["excerpt"] = excerpt_pattern
                                print(f"‚úÖ Excerpt pattern bulundu: {excerpt_pattern}")
                                break
                        
        except Exception as pattern_error:
            print(f"‚ö†Ô∏è Pattern test hatasƒ± ({pattern}): {pattern_error}")
            continue
    
    print(f"üèÅ En iyi selector bulundu: {best_selectors['container']} ({max_containers} konteyner, skor: {best_score})")
    
    return best_selectors, max_containers

def calculate_selector_quality(containers, pattern, url):
    """Selector kalitesini hesapla"""
    try:
        if not containers:
            return 0
        
        score = 0
        sample_size = min(3, len(containers))  # ƒ∞lk 3 konteyner test et
        
        for container in containers[:sample_size]:
            # Ba≈ülƒ±k var mƒ±?
            title_found = False
            title_selectors = ["h1", "h2", "h3", "h4", ".title", ".headline", "a"]
            
            for ts in title_selectors:
                title_elem = container.select_one(ts)
                if title_elem and len(title_elem.get_text(strip=True)) > 10:
                    title_found = True
                    score += 10
                    break
            
            # Link var mƒ±?
            link_found = False
            links = container.select("a[href]")
            for link in links:
                href = link.get('href', '')
                if href and (href.startswith('http') or href.startswith('/')):
                    link_found = True
                    score += 10
                    break
            
            # Tarih var mƒ±?
            date_selectors = ["time", ".date", "[datetime]", "[class*='date']"]
            for ds in date_selectors:
                if container.select_one(ds):
                    score += 5
                    break
            
            # ƒ∞√ßerik var mƒ±?
            content_length = len(container.get_text(strip=True))
            if content_length > 50:
                score += 5
            if content_length > 200:
                score += 5
        
        # Konteyner sayƒ±sƒ± bonusu (ideal aralƒ±k)
        container_count = len(containers)
        if 5 <= container_count <= 20:
            score += 20
        elif 3 <= container_count <= 30:
            score += 10
        elif container_count > 100:
            score -= 20  # √áok fazla konteyner ceza
        
        # Pattern spesifik bonuslar
        if 'article' in pattern.lower():
            score += 10
        if 'post' in pattern.lower():
            score += 8
        if 'story' in pattern.lower():
            score += 8
        if 'card' in pattern.lower():
            score += 5
        
        return score / sample_size  # Ortalama skor
        
    except Exception as e:
        print(f"‚ùå Kalite hesaplama hatasƒ±: {e}")
        return 0

def validate_selectors(soup, selectors):
    """Selector'larƒ±n √ßalƒ±≈üƒ±p √ßalƒ±≈ümadƒ±ƒüƒ±nƒ± kontrol et"""
    
    container_selector = selectors.get("container", "article")
    containers = soup.select(container_selector)
    
    if not containers:
        return False, "Konteyner bulunamadƒ±"
    
    if len(containers) > 100:
        return False, f"√áok fazla konteyner ({len(containers)}), selector √ßok genel"
    
    # Birden fazla konteyner test et
    valid_containers = 0
    sample_titles = []
    
    for i, container in enumerate(containers[:5]):  # ƒ∞lk 5 konteyner test et
        title_selector = selectors.get("title", "h2 a")
        title_elem = container.select_one(title_selector)
        
        # Alternatif title selector'larƒ± dene
        if not title_elem:
            alt_title_selectors = [
                "h1 a", "h2 a", "h3 a", "h4 a",
                ".title a", ".headline a", 
                "[class*='title'] a", "[class*='headline'] a",
                "a[href*='article']", "a[href*='post']",
                "a[href*='/20']",  # Tarih i√ßeren URL'ler
                "a"  # Son √ßare
            ]
            
            for alt_selector in alt_title_selectors:
                title_elem = container.select_one(alt_selector)
                if title_elem and title_elem.get_text(strip=True) and len(title_elem.get_text(strip=True)) > 10:
                    # Ba≈üarƒ±lƒ± selector'ƒ± g√ºncelle
                    selectors["title"] = alt_selector
                    selectors["link"] = alt_selector
                    break
        
        if title_elem:
            title_text = title_elem.get_text(strip=True)
            if len(title_text) >= 10:
                # Link kontrol√º
                link_elem = title_elem if title_elem.name == 'a' else title_elem.find('a')
                if link_elem and link_elem.get('href'):
                    valid_containers += 1
                    sample_titles.append(title_text[:50])
    
    if valid_containers == 0:
        return False, "Hi√ßbir konteynerde ge√ßerli ba≈ülƒ±k/link bulunamadƒ±"
    
    success_rate = (valid_containers / min(len(containers), 5)) * 100
    
    if success_rate < 40:  # %40'dan az ba≈üarƒ± oranƒ±
        return False, f"D√º≈ü√ºk ba≈üarƒ± oranƒ±: %{success_rate:.1f} ({valid_containers}/{min(len(containers), 5)})"
    
    return True, f"‚úÖ {len(containers)} konteyner, {valid_containers} ge√ßerli, √∂rnek: {sample_titles[0] if sample_titles else 'N/A'}..."

def fetch_articles_from_single_source(source):
    """Tek bir kaynaktan makale √ßek"""
    try:
        url = source["url"]
        selectors = source.get("article_selectors", {})
        source_name = source.get("name", "Bilinmeyen")
        
        # Sayfayƒ± √ßek
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        articles = []
        
        # Eƒüer selector_type auto_detect ise veya mevcut selector'lar √ßalƒ±≈ümƒ±yorsa
        if source.get('selector_type') == 'auto_detect' or not selectors:
            print(f"üîç {source_name} i√ßin otomatik selector tespiti yapƒ±lƒ±yor...")
            auto_selectors, container_count = auto_detect_selectors(soup, url)
            
            # Otomatik tespit edilen selector'larƒ± doƒürula
            is_valid, validation_msg = validate_selectors(soup, auto_selectors)
            
            if is_valid:
                print(f"‚úÖ Otomatik tespit ba≈üarƒ±lƒ±: {validation_msg}")
                selectors = auto_selectors
                
                # Ba≈üarƒ±lƒ± selector'larƒ± kaydet
                source["article_selectors"] = auto_selectors
                source["selector_type"] = "auto_detected"
            else:
                print(f"‚ùå Otomatik tespit ba≈üarƒ±sƒ±z: {validation_msg}")
                return []
        else:
            # Mevcut selector'larƒ± doƒürula
            is_valid, validation_msg = validate_selectors(soup, selectors)
            if not is_valid:
                print(f"‚ö†Ô∏è Mevcut selector'lar √ßalƒ±≈ümƒ±yor: {validation_msg}")
                print(f"üîÑ Otomatik tespit deneniyor...")
                
                auto_selectors, container_count = auto_detect_selectors(soup, url)
                is_auto_valid, auto_validation_msg = validate_selectors(soup, auto_selectors)
                
                if is_auto_valid:
                    print(f"‚úÖ Otomatik tespit ile d√ºzeltildi: {auto_validation_msg}")
                    selectors = auto_selectors
                else:
                    print(f"‚ùå Otomatik tespit de ba≈üarƒ±sƒ±z: {auto_validation_msg}")
                    return []
        
        # Makale konteynerlerini bul
        container_selector = selectors.get("container", "article, .article, .post")
        containers = soup.select(container_selector)
        
        if not containers:
            # Alternatif selectors dene
            alternative_selectors = [
                "li.wp-block-post",  # TechCrunch modern
                "article[data-testid='story-card']",  # The Verge modern
                ".c-entry-box", ".c-compact-river__entry",  # The Verge classic
                "article", ".post", ".news-item", ".story", 
                ".entry", ".content-item", "[class*='article']",
                "[class*='post']", "[class*='news']", "[class*='loop-card']"
            ]
            
            for alt_selector in alternative_selectors:
                containers = soup.select(alt_selector)
                if containers:
                    print(f"üîÑ Alternatif selector kullanƒ±ldƒ±: {alt_selector}")
                    break
        
        print(f"üîç {source['name']}: {len(containers)} konteyner bulundu")
        
        for container in containers[:5]:  # ƒ∞lk 5 makale
            try:
                # Ba≈ülƒ±k bul
                title_selector = selectors.get("title", "h1, h2, h3, .title, .headline")
                title_elem = container.select_one(title_selector)
                
                # Alternatif title selector'larƒ± dene
                if not title_elem:
                    alt_title_selectors = [
                        "h3.loop-card__title a",  # TechCrunch modern
                        "h2 a", ".c-entry-box__title a",  # The Verge
                        "h1 a", "h2 a", "h3 a", "h4 a",
                        ".title a", ".headline a", "[class*='title'] a"
                    ]
                    
                    for alt_selector in alt_title_selectors:
                        title_elem = container.select_one(alt_selector)
                        if title_elem:
                            break
                
                if not title_elem:
                    continue
                
                title = title_elem.get_text(strip=True)
                
                # Link bul
                link_elem = title_elem.find('a') if title_elem.name != 'a' else title_elem
                if not link_elem:
                    link_elem = container.select_one('a')
                
                if not link_elem:
                    continue
                
                link = link_elem.get('href', '')
                
                # Relative URL'leri absolute yap
                if link.startswith('/'):
                    from urllib.parse import urljoin
                    link = urljoin(url, link)
                elif not link.startswith('http'):
                    continue
                
                # √ñzet bul
                excerpt_selector = selectors.get("excerpt", ".excerpt, .summary, p")
                excerpt_elem = container.select_one(excerpt_selector)
                excerpt = excerpt_elem.get_text(strip=True)[:200] if excerpt_elem else ""
                
                # Tarih bul (opsiyonel)
                date_selector = selectors.get("date", "time, .date, .published")
                date_elem = container.select_one(date_selector)
                date_str = date_elem.get_text(strip=True) if date_elem else ""
                
                if title and link:
                    # Makale i√ßeriƒüini √ßek (basit y√∂ntem)
                    try:
                        content_response = requests.get(link, headers=headers, timeout=15)
                        content_soup = BeautifulSoup(content_response.text, 'html.parser')
                        
                        # Ana i√ßeriƒüi √ßƒ±kar
                        content_text = ""
                        content_selectors = [
                            'article', '.post-content', '.entry-content', 
                            '.article-content', '.content', 'main', '.story-body'
                        ]
                        
                        for cs in content_selectors:
                            content_elem = content_soup.select_one(cs)
                            if content_elem:
                                # Script ve style taglarƒ±nƒ± kaldƒ±r
                                for script in content_elem(["script", "style", "nav", "footer", "header"]):
                                    script.decompose()
                                content_text = content_elem.get_text(strip=True)
                                break
                        
                        # Eƒüer ana i√ßerik bulunamazsa body'den al
                        if not content_text:
                            body = content_soup.find('body')
                            if body:
                                for script in body(["script", "style", "nav", "footer", "header"]):
                                    script.decompose()
                                content_text = body.get_text(strip=True)
                        
                        # ƒ∞√ßeriƒüi temizle ve sƒ±nƒ±rla
                        content_text = ' '.join(content_text.split())[:2000]
                        
                    except Exception as content_error:
                        print(f"‚ö†Ô∏è ƒ∞√ßerik √ßekme hatasƒ±: {content_error}")
                        content_text = excerpt or title  # Fallback olarak √∂zet veya ba≈ülƒ±ƒüƒ± kullan
                    
                    # Hash olu≈ütur
                    article_hash = hashlib.md5(title.encode()).hexdigest()
                    
                    articles.append({
                        "title": title,
                        "url": link,
                        "content": content_text or excerpt or title,
                        "excerpt": excerpt,
                        "date": date_str,
                        "hash": article_hash,
                        "source": source["name"],
                        "source_id": source["id"],
                        "fetch_date": datetime.now().isoformat(),
                        "is_new": True,
                        "already_posted": False
                    })
                    
            except Exception as e:
                print(f"‚ö†Ô∏è Makale parse hatasƒ±: {e}")
                continue
        
        return articles
        
    except Exception as e:
        print(f"‚ùå {source.get('name', 'Bilinmeyen')} kaynak hatasƒ±: {e}")
        return []

def get_news_sources_stats():
    """Haber kaynaklarƒ± istatistiklerini al - RSS dahil"""
    try:
        config = load_news_sources()
        
        # Normal kaynaklar
        sources = config.get("sources", [])
        rss_sources = config.get("rss_sources", [])
        
        # T√ºm kaynaklarƒ± birle≈ütir
        all_sources = []
        
        # Normal kaynaklarƒ± ekle
        for source in sources:
            source_copy = source.copy()
            source_copy["type"] = "scraping"
            source_copy["total_articles"] = source.get("article_count", 0)
            
            # Son kontrol zamanƒ±nƒ± hesapla
            last_checked = source.get("last_checked")
            if last_checked:
                try:
                    from datetime import datetime
                    last_check_time = datetime.fromisoformat(last_checked.replace('Z', '+00:00'))
                    hours_ago = (datetime.now() - last_check_time).total_seconds() / 3600
                    source_copy["last_check_hours"] = int(hours_ago)
                except:
                    source_copy["last_check_hours"] = 999
            else:
                source_copy["last_check_hours"] = 999
                
            all_sources.append(source_copy)
        
        # RSS kaynaklarƒ±nƒ± ekle
        for rss_source in rss_sources:
            rss_copy = rss_source.copy()
            rss_copy["type"] = "rss"
            rss_copy["total_articles"] = rss_source.get("article_count", 0)
            
            # Son kontrol zamanƒ±nƒ± hesapla
            last_checked = rss_source.get("last_checked")
            if last_checked:
                try:
                    from datetime import datetime
                    last_check_time = datetime.fromisoformat(last_checked.replace('Z', '+00:00'))
                    hours_ago = (datetime.now() - last_check_time).total_seconds() / 3600
                    rss_copy["last_check_hours"] = int(hours_ago)
                except:
                    rss_copy["last_check_hours"] = 999
            else:
                rss_copy["last_check_hours"] = 999
                
            all_sources.append(rss_copy)
        
        # ƒ∞statistikleri hesapla
        total_sources = len(all_sources)
        enabled_sources = len([s for s in all_sources if s.get("enabled", True)])
        total_articles = sum(s.get("total_articles", 0) for s in all_sources)
        avg_success_rate = sum(s.get("success_rate", 0) for s in all_sources) / max(1, total_sources)
        
        return {
            "total_sources": total_sources,
            "enabled_sources": enabled_sources,
            "disabled_sources": total_sources - enabled_sources,
            "total_articles_fetched": total_articles,
            "average_success_rate": round(avg_success_rate, 1),
            "sources": all_sources,  # Birle≈ütirilmi≈ü kaynaklar
            "scraping_sources": sources,  # Sadece scraping kaynaklarƒ±
            "rss_sources": rss_sources,  # Sadece RSS kaynaklarƒ±
            "settings": config["settings"]
        }
        
    except Exception as e:
        return {
            "total_sources": 0,
            "enabled_sources": 0,
            "disabled_sources": 0,
            "total_articles_fetched": 0,
            "average_success_rate": 0,
            "sources": [],
            "scraping_sources": [],
            "rss_sources": [],
            "settings": {},
            "error": str(e)
        }

def update_fetch_articles_function():
    """Ana makale √ßekme fonksiyonunu g√ºncelle - √∂zel kaynaklarƒ± dahil et"""
    try:
        # √ñnce √∂zel kaynaklardan makale √ßek
        custom_articles = fetch_articles_from_custom_sources()
        
        # Sonra mevcut TechCrunch fallback'i √ßek (eƒüer √∂zel kaynaklarda TechCrunch yoksa)
        config = load_news_sources()
        has_techcrunch = any("techcrunch" in s.get("url", "").lower() for s in config["sources"] if s.get("enabled", True))
        
        if not has_techcrunch:
            print("üîÑ TechCrunch fallback ekleniyor...")
            fallback_articles = fetch_latest_ai_articles_fallback()
            custom_articles.extend(fallback_articles)
        
        return custom_articles
        
    except Exception as e:
        print(f"‚ùå G√ºncellenmi≈ü makale √ßekme hatasƒ±: {e}")
        # Fallback olarak eski fonksiyonu √ßaƒüƒ±r
        return fetch_latest_ai_articles_fallback()

# =============================================================================
# G√úVENLƒ∞ LOGGING Sƒ∞STEMƒ∞
# =============================================================================

def safe_log(message, level="INFO", sensitive_data=None):
    """G√ºvenli logging - ≈üifre ve API anahtarlarƒ±nƒ± gizler"""
    import os
    
    # Sadece debug modunda detaylƒ± log
    debug_mode = os.environ.get('DEBUG', 'False').lower() == 'true'
    
    if not debug_mode and level == "DEBUG":
        return
    
    # Mesajƒ± g√ºvenli hale getir
    safe_message = sanitize_log_message(str(message))
    
    # Hassas verileri gizle
    if sensitive_data:
        for key, value in sensitive_data.items():
            if value and len(str(value)) > 3:
                masked_value = str(value)[:3] + "*" * (len(str(value)) - 3)
                safe_message = safe_message.replace(str(value), masked_value)
    
    # Production'da sadece √∂nemli loglarƒ± g√∂ster
    if not debug_mode and level not in ["ERROR", "WARNING", "INFO"]:
        return
    
    print(f"[{level}] {safe_message}")

# =============================================================================
# G√úVENLƒ∞K KONTROL FONKSƒ∞YONLARI
# =============================================================================

def check_security_configuration():
    """G√ºvenlik yapƒ±landƒ±rmasƒ±nƒ± kontrol et - E-posta OTP sistemi i√ßin g√ºncellenmi≈ü"""
    import os
    
    security_issues = []
    
    # 1. Debug mode kontrol√º
    debug_mode = os.environ.get('DEBUG', 'False').lower() == 'true'
    flask_env = os.environ.get('FLASK_ENV', 'production')
    
    if debug_mode and flask_env == 'production':
        security_issues.append("‚ö†Ô∏è Production'da DEBUG modu a√ßƒ±k!")
    
    # 2. E-posta OTP sistemi kontrol√º
    admin_email = os.environ.get('ADMIN_EMAIL', '')
    email_address = os.environ.get('EMAIL_ADDRESS', '')
    email_password = os.environ.get('EMAIL_PASSWORD', '')
    
    if not admin_email:
        security_issues.append("üìß ADMIN_EMAIL yapƒ±landƒ±rƒ±lmamƒ±≈ü! Giri≈ü yapƒ±lamaz.")
    elif not '@' in admin_email or '.' not in admin_email:
        security_issues.append("üìß ADMIN_EMAIL ge√ßersiz format!")
    
    if not email_address:
        security_issues.append("üìß EMAIL_ADDRESS yapƒ±landƒ±rƒ±lmamƒ±≈ü! OTP g√∂nderilemez.")
    elif not '@' in email_address or '.' not in email_address:
        security_issues.append("üìß EMAIL_ADDRESS ge√ßersiz format!")
    
    if not email_password:
        security_issues.append("üîê EMAIL_PASSWORD yapƒ±landƒ±rƒ±lmamƒ±≈ü! SMTP baƒülantƒ±sƒ± kurulamaz.")
    elif len(email_password) < 8:
        security_issues.append("üîê EMAIL_PASSWORD √ßok kƒ±sa! App Password kullanƒ±n.")
    
    # 3. Secret key kontrol√º
    secret_key = os.environ.get('SECRET_KEY', 'your-secret-key-here')
    if secret_key == 'your-secret-key-here' or len(secret_key) < 32:
        security_issues.append("üîë G√º√ßl√º SECRET_KEY kullanƒ±n! (En az 32 karakter)")
    
    # 4. API anahtarlarƒ± kontrol√º
    api_keys = [
        'GOOGLE_API_KEY',
        'TWITTER_API_KEY',
        'TWITTER_API_SECRET',
        'TWITTER_ACCESS_TOKEN',
        'TWITTER_ACCESS_TOKEN_SECRET',
        'TWITTER_BEARER_TOKEN'
    ]
    
    for key in api_keys:
        value = os.environ.get(key, '')
        if value and ('your-' in value.lower() or 'example' in value.lower() or 'test' in value.lower()):
            security_issues.append(f"üîê {key} √∂rnek/test deƒüer i√ßeriyor!")
    
    # 5. Telegram g√ºvenlik kontrol√º
    telegram_token = os.environ.get('TELEGRAM_BOT_TOKEN', '')
    if telegram_token and len(telegram_token) < 40:
        security_issues.append("ü§ñ TELEGRAM_BOT_TOKEN √ßok kƒ±sa! Ge√ßerli token kullanƒ±n.")
    
    # 6. Gmail g√ºvenlik kontrol√º
    gmail_email = os.environ.get('GMAIL_EMAIL', '')
    gmail_password = os.environ.get('GMAIL_APP_PASSWORD', '')
    
    if gmail_email and not gmail_password:
        security_issues.append("üìß GMAIL_EMAIL var ama GMAIL_APP_PASSWORD eksik!")
    
    # 7. G√ºvenlik seviyesi deƒüerlendirmesi
    security_score = 100 - (len(security_issues) * 10)
    security_level = "Y√ºksek" if security_score >= 80 else "Orta" if security_score >= 60 else "D√º≈ü√ºk"
    
    return {
        "secure": len(security_issues) == 0,
        "issues": security_issues,
        "debug_mode": debug_mode,
        "flask_env": flask_env,
        "auth_method": "E-posta OTP",
        "admin_email": admin_email,
        "email_configured": bool(email_address and email_password),
        "security_score": security_score,
        "security_level": security_level,
        "total_checks": 7,
        "passed_checks": 7 - len(security_issues)
    }

def sanitize_log_message(message):
    """Log mesajlarƒ±ndan hassas bilgileri temizle"""
    import re
    
    # API anahtarƒ± pattern'leri
    patterns = [
        r'AIza[0-9A-Za-z-_]{35}',  # Google API Key
        r'sk-[a-zA-Z0-9]{48}',     # OpenAI API Key
        r'[0-9]{10}:[A-Za-z0-9_-]{35}',  # Telegram Bot Token
        r'[A-Za-z0-9]{25}',        # Twitter Bearer Token
        r'[A-Za-z0-9]{15,25}',     # Twitter API Keys
    ]
    
    for pattern in patterns:
        message = re.sub(pattern, '***MASKED***', message)
    
    # ≈ûifre pattern'leri
    password_patterns = [
        r'password["\s]*[:=]["\s]*[^"\s]+',
        r'pass["\s]*[:=]["\s]*[^"\s]+',
        r'token["\s]*[:=]["\s]*[^"\s]+',
        r'key["\s]*[:=]["\s]*[^"\s]+',
    ]
    
    for pattern in password_patterns:
        message = re.sub(pattern, 'password=***MASKED***', message, flags=re.IGNORECASE)
    
    return message

def calculate_text_similarity(text1, text2):
    """ƒ∞ki metin arasƒ±ndaki benzerlik oranƒ±nƒ± hesapla (0-1 arasƒ±)"""
    if not text1 or not text2:
        return 0.0
    
    # Metinleri normalize et
    text1 = text1.lower().strip()
    text2 = text2.lower().strip()
    
    # √áok kƒ±sa metinler i√ßin direkt kar≈üƒ±la≈ütƒ±rma
    if len(text1) < 10 or len(text2) < 10:
        return 1.0 if text1 == text2 else 0.0
    
    # SequenceMatcher ile benzerlik hesapla
    similarity = SequenceMatcher(None, text1, text2).ratio()
    return similarity

def normalize_title_for_comparison(title):
    """Ba≈ülƒ±ƒüƒ± kar≈üƒ±la≈ütƒ±rma i√ßin normalize et"""
    if not title:
        return ""
    
    # K√º√ß√ºk harfe √ßevir
    normalized = title.lower()
    
    # Gereksiz karakterleri kaldƒ±r
    normalized = re.sub(r'[^\w\s]', ' ', normalized)
    
    # √áoklu bo≈üluklarƒ± tek bo≈üluƒüa √ßevir
    normalized = re.sub(r'\s+', ' ', normalized).strip()
    
    # Yaygƒ±n kelimeler ve ifadeleri kaldƒ±r
    stop_words = ['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'says', 'announces', 'reveals', 'launches', 'introduces']
    words = normalized.split()
    filtered_words = [word for word in words if word not in stop_words and len(word) > 2]
    
    return ' '.join(filtered_words)

def extract_key_content_features(content):
    """ƒ∞√ßerikten anahtar √∂zellikler √ßƒ±kar"""
    if not content:
        return set()
    
    # Metni normalize et
    normalized = content.lower()
    
    # Sayƒ±larƒ± ve √∂zel terimleri √ßƒ±kar
    features = set()
    
    # Sayƒ±sal deƒüerler (para, y√ºzde, sayƒ±lar)
    numbers = re.findall(r'\$[\d,]+(?:\.\d+)?[bmk]?|\d+(?:\.\d+)?%|\d+(?:\.\d+)?(?:\s*(?:billion|million|thousand|percent))?', normalized)
    features.update(numbers)
    
    # ≈ûirket isimleri (b√ºy√ºk harfle ba≈ülayan kelimeler)
    companies = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', content)
    features.update([c.lower() for c in companies])
    
    # √ñnemli teknoloji terimleri
    tech_terms = re.findall(r'\b(?:ai|artificial intelligence|machine learning|deep learning|neural network|algorithm|api|cloud|blockchain|cryptocurrency|robot|automation|quantum|5g|iot|ar|vr|metaverse)\b', normalized)
    features.update(tech_terms)
    
    return features

def check_content_similarity(article1, article2, title_threshold=0.8, content_threshold=0.6):
    """ƒ∞ki makale arasƒ±ndaki benzerliƒüi kontrol et"""
    try:
        title1 = article1.get('title', '')
        title2 = article2.get('title', '')
        content1 = article1.get('content', '')
        content2 = article2.get('content', '')
        
        # Ba≈ülƒ±k benzerliƒüi
        normalized_title1 = normalize_title_for_comparison(title1)
        normalized_title2 = normalize_title_for_comparison(title2)
        title_similarity = calculate_text_similarity(normalized_title1, normalized_title2)
        
        # Eƒüer ba≈ülƒ±klar √ßok benzer ise, muhtemelen aynƒ± haber
        if title_similarity >= title_threshold:
            return True, title_similarity, "title"
        
        # ƒ∞√ßerik √∂zellik benzerliƒüi
        features1 = extract_key_content_features(content1)
        features2 = extract_key_content_features(content2)
        
        if features1 and features2:
            # Jaccard benzerliƒüi (kesi≈üim / birle≈üim)
            intersection = len(features1.intersection(features2))
            union = len(features1.union(features2))
            feature_similarity = intersection / union if union > 0 else 0.0
            
            if feature_similarity >= content_threshold:
                return True, feature_similarity, "content_features"
        
        # ƒ∞√ßerik metni benzerliƒüi (sadece kƒ±sa i√ßerikler i√ßin)
        if len(content1) < 1000 and len(content2) < 1000:
            content_similarity = calculate_text_similarity(content1[:500], content2[:500])
            if content_similarity >= content_threshold:
                return True, content_similarity, "content_text"
        
        return False, max(title_similarity, feature_similarity if 'feature_similarity' in locals() else 0), "no_match"
        
    except Exception as e:
        print(f"Benzerlik kontrol√º hatasƒ±: {e}")
        return False, 0.0, "error"

def filter_duplicate_articles(new_articles, existing_articles=None):
    """Yeni makalelerden duplikatlarƒ± filtrele"""
    try:
        # Ayarlarƒ± y√ºkle
        settings = load_automation_settings()
        
        # Eƒüer duplikat tespiti kapalƒ±ysa, sadece temel kontrolleri yap
        if not settings.get('enable_duplicate_detection', True):
            print("üîÑ Geli≈ümi≈ü duplikat tespiti kapalƒ±, sadece temel kontroller yapƒ±lƒ±yor...")
            return basic_duplicate_filter(new_articles, existing_articles)
        
        # E≈üik deƒüerlerini ayarlardan al
        title_threshold = settings.get('title_similarity_threshold', 0.8)
        content_threshold = settings.get('content_similarity_threshold', 0.6)
        
        print(f"üîç Geli≈ümi≈ü duplikat tespiti aktif (Ba≈ülƒ±k: {title_threshold:.0%}, ƒ∞√ßerik: {content_threshold:.0%})")
        
        if existing_articles is None:
            # Mevcut payla≈üƒ±lan makaleleri y√ºkle
            existing_articles = load_json(HISTORY_FILE)
        
        # Bekleyen tweet'leri de kontrol et
        pending_tweets = load_json("pending_tweets.json")
        pending_articles = [tweet.get('article', {}) for tweet in pending_tweets if tweet.get('article')]
        
        # T√ºm mevcut makaleleri birle≈ütir (silinen makaleler de dahil)
        all_existing = existing_articles + pending_articles
        
        # Silinen makaleleri de kontrol et (deleted=True olanlar)
        deleted_articles = [article for article in existing_articles if article.get('deleted', False)]
        if deleted_articles:
            print(f"üóëÔ∏è {len(deleted_articles)} silinen makale duplikat kontrole dahil edildi")
        
        filtered_articles = []
        duplicate_count = 0
        
        for new_article in new_articles:
            is_duplicate = False
            
            # √ñnce URL kontrol√º (hƒ±zlƒ±)
            new_url = new_article.get('url', '')
            for existing in all_existing:
                if new_url == existing.get('url', ''):
                    is_duplicate = True
                    break
            
            if is_duplicate:
                duplicate_count += 1
                print(f"üîÑ URL duplikatƒ± atlandƒ±: {new_article.get('title', '')[:50]}...")
                continue
            
            # Hash kontrol√º (hƒ±zlƒ±)
            new_hash = new_article.get('hash', '')
            if new_hash:
                for existing in all_existing:
                    if new_hash == existing.get('hash', ''):
                        is_duplicate = True
                        break
            
            if is_duplicate:
                duplicate_count += 1
                print(f"üîÑ Hash duplikatƒ± atlandƒ±: {new_article.get('title', '')[:50]}...")
                continue
            
            # ƒ∞√ßerik benzerliƒüi kontrol√º (yava≈ü ama etkili)
            for existing in all_existing:
                is_similar, similarity_score, match_type = check_content_similarity(
                    new_article, existing, title_threshold, content_threshold
                )
                if is_similar:
                    is_duplicate = True
                    print(f"üîÑ ƒ∞√ßerik benzerliƒüi ({match_type}: {similarity_score:.2f}) - atlandƒ±: {new_article.get('title', '')[:50]}...")
                    break
            
            if not is_duplicate:
                # Aynƒ± batch i√ßinde de kontrol et
                for other_article in filtered_articles:
                    is_similar, similarity_score, match_type = check_content_similarity(
                        new_article, other_article, title_threshold, content_threshold
                    )
                    if is_similar:
                        is_duplicate = True
                        print(f"üîÑ Batch i√ßi benzerlik ({match_type}: {similarity_score:.2f}) - atlandƒ±: {new_article.get('title', '')[:50]}...")
                        break
            
            if not is_duplicate:
                filtered_articles.append(new_article)
                print(f"‚úÖ Yeni makale eklendi: {new_article.get('title', '')[:50]}...")
            else:
                duplicate_count += 1
        
        print(f"üìä Duplikat filtreleme tamamlandƒ±: {len(new_articles)} makale ‚Üí {len(filtered_articles)} benzersiz makale ({duplicate_count} duplikat)")
        return filtered_articles
        
    except Exception as e:
        print(f"Duplikat filtreleme hatasƒ±: {e}")
        return new_articles  # Hata durumunda orijinal listeyi d√∂nd√ºr

def basic_duplicate_filter(new_articles, existing_articles=None):
    """Temel duplikat filtreleme - sadece URL ve hash kontrol√º"""
    try:
        if existing_articles is None:
            existing_articles = load_json(HISTORY_FILE)
        
        # Bekleyen tweet'leri de kontrol et
        pending_tweets = load_json("pending_tweets.json")
        pending_articles = [tweet.get('article', {}) for tweet in pending_tweets if tweet.get('article')]
        
        # T√ºm mevcut makaleleri birle≈ütir (silinen makaleler de dahil)
        all_existing = existing_articles + pending_articles
        
        # Silinen makaleleri de kontrol et (deleted=True olanlar)
        deleted_articles = [article for article in existing_articles if article.get('deleted', False)]
        if deleted_articles:
            print(f"üóëÔ∏è {len(deleted_articles)} silinen makale temel duplikat kontrole dahil edildi")
        
        # Mevcut URL'ler ve hash'ler
        existing_urls = set(article.get('url', '') for article in all_existing)
        existing_hashes = set(article.get('hash', '') for article in all_existing)
        
        filtered_articles = []
        duplicate_count = 0
        
        for new_article in new_articles:
            new_url = new_article.get('url', '')
            new_hash = new_article.get('hash', '')
            
            # URL kontrol√º
            if new_url in existing_urls:
                duplicate_count += 1
                print(f"üîÑ URL duplikatƒ± atlandƒ±: {new_article.get('title', '')[:50]}...")
                continue
            
            # Hash kontrol√º
            if new_hash and new_hash in existing_hashes:
                duplicate_count += 1
                print(f"üîÑ Hash duplikatƒ± atlandƒ±: {new_article.get('title', '')[:50]}...")
                continue
            
            # Aynƒ± batch i√ßinde URL kontrol√º
            batch_urls = set(article.get('url', '') for article in filtered_articles)
            if new_url in batch_urls:
                duplicate_count += 1
                print(f"üîÑ Batch i√ßi URL duplikatƒ± atlandƒ±: {new_article.get('title', '')[:50]}...")
                continue
            
            filtered_articles.append(new_article)
            print(f"‚úÖ Yeni makale eklendi: {new_article.get('title', '')[:50]}...")
        
        print(f"üìä Temel duplikat filtreleme tamamlandƒ±: {len(new_articles)} makale ‚Üí {len(filtered_articles)} benzersiz makale ({duplicate_count} duplikat)")
        return filtered_articles
        
    except Exception as e:
        print(f"Temel duplikat filtreleme hatasƒ±: {e}")
        return new_articles

def clean_duplicate_pending_tweets():
    """Bekleyen tweet'lerdeki duplikatlarƒ± temizle"""
    try:
        pending_tweets = load_json("pending_tweets.json")
        
        if not pending_tweets:
            return {
                "success": True,
                "message": "Bekleyen tweet bulunamadƒ±",
                "original_count": 0,
                "cleaned_count": 0,
                "removed_count": 0
            }
        
        print(f"üîç {len(pending_tweets)} bekleyen tweet kontrol ediliyor...")
        
        # Benzersiz tweet'leri saklamak i√ßin
        unique_tweets = []
        seen_urls = set()
        seen_hashes = set()
        seen_titles = set()
        
        duplicate_count = 0
        
        for tweet in pending_tweets:
            article = tweet.get('article', {})
            url = article.get('url', '')
            hash_val = article.get('hash', '')
            title = article.get('title', '')
            
            # URL kontrol√º
            if url and url in seen_urls:
                duplicate_count += 1
                print(f"üîÑ URL duplikatƒ± atlandƒ±: {title[:50]}...")
                continue
            
            # Hash kontrol√º
            if hash_val and hash_val in seen_hashes:
                duplicate_count += 1
                print(f"üîÑ Hash duplikatƒ± atlandƒ±: {title[:50]}...")
                continue
            
            # Ba≈ülƒ±k kontrol√º (normalize edilmi≈ü)
            if title:
                normalized_title = normalize_title_for_comparison(title)
                if normalized_title in seen_titles:
                    duplicate_count += 1
                    print(f"üîÑ Ba≈ülƒ±k duplikatƒ± atlandƒ±: {title[:50]}...")
                    continue
                seen_titles.add(normalized_title)
            
            # Benzersiz tweet olarak ekle
            unique_tweets.append(tweet)
            if url:
                seen_urls.add(url)
            if hash_val:
                seen_hashes.add(hash_val)
            
            print(f"‚úÖ Benzersiz tweet korundu: {title[:50]}...")
        
        # Temizlenmi≈ü listeyi kaydet
        save_json("pending_tweets.json", unique_tweets)
        
        result = {
            "success": True,
            "message": f"‚úÖ Bekleyen tweet'ler temizlendi",
            "original_count": len(pending_tweets),
            "cleaned_count": len(unique_tweets),
            "removed_count": duplicate_count
        }
        
        print(f"üìä Duplikat temizleme tamamlandƒ±: {len(pending_tweets)} ‚Üí {len(unique_tweets)} tweet ({duplicate_count} duplikat kaldƒ±rƒ±ldƒ±)")
        
        return result
        
    except Exception as e:
        print(f"‚ùå Duplikat temizleme hatasƒ±: {e}")
        return {
            "success": False,
            "message": f"‚ùå Temizleme hatasƒ±: {str(e)}",
            "original_count": 0,
            "cleaned_count": 0,
            "removed_count": 0
        }

# =============================================================================
# Rate limit y√∂netimi i√ßin global deƒüi≈ükenler
RATE_LIMIT_FILE = "rate_limit_status.json"
TWITTER_RATE_LIMITS = {
    "tweets": {"limit": 5, "window": 900},  # 15 dakikada 5 tweet (Free plan i√ßin g√ºvenli)
    "user_lookup": {"limit": 50, "window": 900},  # 15 dakikada 50 kullanƒ±cƒ± sorgusu
    "timeline": {"limit": 30, "window": 900}  # 15 dakikada 30 timeline sorgusu
}

def load_rate_limit_status():
    """Rate limit durumunu y√ºkle"""
    try:
        if os.path.exists(RATE_LIMIT_FILE):
            with open(RATE_LIMIT_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}
    except Exception as e:
        print(f"Rate limit durumu y√ºklenirken hata: {e}")
        return {}

def save_rate_limit_status(status):
    """Rate limit durumunu kaydet"""
    try:
        with open(RATE_LIMIT_FILE, 'w', encoding='utf-8') as f:
            json.dump(status, f, indent=2, ensure_ascii=False)
    except Exception as e:
        print(f"Rate limit durumu kaydedilirken hata: {e}")

def check_rate_limit(endpoint="tweets"):
    """Rate limit kontrol√º yap"""
    try:
        status = load_rate_limit_status()
        current_time = time.time()
        
        if endpoint not in status:
            status[endpoint] = {
                "requests": 0,
                "reset_time": current_time + TWITTER_RATE_LIMITS[endpoint]["window"],
                "last_request": current_time
            }
        
        endpoint_status = status[endpoint]
        
        # Reset zamanƒ± ge√ßtiyse sƒ±fƒ±rla
        if current_time >= endpoint_status["reset_time"]:
            endpoint_status["requests"] = 0
            endpoint_status["reset_time"] = current_time + TWITTER_RATE_LIMITS[endpoint]["window"]
        
        # Limit kontrol√º
        if endpoint_status["requests"] >= TWITTER_RATE_LIMITS[endpoint]["limit"]:
            wait_time = endpoint_status["reset_time"] - current_time
            return {
                "allowed": False,
                "wait_time": wait_time,
                "requests_made": endpoint_status["requests"],
                "limit": TWITTER_RATE_LIMITS[endpoint]["limit"]
            }
        
        return {
            "allowed": True,
            "requests_made": endpoint_status["requests"],
            "limit": TWITTER_RATE_LIMITS[endpoint]["limit"],
            "reset_time": endpoint_status["reset_time"]
        }
        
    except Exception as e:
        print(f"Rate limit kontrol√º hatasƒ±: {e}")
        return {"allowed": True}  # Hata durumunda izin ver

def update_rate_limit_usage(endpoint="tweets"):
    """Rate limit kullanƒ±mƒ±nƒ± g√ºncelle"""
    try:
        status = load_rate_limit_status()
        current_time = time.time()
        
        if endpoint not in status:
            status[endpoint] = {
                "requests": 0,
                "reset_time": current_time + TWITTER_RATE_LIMITS[endpoint]["window"],
                "last_request": current_time
            }
        
        # Reset zamanƒ± ge√ßtiyse sƒ±fƒ±rla
        if current_time >= status[endpoint]["reset_time"]:
            status[endpoint]["requests"] = 0
            status[endpoint]["reset_time"] = current_time + TWITTER_RATE_LIMITS[endpoint]["window"]
        
        # Kullanƒ±mƒ± artƒ±r
        status[endpoint]["requests"] += 1
        status[endpoint]["last_request"] = current_time
        
        save_rate_limit_status(status)
        
        print(f"Rate limit g√ºncellendi - {endpoint}: {status[endpoint]['requests']}/{TWITTER_RATE_LIMITS[endpoint]['limit']}")
        
    except Exception as e:
        print(f"Rate limit g√ºncelleme hatasƒ±: {e}")

def get_rate_limit_info():
    """Rate limit bilgilerini al"""
    try:
        status = load_rate_limit_status()
        current_time = time.time()
        info = {}
        
        for endpoint in TWITTER_RATE_LIMITS:
            if endpoint in status:
                endpoint_status = status[endpoint]
                
                # Reset zamanƒ± ge√ßtiyse sƒ±fƒ±rla
                if current_time >= endpoint_status["reset_time"]:
                    requests_made = 0
                    reset_time = current_time + TWITTER_RATE_LIMITS[endpoint]["window"]
                else:
                    requests_made = endpoint_status["requests"]
                    reset_time = endpoint_status["reset_time"]
                
                info[endpoint] = {
                    "requests_made": requests_made,
                    "limit": TWITTER_RATE_LIMITS[endpoint]["limit"],
                    "remaining": TWITTER_RATE_LIMITS[endpoint]["limit"] - requests_made,
                    "reset_time": reset_time,
                    "reset_in_minutes": max(0, (reset_time - current_time) / 60)
                }
            else:
                info[endpoint] = {
                    "requests_made": 0,
                    "limit": TWITTER_RATE_LIMITS[endpoint]["limit"],
                    "remaining": TWITTER_RATE_LIMITS[endpoint]["limit"],
                    "reset_time": current_time + TWITTER_RATE_LIMITS[endpoint]["window"],
                    "reset_in_minutes": TWITTER_RATE_LIMITS[endpoint]["window"] / 60
                }
        
        return info
        
    except Exception as e:
        print(f"Rate limit bilgisi alma hatasƒ±: {e}")
        return {}

def retry_pending_tweets_after_rate_limit():
    """Rate limit sƒ±fƒ±rlandƒ±ktan sonra bekleyen tweet'leri tekrar dene"""
    try:
        # Rate limit durumunu kontrol et
        rate_check = check_rate_limit("tweets")
        if not rate_check.get("allowed", True):
            print(f"Rate limit hala aktif, {int(rate_check.get('wait_time', 0) / 60)} dakika daha beklenecek")
            return {"success": False, "message": "Rate limit hala aktif"}
        
        # Bekleyen tweet'leri y√ºkle
        pending_tweets = load_json("pending_tweets.json")
        if not pending_tweets:
            return {"success": True, "message": "Bekleyen tweet yok"}
        
        # Rate limit hatasƒ± olan tweet'leri filtrele
        rate_limited_tweets = []
        other_tweets = []
        
        for tweet in pending_tweets:
            error_reason = tweet.get('error_reason', '')
            if 'rate limit' in error_reason.lower() or '429' in error_reason:
                rate_limited_tweets.append(tweet)
            else:
                other_tweets.append(tweet)
        
        if not rate_limited_tweets:
            return {"success": True, "message": "Rate limit hatasƒ± olan tweet yok"}
        
        print(f"üîÑ {len(rate_limited_tweets)} rate limit hatasƒ± olan tweet tekrar deneniyor...")
        
        successful_posts = 0
        failed_posts = 0
        
        for tweet in rate_limited_tweets:
            try:
                # Rate limit kontrol√º yap (her tweet i√ßin)
                rate_check = check_rate_limit("tweets")
                if not rate_check.get("allowed", True):
                    print("Rate limit tekrar a≈üƒ±ldƒ±, kalan tweet'ler beklemede kalacak")
                    break
                
                # Tweet'i payla≈ü
                tweet_text = tweet['tweet_data']['tweet']
                result = post_text_tweet_v2(tweet_text)
                
                if result.get('success'):
                    # Ba≈üarƒ±lƒ± payla≈üƒ±m - posted_articles'a ekle
                    article = tweet['article']
                    article['posted_date'] = datetime.now().isoformat()
                    article['tweet_text'] = tweet_text
                    article['tweet_url'] = result.get('url', '')
                    article['manual_post'] = False
                    
                    # Posted articles'a ekle
                    posted_articles = load_json("posted_articles.json")
                    posted_articles.append(article)
                    save_json("posted_articles.json", posted_articles)
                    
                    successful_posts += 1
                    print(f"‚úÖ Tweet ba≈üarƒ±yla payla≈üƒ±ldƒ±: {article.get('title', '')[:50]}...")
                    
                else:
                    # Hala hata var - tweet'i other_tweets'e ekle
                    tweet['retry_count'] = tweet.get('retry_count', 0) + 1
                    tweet['error_reason'] = result.get('error', 'Bilinmeyen hata')
                    other_tweets.append(tweet)
                    failed_posts += 1
                    print(f"‚ùå Tweet payla≈üƒ±m hatasƒ±: {result.get('error', 'Bilinmeyen hata')}")
                    
                    # Rate limit hatasƒ± ise dur
                    if result.get('rate_limited'):
                        print("Rate limit tekrar a≈üƒ±ldƒ±, kalan tweet'ler beklemede kalacak")
                        # Kalan tweet'leri de other_tweets'e ekle
                        remaining_tweets = rate_limited_tweets[rate_limited_tweets.index(tweet) + 1:]
                        other_tweets.extend(remaining_tweets)
                        break
                
            except Exception as e:
                print(f"Tweet retry hatasƒ±: {e}")
                tweet['retry_count'] = tweet.get('retry_count', 0) + 1
                tweet['error_reason'] = str(e)
                other_tweets.append(tweet)
                failed_posts += 1
        
        # G√ºncellenmi≈ü pending tweet'leri kaydet
        save_json("pending_tweets.json", other_tweets)
        
        message = f"‚úÖ {successful_posts} tweet ba≈üarƒ±yla payla≈üƒ±ldƒ±"
        if failed_posts > 0:
            message += f", {failed_posts} tweet hala beklemede"
        
        print(f"üìä Retry tamamlandƒ±: {successful_posts} ba≈üarƒ±lƒ±, {failed_posts} ba≈üarƒ±sƒ±z")
        
        return {
            "success": True,
            "message": message,
            "successful_posts": successful_posts,
            "failed_posts": failed_posts
        }
        
    except Exception as e:
        print(f"‚ùå Retry i≈ülemi hatasƒ±: {e}")
        return {"success": False, "message": str(e)}

# ... existing code ...

def terminal_log(message, level='info'):
    """Konsol log fonksiyonu (terminal i≈ülevi kaldƒ±rƒ±ldƒ±)"""
    import time
    
    # Konsola yazdƒ±r
    level_colors = {
        'info': '\033[92m',      # Ye≈üil
        'warning': '\033[93m',   # Sarƒ±
        'error': '\033[91m',     # Kƒ±rmƒ±zƒ±
        'debug': '\033[96m',     # Cyan
        'success': '\033[92m'    # Ye≈üil
    }
    
    color = level_colors.get(level, '\033[0m')
    reset = '\033[0m'
    timestamp = time.strftime('%H:%M:%S')
    
    print(f"{color}[{timestamp}] [{level.upper()}] {message}{reset}")

def advanced_web_scraper(url, wait_time=3, use_js=False, return_html=False):
    """Geli≈ümi≈ü web scraping - MCP'ye alternatif"""
    try:
        print(f"üîç Geli≈ümi≈ü scraper ile √ßekiliyor: {url}")
        
        # Kullanƒ±labilir y√∂ntemleri kontrol et
        available_methods = []
        if REQUESTS_HTML_AVAILABLE and requests_html:
            available_methods.append("requests-html")
        if SELENIUM_AVAILABLE and webdriver:
            available_methods.append("selenium")
        available_methods.append("requests")  # Her zaman mevcut
        
        print(f"üìã Kullanƒ±labilir y√∂ntemler: {', '.join(available_methods)}")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,tr;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        }
        
        # Y√∂ntem 1: requests-html (JavaScript desteƒüi ile)
        if REQUESTS_HTML_AVAILABLE and use_js and requests_html:
            try:
                from requests_html import HTMLSession  # type: ignore
                session = HTMLSession()
                
                print("üöÄ requests-html ile JavaScript render ediliyor...")
                r = session.get(url, headers=headers, timeout=30)
                r.html.render(wait=wait_time, timeout=20)
                
                content = r.html.html
                soup = BeautifulSoup(content, 'html.parser')
                
                print(f"‚úÖ requests-html ba≈üarƒ±lƒ±: {len(content)} karakter")
                return {
                    "success": True,
                    "content": extract_main_content(soup),
                    "html": content,
                    "method": "requests-html"
                }
                
            except Exception as rh_error:
                print(f"‚ö†Ô∏è requests-html hatasƒ±: {rh_error}")
        
        # Y√∂ntem 2: Selenium (headless Chrome)
        if SELENIUM_AVAILABLE and use_js and webdriver and Options:
            try:
                print("üöÄ Selenium ile JavaScript render ediliyor...")
                
                chrome_options = Options()
                chrome_options.add_argument('--headless')
                chrome_options.add_argument('--no-sandbox')
                chrome_options.add_argument('--disable-dev-shm-usage')
                chrome_options.add_argument('--disable-gpu')
                chrome_options.add_argument('--window-size=1920,1080')
                chrome_options.add_argument(f'--user-agent={headers["User-Agent"]}')
                
                driver = webdriver.Chrome(options=chrome_options)
                driver.set_page_load_timeout(30)
                
                driver.get(url)
                
                # Sayfanƒ±n y√ºklenmesini bekle
                if WebDriverWait and EC and By:
                    WebDriverWait(driver, wait_time).until(
                        EC.presence_of_element_located((By.TAG_NAME, "body"))
                    )
                else:
                    time.sleep(wait_time)
                
                # Biraz daha bekle (dinamik i√ßerik i√ßin)
                time.sleep(wait_time)
                
                content = driver.page_source
                driver.quit()
                
                soup = BeautifulSoup(content, 'html.parser')
                
                print(f"‚úÖ Selenium ba≈üarƒ±lƒ±: {len(content)} karakter")
                return {
                    "success": True,
                    "content": extract_main_content(soup),
                    "html": content,
                    "method": "selenium"
                }
                
            except Exception as selenium_error:
                print(f"‚ö†Ô∏è Selenium hatasƒ±: {selenium_error}")
                try:
                    driver.quit()
                except:
                    pass
        
        # Y√∂ntem 3: Basit requests (fallback)
        print("üîÑ Basit HTTP request ile deneniyor...")
        
        session = requests.Session()
        session.headers.update(headers)
        
        response = session.get(url, timeout=30, allow_redirects=True)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        print(f"‚úÖ Basit request ba≈üarƒ±lƒ±: {len(response.text)} karakter")
        return {
            "success": True,
            "content": extract_main_content(soup),
            "html": response.text,
            "method": "requests"
        }
        
    except Exception as e:
        print(f"‚ùå Geli≈ümi≈ü scraper hatasƒ±: {e}")
        return {"success": False, "error": str(e)}

def extract_main_content(soup):
    """Sayfadan ana i√ßeriƒüi √ßƒ±kar"""
    try:
        # Ana i√ßerik selector'larƒ± (√∂ncelik sƒ±rasƒ±na g√∂re)
        main_content_selectors = [
            'article',
            '[role="main"]',
            'main',
            '.post-content',
            '.entry-content',
            '.article-content',
            '.content',
            '.story-body',
            '.article-body',
            '#content',
            '.main-content'
        ]
        
        content_text = ""
        
        for selector in main_content_selectors:
            elements = soup.select(selector)
            if elements:
                element = elements[0]
                
                # Gereksiz elementleri kaldƒ±r
                for unwanted in element(["script", "style", "nav", "footer", "header", "aside", ".advertisement", ".ads", ".social-share"]):
                    unwanted.decompose()
                
                content_text = element.get_text(strip=True)
                if len(content_text) > 200:  # Yeterli i√ßerik varsa
                    break
        
        # Eƒüer ana i√ßerik bulunamazsa body'den al
        if not content_text or len(content_text) < 200:
            body = soup.find('body')
            if body:
                # Gereksiz elementleri kaldƒ±r
                for unwanted in body(["script", "style", "nav", "footer", "header", "aside", ".advertisement", ".ads", ".social-share", ".menu", ".sidebar"]):
                    unwanted.decompose()
                
                content_text = body.get_text(strip=True)
        
        # ƒ∞√ßeriƒüi temizle
        content_text = ' '.join(content_text.split())
        # content_text = content_text[:3000]  # ƒ∞lk 3000 karakter sƒ±nƒ±rƒ± kaldƒ±rƒ±ldƒ±
        
        return content_text
        
    except Exception as e:
        print(f"‚ùå ƒ∞√ßerik √ßƒ±karma hatasƒ±: {e}")
        return ""

# ==========================================
# PYTHONANYWHERE UYUMLU HABER √áEKME Sƒ∞STEMƒ∞
# ==========================================

def fetch_latest_ai_articles_pythonanywhere():
    """PythonAnywhere i√ßin optimize edilmi≈ü haber √ßekme sistemi - √ñzel kaynaklar + API'ler"""
    try:
        # √ñnce mevcut yayƒ±nlanan makaleleri y√ºkle
        posted_articles = load_json(HISTORY_FILE)
        posted_urls = [article.get('url', '') for article in posted_articles]
        posted_hashes = [article.get('hash', '') for article in posted_articles]
        
        print("üîç PythonAnywhere uyumlu haber √ßekme sistemi ba≈ülatƒ±lƒ±yor...")
        
        all_articles = []
        
        # 1. √ñnce √∂zel haber kaynaklarƒ±ndan √ßek (news_sources.json)
        try:
            custom_articles = fetch_articles_from_custom_sources_pythonanywhere()
            if custom_articles:
                all_articles.extend(custom_articles)
                print(f"‚úÖ √ñzel kaynaklardan {len(custom_articles)} makale bulundu")
        except Exception as custom_error:
            print(f"‚ö†Ô∏è √ñzel kaynaklar hatasƒ±: {custom_error}")
        
        # 2. RSS Feed'lerden makale √ßek (sadece √∂zel kaynaklarda RSS yoksa)
        try:
            rss_articles = fetch_articles_from_rss_feeds()
            if rss_articles:
                all_articles.extend(rss_articles)
                print(f"‚úÖ RSS'den {len(rss_articles)} makale bulundu")
        except Exception as rss_error:
            print(f"‚ö†Ô∏è RSS √ßekme hatasƒ±: {rss_error}")
        
        # 3. Hacker News API'den AI ile ilgili haberleri √ßek
        try:
            hn_articles = fetch_articles_from_hackernews()
            if hn_articles:
                all_articles.extend(hn_articles)
                print(f"‚úÖ Hacker News'den {len(hn_articles)} makale bulundu")
        except Exception as hn_error:
            print(f"‚ö†Ô∏è Hacker News hatasƒ±: {hn_error}")
        
        # 4. Reddit API'den AI subreddit'lerinden makale √ßek
        try:
            reddit_articles = fetch_articles_from_reddit()
            if reddit_articles:
                all_articles.extend(reddit_articles)
                print(f"‚úÖ Reddit'den {len(reddit_articles)} makale bulundu")
        except Exception as reddit_error:
            print(f"‚ö†Ô∏è Reddit hatasƒ±: {reddit_error}")
        
        # Duplikat filtreleme
        unique_articles = []
        seen_hashes = set()
        seen_urls = set()
        
        for article in all_articles:
            article_hash = article.get('hash', '')
            article_url = article.get('url', '')
            
            # Zaten payla≈üƒ±lmƒ±≈ü mƒ± kontrol et
            if (article_hash not in posted_hashes and 
                article_url not in posted_urls and
                article_hash not in seen_hashes and
                article_url not in seen_urls):
                
                unique_articles.append(article)
                seen_hashes.add(article_hash)
                seen_urls.add(article_url)
        
        # En yeni makaleleri √∂nce getir
        unique_articles.sort(key=lambda x: x.get('fetch_date', ''), reverse=True)
        
        print(f"üìä PythonAnywhere sistemi ile toplam {len(unique_articles)} benzersiz makale bulundu")
        
        return unique_articles[:10]  # En fazla 10 makale d√∂nd√ºr
        
    except Exception as e:
        print(f"‚ùå PythonAnywhere haber √ßekme hatasƒ±: {e}")
        return []

def fetch_articles_from_custom_sources_pythonanywhere():
    """PythonAnywhere uyumlu √∂zel haber kaynaklarƒ± √ßekme"""
    try:
        config = load_news_sources()
        all_articles = []
        
        enabled_sources = [s for s in config["sources"] if s.get("enabled", True)]
        
        if not enabled_sources:
            print("‚ö†Ô∏è Aktif haber kaynaƒüƒ± bulunamadƒ±")
            return []
        
        print(f"üîç {len(enabled_sources)} √∂zel haber kaynaƒüƒ±ndan makale √ßekiliyor (PythonAnywhere uyumlu)...")
        
        for source in enabled_sources:
            try:
                print(f"üì∞ {source['name']} kaynaƒüƒ± kontrol ediliyor...")
                
                # PythonAnywhere uyumlu basit scraping kullan
                articles = fetch_articles_from_single_source_pythonanywhere(source)
                
                if articles:
                    all_articles.extend(articles)
                    source["article_count"] = len(articles)
                    source["success_rate"] = min(100, source.get("success_rate", 0) + 10)
                    print(f"‚úÖ {source['name']}: {len(articles)} makale bulundu")
                else:
                    source["success_rate"] = max(0, source.get("success_rate", 100) - 20)
                    print(f"‚ö†Ô∏è {source['name']}: Makale bulunamadƒ±")
                
                source["last_checked"] = datetime.now().isoformat()
                
            except Exception as e:
                print(f"‚ùå {source['name']} hatasƒ±: {e}")
                source["success_rate"] = max(0, source.get("success_rate", 100) - 30)
                source["last_checked"] = datetime.now().isoformat()
        
        # G√ºncellenmi≈ü config'i kaydet
        try:
            save_json(NEWS_SOURCES_FILE, config)
        except Exception as save_error:
            print(f"‚ö†Ô∏è Haber kaynaklarƒ± kaydetme hatasƒ±: {save_error}")
        
        print(f"üìä √ñzel kaynaklardan toplam {len(all_articles)} makale bulundu")
        return all_articles
        
    except Exception as e:
        print(f"‚ùå √ñzel kaynaklar genel hatasƒ±: {e}")
        return []

def fetch_articles_from_single_source_pythonanywhere(source):
    """PythonAnywhere uyumlu tek kaynak makale √ßekme"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        # Timeout ile g√ºvenli istek
        response = requests.get(source['url'], headers=headers, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Makale container'larƒ±nƒ± bul
        selectors = source.get('article_selectors', {})
        container_selector = selectors.get('container', 'article')
        
        articles = soup.select(container_selector)[:5]  # En fazla 5 makale
        
        if not articles:
            # Fallback selector'larƒ± dene
            fallback_selectors = ['article', '.post', '.news-item', '.article-item', 'li']
            for fallback in fallback_selectors:
                articles = soup.select(fallback)[:5]
                if articles:
                    break
        
        parsed_articles = []
        
        for article in articles:
            try:
                # Ba≈ülƒ±k bul
                title_selector = selectors.get('title', 'h1, h2, h3, .title')
                title_elem = article.select_one(title_selector)
                
                if not title_elem:
                    continue
                
                title = title_elem.get_text().strip()
                
                # Link bul
                link_selector = selectors.get('link', 'a')
                link_elem = article.select_one(link_selector)
                
                if not link_elem:
                    link_elem = title_elem.find('a') or title_elem.find_parent('a')
                
                if not link_elem:
                    continue
                
                link = link_elem.get('href', '')
                
                # TechCrunch i√ßin √∂zel d√ºzeltme
                if 'techcrunch.com' in source['url'] and not link:
                    # Ba≈ülƒ±k i√ßindeki a tag'ƒ±nƒ± bul
                    title_link = title_elem.find('a')
                    if title_link:
                        link = title_link.get('href', '')
                
                # Relative URL'leri absolute yap
                if link.startswith('/'):
                    from urllib.parse import urljoin
                    link = urljoin(source['url'], link)
                elif not link.startswith('http'):
                    continue
                
                # AI ile ilgili mi kontrol et
                title_lower = title.lower()
                ai_keywords = ['ai', 'artificial intelligence', 'machine learning', 'deep learning', 'neural', 'gpt', 'llm', 'chatbot', 'automation', 'anthropic', 'openai', 'claude', 'gemini', 'copilot']
                is_ai_related = any(keyword in title_lower for keyword in ai_keywords)
                
                if not is_ai_related:
                    continue
                
                # √ñzet bul
                excerpt_selector = selectors.get('excerpt', '.excerpt, .summary, p')
                excerpt_elem = article.select_one(excerpt_selector)
                excerpt = excerpt_elem.get_text().strip()[:500] if excerpt_elem else ""
                
                # Tarih bul
                date_selector = selectors.get('date', 'time, .date, .published')
                date_elem = article.select_one(date_selector)
                date_str = date_elem.get_text().strip() if date_elem else ""
                
                # Hash olu≈ütur
                article_hash = hashlib.md5(title.encode()).hexdigest()
                
                parsed_articles.append({
                    "title": title,
                    "url": link,
                    "content": excerpt or title,
                    "excerpt": excerpt,
                    "date": date_str,
                    "hash": article_hash,
                    "source": f"{source['name']} (PA)",
                    "source_id": source["id"],
                    "fetch_date": datetime.now().isoformat(),
                    "is_new": True,
                    "already_posted": False
                })
                
            except Exception as article_error:
                print(f"‚ö†Ô∏è Makale parse hatasƒ±: {article_error}")
                continue
        
        return parsed_articles
        
    except Exception as e:
        print(f"‚ùå {source.get('name', 'Bilinmeyen')} kaynak hatasƒ±: {e}")
        return []

def fetch_articles_with_rss_only():
    """Sadece RSS y√∂ntemi ile haber kaynaklarƒ±ndan makale √ßekme - Son 24 saat filtreli"""
    try:
        print("üîç RSS y√∂ntemi ile haber √ßekme ba≈ülatƒ±lƒ±yor (Son 24 saat)...")
        
        # Bug√ºn√ºn tarih ve saatini al
        now = datetime.now()
        twenty_four_hours_ago = now - timedelta(hours=24)
        
        print(f"üìÖ Bug√ºn: {now.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"‚è∞ 24 saat √∂ncesi: {twenty_four_hours_ago.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # √ñnce mevcut yayƒ±nlanan makaleleri y√ºkle
        posted_articles = load_json(HISTORY_FILE)
        posted_urls = [article.get('url', '') for article in posted_articles]
        posted_hashes = [article.get('hash', '') for article in posted_articles]
        
        # Son 24 saat i√ßinde payla≈üƒ±lan makaleleri de kontrol et
        recent_posted_urls = []
        recent_posted_hashes = []
        
        for article in posted_articles:
            posted_date_str = article.get('posted_date') or article.get('fetch_date')
            if posted_date_str:
                try:
                    posted_date = datetime.fromisoformat(posted_date_str.replace('Z', '+00:00').replace('+00:00', ''))
                    if posted_date >= twenty_four_hours_ago:
                        recent_posted_urls.append(article.get('url', ''))
                        recent_posted_hashes.append(article.get('hash', ''))
                except Exception as date_error:
                    print(f"‚ö†Ô∏è Tarih parse hatasƒ±: {date_error}")
                    continue
        
        print(f"üìä Son 24 saatte payla≈üƒ±lan makale sayƒ±sƒ±: {len(recent_posted_urls)}")
        
        # RSS kaynaklarƒ±nƒ± y√ºkle
        config = load_news_sources()
        rss_sources = config.get("rss_sources", [])
        enabled_rss_sources = [s for s in rss_sources if s.get("enabled", True)]
        
        if not enabled_rss_sources:
            print("‚ö†Ô∏è Aktif RSS kaynaƒüƒ± bulunamadƒ±")
            return []
        
        print(f"üì∞ {len(enabled_rss_sources)} RSS kaynaƒüƒ±ndan makale √ßekiliyor...")
        
        all_articles = []
        
        for rss_source in enabled_rss_sources:
            try:
                print(f"üîç RSS √ßekiliyor: {rss_source['name']}")
                
                # RSS feed'i parse et
                import feedparser
                feed = feedparser.parse(rss_source['url'])
                
                if not feed.entries:
                    print(f"‚ö†Ô∏è {rss_source['name']}: RSS feed'de entry bulunamadƒ±")
                    rss_source["success_rate"] = max(0, rss_source.get("success_rate", 100) - 20)
                    continue
                
                source_articles = []
                
                for entry in feed.entries[:10]:  # Her feed'den en fazla 10 makale kontrol et
                    try:
                        title = getattr(entry, 'title', '')
                        url = getattr(entry, 'link', '')
                        
                        if not title or not url:
                            continue
                        
                        # URL kontrol√º
                        if url in posted_urls or url in recent_posted_urls:
                            print(f"‚úÖ RSS makale zaten payla≈üƒ±lmƒ±≈ü: {title[:50]}...")
                            continue
                        
                        # Tarih kontrol√º
                        entry_date = None
                        date_str = ""
                        
                        # RSS entry'den tarih al
                        if hasattr(entry, 'published_parsed') and entry.published_parsed:
                            try:
                                import time
                                entry_date = datetime(*entry.published_parsed[:6])
                                date_str = entry.published
                            except:
                                pass
                        elif hasattr(entry, 'published'):
                            try:
                                # RFC 2822 formatƒ±nƒ± parse et
                                from email.utils import parsedate_to_datetime
                                entry_date = parsedate_to_datetime(entry.published)
                                date_str = entry.published
                            except:
                                pass
                        
                        # 24 saat kontrol√º
                        if entry_date:
                            if entry_date < twenty_four_hours_ago:
                                print(f"‚è∞ RSS makale 24 saatten eski: {entry_date.strftime('%Y-%m-%d %H:%M')} - {title[:50]}...")
                            continue
                        
                        # ƒ∞√ßerik al
                        content = ""
                        if hasattr(entry, 'summary'):
                            content = entry.summary
                        elif hasattr(entry, 'description'):
                            content = entry.description
                        elif hasattr(entry, 'content'):
                            if isinstance(entry.content, list) and len(entry.content) > 0:
                                content = entry.content[0].value
                            else:
                                content = str(entry.content)
                        
                        # HTML etiketlerini temizle
                        if content:
                            from bs4 import BeautifulSoup
                            content = BeautifulSoup(content, 'html.parser').get_text()
                            content = ' '.join(content.split())[:2000]
                        
                        # Eƒüer i√ßerik yoksa ba≈ülƒ±ƒüƒ± kullan
                        if not content:
                            content = title
                        
                        # Hash olu≈ütur
                        article_hash = hashlib.md5(title.encode()).hexdigest()
                        
                        # Tekrar kontrol√º
                        if (article_hash not in posted_hashes and 
                            article_hash not in recent_posted_hashes):
                            
                            source_articles.append({
                            "title": title,
                            "url": url,
                                "content": content,
                            "hash": article_hash,
                            "fetch_date": datetime.now().isoformat(),
                            "is_new": True,
                                "already_posted": False,
                                "source": f"RSS - {rss_source['name']}",
                                "source_id": rss_source["id"],
                                "article_date": entry_date.isoformat() if entry_date else datetime.now().isoformat(),
                                "is_within_24h": True,
                                "rss_published": date_str
                            })
                            print(f"üÜï RSS ile yeni makale (24h i√ßinde): {title[:50]}...")
                        else:
                            if article_hash in recent_posted_hashes:
                                print(f"‚è∞ Son 24 saatte payla≈üƒ±lmƒ±≈ü: {title[:50]}...")
                            else:
                                print(f"‚úÖ Makale zaten payla≈üƒ±lmƒ±≈ü: {title[:50]}...")
                        
                        # En fazla 5 makale al
                        if len(source_articles) >= 5:
                            break
                        
                    except Exception as entry_error:
                        print(f"‚ö†Ô∏è RSS entry hatasƒ±: {entry_error}")
                        continue
                        
                if source_articles:
                    all_articles.extend(source_articles)
                    rss_source["article_count"] = len(source_articles)
                    rss_source["success_rate"] = min(100, rss_source.get("success_rate", 0) + 10)
                    print(f"‚úÖ {rss_source['name']}: {len(source_articles)} yeni makale bulundu")
                else:
                    rss_source["success_rate"] = max(0, rss_source.get("success_rate", 100) - 10)
                    print(f"‚ö†Ô∏è {rss_source['name']}: Yeni makale bulunamadƒ±")
                
                rss_source["last_checked"] = datetime.now().isoformat()
                
            except Exception as source_error:
                print(f"‚ùå {rss_source['name']} RSS hatasƒ±: {source_error}")
                rss_source["success_rate"] = max(0, rss_source.get("success_rate", 100) - 30)
                rss_source["last_checked"] = datetime.now().isoformat()
        
        # G√ºncellenmi≈ü config'i kaydet
        try:
            save_json(NEWS_SOURCES_FILE, config)
        except Exception as save_error:
            print(f"‚ö†Ô∏è RSS kaynaklarƒ± kaydetme hatasƒ±: {save_error}")
        
        print(f"üìä RSS ile toplam {len(all_articles)} yeni makale bulundu (Son 24 saat filtreli)")
        
        # Duplikat filtreleme uygula
        if all_articles:
            all_articles = filter_duplicate_articles(all_articles)
            print(f"üîÑ Duplikat filtreleme sonrasƒ±: {len(all_articles)} benzersiz makale")
        
        # 24 saat i√ßindeki makaleleri i≈üaretle
        for article in all_articles:
            article['filtered_by_24h'] = True
            article['filter_applied_at'] = datetime.now().isoformat()
            article['method'] = 'rss'
        
        return all_articles
        
    except ImportError:
        print("‚ö†Ô∏è feedparser mod√ºl√º bulunamadƒ±, RSS atlanƒ±yor")
        return []
    except Exception as e:
        print(f"‚ùå RSS haber √ßekme genel hatasƒ±: {e}")
        return []

def fetch_articles_hybrid_mcp_rss():
    """Hibrit sistem: MCP + RSS fallback ile haber √ßekme"""
    try:
        print("üîÑ Hibrit haber √ßekme sistemi ba≈ülatƒ±lƒ±yor (MCP + RSS)...")
        
        all_articles = []
        
        # 1. √ñnce MCP ile dene
        try:
            print("ü§ñ MCP y√∂ntemi deneniyor...")
            mcp_articles = fetch_articles_with_mcp_only()
            
            if mcp_articles:
                all_articles.extend(mcp_articles)
                print(f"‚úÖ MCP ile {len(mcp_articles)} makale bulundu")
            else:
                print("‚ö†Ô∏è MCP ile makale bulunamadƒ±")
                
        except Exception as mcp_error:
            print(f"‚ùå MCP hatasƒ±: {mcp_error}")
        
        # 2. Eƒüer MCP'den yeterli makale gelmezse RSS dene
        if len(all_articles) < 3:  # 3'ten az makale varsa RSS'yi de dene
            try:
                print("üì° RSS y√∂ntemi devreye giriyor...")
                rss_articles = fetch_articles_with_rss_only()
                
                if rss_articles:
                    # RSS makalelerini ekle (duplikat kontrol√º ile)
                    existing_urls = [article.get('url', '') for article in all_articles]
                    existing_hashes = [article.get('hash', '') for article in all_articles]
                    
                    new_rss_articles = []
                    for rss_article in rss_articles:
                        if (rss_article.get('url', '') not in existing_urls and 
                            rss_article.get('hash', '') not in existing_hashes):
                            new_rss_articles.append(rss_article)
                    
                    all_articles.extend(new_rss_articles)
                    print(f"‚úÖ RSS ile {len(new_rss_articles)} ek makale bulundu")
                else:
                    print("‚ö†Ô∏è RSS ile de makale bulunamadƒ±")
                    
            except Exception as rss_error:
                print(f"‚ùå RSS hatasƒ±: {rss_error}")
        else:
            print(f"‚úÖ MCP'den yeterli makale var ({len(all_articles)}), RSS atlanƒ±yor")
        
        # 3. Sonu√ßlarƒ± birle≈ütir ve filtrele
        if all_articles:
            # Duplikat filtreleme
            all_articles = filter_duplicate_articles(all_articles)
            
            # Hibrit i≈üareti ekle
            for article in all_articles:
                article['hybrid_method'] = True
                article['methods_used'] = 'MCP+RSS' if any('RSS' in a.get('source', '') for a in all_articles) else 'MCP'
            
            print(f"üéØ Hibrit sistem sonucu: {len(all_articles)} benzersiz makale")
            
            # Kaynak daƒüƒ±lƒ±mƒ±nƒ± g√∂ster
            mcp_count = len([a for a in all_articles if 'MCP' in a.get('source', '')])
            rss_count = len([a for a in all_articles if 'RSS' in a.get('source', '')])
            print(f"üìä Kaynak daƒüƒ±lƒ±mƒ±: MCP={mcp_count}, RSS={rss_count}")
            
        return all_articles
        
    except Exception as e:
        print(f"‚ùå Hibrit sistem hatasƒ±: {e}")
        return []

def fetch_articles_from_rss_feeds():
    """Eski RSS fonksiyonu - geriye uyumluluk i√ßin"""
    return fetch_articles_with_rss_only()

def fetch_articles_with_simple_scraping():
    """Basit web scraping ile makale √ßek"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        # Scraping hedefleri
        scraping_targets = [
            {
                "url": "https://techcrunch.com/category/artificial-intelligence/",
                "name": "TechCrunch AI",
                "article_selector": "article.post-block",
                "title_selector": "h2.post-block__title a",
                "link_selector": "h2.post-block__title a",
                "excerpt_selector": ".post-block__content"
            },
            {
                "url": "https://www.theverge.com/ai-artificial-intelligence",
                "name": "The Verge AI",
                "article_selector": "article",
                "title_selector": "h2 a",
                "link_selector": "h2 a",
                "excerpt_selector": "p"
            }
        ]
        
        all_articles = []
        
        for target in scraping_targets:
            try:
                print(f"üîç Web scraping: {target['name']}")
                
                response = requests.get(target['url'], headers=headers, timeout=15)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'html.parser')
                articles = soup.select(target['article_selector'])[:5]
                
                for article in articles:
                    try:
                        title_elem = article.select_one(target['title_selector'])
                        if not title_elem:
                            continue
                        
                        title = title_elem.get_text().strip()
                        link = title_elem.get('href', '')
                        
                        # Relative URL'leri absolute yap
                        if link.startswith('/'):
                            from urllib.parse import urljoin
                            link = urljoin(target['url'], link)
                        
                        # Excerpt al
                        excerpt = ""
                        excerpt_elem = article.select_one(target['excerpt_selector'])
                        if excerpt_elem:
                            excerpt = excerpt_elem.get_text().strip()[:500]
                        
                        # Hash olu≈ütur
                        article_hash = hashlib.md5(title.encode()).hexdigest()
                        
                        all_articles.append({
                            "title": title,
                            "url": link,
                            "content": excerpt or title,
                            "excerpt": excerpt,
                            "hash": article_hash,
                            "source": f"Scraping - {target['name']}",
                            "fetch_date": datetime.now().isoformat(),
                            "is_new": True,
                            "already_posted": False
                        })
                        
                    except Exception as article_error:
                        print(f"‚ö†Ô∏è Makale parse hatasƒ±: {article_error}")
                        continue
                        
            except Exception as target_error:
                print(f"‚ö†Ô∏è Scraping hatasƒ± ({target['name']}): {target_error}")
                continue
        
        return all_articles
        
    except Exception as e:
        print(f"‚ùå Web scraping genel hatasƒ±: {e}")
        return []

def fetch_articles_from_hackernews():
    """Hacker News API'den AI ile ilgili haberleri √ßek - Geli≈ümi≈ü i√ßerik √ßekme ile"""
    try:
        # Hacker News API'den top stories al
        top_stories_url = "https://hacker-news.firebaseio.com/v0/topstories.json"
        response = requests.get(top_stories_url, timeout=10)
        story_ids = response.json()[:50]  # ƒ∞lk 50 hikaye
        
        ai_articles = []
        ai_keywords = ['ai', 'artificial intelligence', 'machine learning', 'deep learning', 'neural', 'gpt', 'llm', 'openai', 'anthropic', 'claude', 'chatgpt']
        
        for story_id in story_ids[:20]:  # ƒ∞lk 20'sini kontrol et
            try:
                story_url = f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
                story_response = requests.get(story_url, timeout=5)
                story = story_response.json()
                
                if not story or story.get('type') != 'story':
                    continue
                
                title = story.get('title', '')
                url = story.get('url', '')
                
                # AI ile ilgili mi kontrol et - Daha sƒ±kƒ± filtreleme
                title_lower = title.lower()
                is_ai_related = any(keyword in title_lower for keyword in ai_keywords)
                
                # AI ile ilgili olmayan konularƒ± filtrele
                non_ai_keywords = ['wood', 'dried', 'kiln', 'furniture', 'cooking', 'recipe', 'travel', 'music', 'art', 'painting', 'photography']
                has_non_ai_content = any(keyword in title_lower for keyword in non_ai_keywords)
                
                if not is_ai_related or not url or has_non_ai_content:
                    if has_non_ai_content:
                        terminal_log(f"‚ö†Ô∏è AI olmayan i√ßerik filtrelendi: {title[:50]}...", "warning")
                    continue
                
                # Ger√ßek makale i√ßeriƒüini √ßekmeye √ßalƒ±≈ü
                article_content = title  # Fallback olarak ba≈ülƒ±k
                
                try:
                    # Geli≈ümi≈ü scraper ile i√ßerik √ßek
                    content_result = advanced_web_scraper(url, wait_time=2, use_js=False)
                    
                    if content_result.get("success") and content_result.get("content"):
                        scraped_content = content_result["content"]
                        
                        # ƒ∞√ßeriƒüi temizle ve kƒ±salt
                        if len(scraped_content) > 500:
                            article_content = scraped_content[:500] + "..."
                        else:
                            article_content = scraped_content
                        
                        terminal_log(f"‚úÖ HN makale i√ßeriƒüi √ßekildi: {title[:50]}... ({len(scraped_content)} karakter)", "success")
                    else:
                        # MCP fallback dene
                        try:
                            mcp_result = mcp_firecrawl_scrape({
                                "url": url,
                                "formats": ["markdown"],
                                "onlyMainContent": True,
                                "waitFor": 1000
                            })
                            
                            if mcp_result.get("success") and mcp_result.get("content"):
                                mcp_content = mcp_result["content"]
                                if len(mcp_content) > 500:
                                    article_content = mcp_content[:500] + "..."
                                else:
                                    article_content = mcp_content
                                
                                terminal_log(f"‚úÖ HN makale i√ßeriƒüi MCP ile √ßekildi: {title[:50]}...", "success")
                            else:
                                terminal_log(f"‚ö†Ô∏è HN makale i√ßeriƒüi √ßekilemedi, ba≈ülƒ±k kullanƒ±lƒ±yor: {title[:50]}...", "warning")
                                
                        except Exception as mcp_error:
                            terminal_log(f"‚ö†Ô∏è HN MCP fallback hatasƒ±: {mcp_error}", "warning")
                            
                except Exception as content_error:
                    terminal_log(f"‚ö†Ô∏è HN i√ßerik √ßekme hatasƒ±: {content_error}", "warning")
                
                # Hash olu≈ütur
                article_hash = hashlib.md5(title.encode()).hexdigest()
                
                ai_articles.append({
                    "title": title,
                    "url": url,
                    "content": article_content,  # Ger√ßek i√ßerik veya ba≈ülƒ±k
                    "score": story.get('score', 0),
                    "hash": article_hash,
                    "source": "Hacker News",
                    "fetch_date": datetime.now().isoformat(),
                    "is_new": True,
                    "already_posted": False
                })
                
                if len(ai_articles) >= 5:  # En fazla 5 makale
                    break
                    
            except Exception as story_error:
                terminal_log(f"‚ö†Ô∏è HN story hatasƒ±: {story_error}", "warning")
                continue
        
        terminal_log(f"üìä Hacker News'den {len(ai_articles)} AI makalesi bulundu", "info")
        return ai_articles
        
    except Exception as e:
        terminal_log(f"‚ùå Hacker News API hatasƒ±: {e}", "error")
        return []

def fetch_articles_from_reddit():
    """Reddit'den AI subreddit'lerinden makale √ßek - Geli≈ümi≈ü i√ßerik √ßekme ile"""
    try:
        # Reddit JSON API kullan (auth gerektirmez)
        subreddits = ['artificial', 'MachineLearning', 'deeplearning', 'singularity']
        all_articles = []
        
        for subreddit in subreddits:
            try:
                url = f"https://www.reddit.com/r/{subreddit}/hot.json?limit=10"
                headers = {'User-Agent': 'AI News Bot 1.0'}
                
                response = requests.get(url, headers=headers, timeout=10)
                data = response.json()
                
                posts = data.get('data', {}).get('children', [])
                
                for post in posts[:3]:  # Her subreddit'den 3 post
                    try:
                        post_data = post.get('data', {})
                        
                        title = post_data.get('title', '')
                        url = post_data.get('url', '')
                        selftext = post_data.get('selftext', '')
                        score = post_data.get('score', 0)
                        
                        # Sadece external link'leri al (reddit post'larƒ± deƒüil)
                        if not url or 'reddit.com' in url or score < 10:
                            continue
                        
                        # ƒ∞√ßerik olu≈ütur - √ñnce selftext, sonra ger√ßek makale i√ßeriƒüi
                        article_content = selftext[:500] if selftext else title
                        
                        # Eƒüer selftext yoksa veya √ßok kƒ±saysa, ger√ßek makale i√ßeriƒüini √ßekmeye √ßalƒ±≈ü
                        if not selftext or len(selftext) < 100:
                            try:
                                # Geli≈ümi≈ü scraper ile i√ßerik √ßek
                                content_result = advanced_web_scraper(url, wait_time=2, use_js=False)
                                
                                if content_result.get("success") and content_result.get("content"):
                                    scraped_content = content_result["content"]
                                    
                                    # ƒ∞√ßeriƒüi temizle ve kƒ±salt
                                    if len(scraped_content) > 500:
                                        article_content = scraped_content[:500] + "..."
                                    else:
                                        article_content = scraped_content
                                    
                                    terminal_log(f"‚úÖ Reddit makale i√ßeriƒüi √ßekildi: {title[:50]}... ({len(scraped_content)} karakter)", "success")
                                else:
                                    # MCP fallback dene
                                    try:
                                        mcp_result = mcp_firecrawl_scrape({
                                            "url": url,
                                            "formats": ["markdown"],
                                            "onlyMainContent": True,
                                            "waitFor": 1000
                                        })
                                        
                                        if mcp_result.get("success") and mcp_result.get("content"):
                                            mcp_content = mcp_result["content"]
                                            if len(mcp_content) > 500:
                                                article_content = mcp_content[:500] + "..."
                                            else:
                                                article_content = mcp_content
                                            
                                            terminal_log(f"‚úÖ Reddit makale i√ßeriƒüi MCP ile √ßekildi: {title[:50]}...", "success")
                                        else:
                                            terminal_log(f"‚ö†Ô∏è Reddit makale i√ßeriƒüi √ßekilemedi, ba≈ülƒ±k kullanƒ±lƒ±yor: {title[:50]}...", "warning")
                                            
                                    except Exception as mcp_error:
                                        terminal_log(f"‚ö†Ô∏è Reddit MCP fallback hatasƒ±: {mcp_error}", "warning")
                                        
                            except Exception as content_error:
                                terminal_log(f"‚ö†Ô∏è Reddit i√ßerik √ßekme hatasƒ±: {content_error}", "warning")
                        
                        # Hash olu≈ütur
                        article_hash = hashlib.md5(title.encode()).hexdigest()
                        
                        all_articles.append({
                            "title": title,
                            "url": url,
                            "content": article_content,
                            "score": score,
                            "hash": article_hash,
                            "source": f"Reddit - r/{subreddit}",
                            "fetch_date": datetime.now().isoformat(),
                            "is_new": True,
                            "already_posted": False
                        })
                        
                    except Exception as post_error:
                        terminal_log(f"‚ö†Ô∏è Reddit post hatasƒ±: {post_error}", "warning")
                        continue
                        
            except Exception as subreddit_error:
                terminal_log(f"‚ö†Ô∏è Reddit subreddit hatasƒ± ({subreddit}): {subreddit_error}", "warning")
                continue
        
        terminal_log(f"üìä Reddit'den {len(all_articles)} AI makalesi bulundu", "info")
        return all_articles
        
    except Exception as e:
        terminal_log(f"‚ùå Reddit API hatasƒ±: {e}", "error")
        return []

# ==========================================
# HABER √áEKME Y√ñNTEMƒ∞ SE√áƒ∞Cƒ∞
# ==========================================

def get_news_fetching_method():
    """Ayarlardan haber √ßekme y√∂ntemini al"""
    try:
        # Automation settings'den kontrol et
        settings = load_automation_settings()
        method = settings.get('news_fetching_method', 'auto')
        
        # MCP config'den de kontrol et
        mcp_config = load_json(MCP_CONFIG_FILE) if os.path.exists(MCP_CONFIG_FILE) else {}
        mcp_enabled = mcp_config.get('mcp_enabled', False)
        
        return {
            'method': method,
            'mcp_enabled': mcp_enabled,
            'available_methods': ['auto', 'mcp_only', 'pythonanywhere_only', 'custom_sources_only']
        }
    except Exception as e:
        print(f"Haber √ßekme y√∂ntemi alma hatasƒ±: {e}")
        return {
            'method': 'auto',
            'mcp_enabled': False,
            'available_methods': ['auto', 'mcp_only', 'pythonanywhere_only', 'custom_sources_only']
        }

def fetch_latest_ai_articles_smart():
    """Akƒ±llƒ± haber √ßekme - Ayarlara g√∂re y√∂ntem se√ßer"""
    try:
        method_info = get_news_fetching_method()
        method = method_info['method']
        mcp_enabled = method_info['mcp_enabled']
        
        print(f"üéØ Haber √ßekme y√∂ntemi: {method} (MCP: {'Aktif' if mcp_enabled else 'Pasif'})")
        
        if method == 'mcp_only' and mcp_enabled:
            # Sadece MCP kullan
            return fetch_latest_ai_articles_with_firecrawl()
            
        elif method == 'pythonanywhere_only':
            # Sadece PythonAnywhere uyumlu sistem kullan
            return fetch_latest_ai_articles_pythonanywhere()
            
        elif method == 'custom_sources_only':
            # Sadece √∂zel kaynaklarƒ± kullan
            return fetch_articles_from_custom_sources()
            
        else:  # method == 'auto'
            # Otomatik se√ßim - √ñncelik sƒ±rasƒ±na g√∂re dene
            all_articles = []
            
            # 1. √ñnce √∂zel kaynaklarƒ± dene
            try:
                custom_articles = fetch_articles_from_custom_sources()
                if custom_articles:
                    all_articles.extend(custom_articles)
                    print(f"‚úÖ √ñzel kaynaklardan {len(custom_articles)} makale")
            except Exception as e:
                print(f"‚ö†Ô∏è √ñzel kaynaklar hatasƒ±: {e}")
            
            # 2. PythonAnywhere sistemini dene
            try:
                pa_articles = fetch_latest_ai_articles_pythonanywhere()
                if pa_articles:
                    all_articles.extend(pa_articles)
                    print(f"‚úÖ PythonAnywhere sisteminden {len(pa_articles)} makale")
            except Exception as e:
                print(f"‚ö†Ô∏è PythonAnywhere sistemi hatasƒ±: {e}")
            
            # 3. MCP varsa onu da dene
            if mcp_enabled:
                try:
                    mcp_articles = fetch_latest_ai_articles_with_firecrawl()
                    if mcp_articles:
                        all_articles.extend(mcp_articles)
                        print(f"‚úÖ MCP'den {len(mcp_articles)} makale")
                except Exception as e:
                    print(f"‚ö†Ô∏è MCP hatasƒ±: {e}")
            
            # 4. Son √ßare fallback
            if not all_articles:
                try:
                    fallback_articles = fetch_latest_ai_articles_fallback()
                    if fallback_articles:
                        all_articles.extend(fallback_articles)
                        print(f"‚úÖ Fallback'den {len(fallback_articles)} makale")
                except Exception as e:
                    print(f"‚ö†Ô∏è Fallback hatasƒ±: {e}")
            
            # Duplikat temizleme
            if all_articles:
                unique_articles = filter_duplicate_articles(all_articles)
                print(f"üìä Toplam {len(unique_articles)} benzersiz makale bulundu")
                return unique_articles[:10]
            
            return []
        
    except Exception as e:
        print(f"‚ùå Akƒ±llƒ± haber √ßekme hatasƒ±: {e}")
        # Son √ßare fallback
        return fetch_latest_ai_articles_fallback()

def fetch_articles_with_mcp_only():
    """Sadece MCP y√∂ntemi ile haber kaynaklarƒ±ndan makale √ßekme - Son 24 saat filtreli"""
    try:
        print("üîç MCP y√∂ntemi ile haber √ßekme ba≈ülatƒ±lƒ±yor (Son 24 saat)...")
        
        # Bug√ºn√ºn tarih ve saatini al
        now = datetime.now()
        twenty_four_hours_ago = now - timedelta(hours=24)
        
        print(f"üìÖ Bug√ºn: {now.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"‚è∞ 24 saat √∂ncesi: {twenty_four_hours_ago.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # √ñnce mevcut yayƒ±nlanan makaleleri y√ºkle
        posted_articles = load_json(HISTORY_FILE)
        posted_urls = [article.get('url', '') for article in posted_articles]
        posted_hashes = [article.get('hash', '') for article in posted_articles]
        
        # Son 24 saat i√ßinde payla≈üƒ±lan makaleleri de kontrol et
        recent_posted_urls = []
        recent_posted_hashes = []
        
        for article in posted_articles:
            posted_date_str = article.get('posted_date') or article.get('fetch_date')
            if posted_date_str:
                try:
                    # ISO format tarih parse et
                    posted_date = datetime.fromisoformat(posted_date_str.replace('Z', '+00:00').replace('+00:00', ''))
                    if posted_date >= twenty_four_hours_ago:
                        recent_posted_urls.append(article.get('url', ''))
                        recent_posted_hashes.append(article.get('hash', ''))
                except Exception as date_error:
                    print(f"‚ö†Ô∏è Tarih parse hatasƒ±: {date_error}")
                    continue
        
        print(f"üìä Son 24 saatte payla≈üƒ±lan makale sayƒ±sƒ±: {len(recent_posted_urls)}")
        
        # Haber kaynaklarƒ±nƒ± y√ºkle
        config = load_news_sources()
        enabled_sources = [s for s in config.get("sources", []) if s.get("enabled", True)]
        
        if not enabled_sources:
            print("‚ö†Ô∏è Aktif haber kaynaƒüƒ± bulunamadƒ±")
            return []
        
        print(f"üì∞ {len(enabled_sources)} haber kaynaƒüƒ±ndan MCP ile makale √ßekiliyor...")
        
        all_articles = []
        
        for source in enabled_sources:
            try:
                print(f"üîç MCP ile √ßekiliyor: {source['name']}")
                
                # Geli≈ümi≈ü scraper ile ana sayfa √ßek (MCP fallback)
                try:
                    scrape_result = advanced_web_scraper(source['url'], wait_time=5, use_js=True, return_html=True)
                    
                    if not scrape_result or 'html' not in scrape_result:
                        print(f"[MCP] Geli≈ümi≈ü scraper deneniyor (JS: True)...")
                        scrape_result = mcp_firecrawl_scrape({
                            "url": source['url'],
                            "formats": ["markdown", "links"],
                            "onlyMainContent": True,
                            "waitFor": 2000
                        })
                        
                        if not scrape_result.get("success", False):
                            print(f"‚ö†Ô∏è {source['name']} MCP ile √ßekilemedi")
                            source["success_rate"] = max(0, source.get("success_rate", 100) - 20)
                            continue
                        
                        # Markdown i√ßeriƒüinden makale linklerini √ßƒ±kar
                        markdown_content = scrape_result.get("markdown", "")
                        print(f"[MCP] Firecrawl scrape ba≈üarƒ±lƒ±: {len(markdown_content)} karakter (firecrawl)")
                    else:
                        # HTML'den BeautifulSoup ile parse et
                        html_content = scrape_result['html']
                        soup = BeautifulSoup(html_content, 'html.parser')
                        
                                                 # TechCrunch i√ßin √∂zel parsing
                        if 'techcrunch.com' in source['url']:
                            # .loop-card elementlerini bul
                            loop_cards = soup.select('.loop-card')
                            print(f"[MCP] TechCrunch: {len(loop_cards)} loop-card bulundu")
                            
                            article_urls = []
                            for card in loop_cards:
                                title_link = card.select_one('h3 a, h2 a, .loop-card__title a')
                                if title_link:
                                    href = title_link.get('href', '')
                                    title = title_link.get_text().strip()
                                    
                                    if href and title:
                                        if href.startswith('/'):
                                            href = 'https://techcrunch.com' + href
                                        article_urls.append(href)
                                        print(f"   üì∞ {title[:50]}... -> {href}")
                            
                            # TechCrunch kategorilerini de kontrol et
                            category_links = soup.find_all('a', href=True)
                            for link in category_links:
                                href = link.get('href', '')
                                text = link.get_text().strip()
                                
                                # AI, Apps, Robotics gibi kategorilerdeki makaleleri de al
                                if (href and 'techcrunch.com' in href and 
                                    ('2025' in href or '2024' in href) and
                                    text and len(text) > 15 and
                                    href not in article_urls):
                                    
                                    # AI ile ilgili kategorileri kontrol et
                                    ai_keywords = ['ai', 'artificial', 'machine learning', 'robotics', 'automation', 'chatgpt', 'openai']
                                    if any(keyword in text.lower() for keyword in ai_keywords):
                                        article_urls.append(href)
                                        print(f"   ü§ñ Kategori makalesi: {text[:50]}... -> {href}")
                            
                            # Markdown content sim√ºlasyonu
                            markdown_content = '\n'.join([f"[Article]({url})" for url in article_urls])
                        else:
                            # Diƒüer siteler i√ßin genel parsing
                            all_links = soup.find_all('a', href=True)
                            article_urls = []
                            
                            for link in all_links:
                                href = link.get('href', '')
                                text = link.get_text().strip()
                                
                                if (href and text and len(text) > 15 and
                                    ('2025' in href or '2024' in href) and
                                    source['url'].split('/')[2] in href):
                                    
                                    if href.startswith('/'):
                                        base_url = f"https://{source['url'].split('/')[2]}"
                                        href = base_url + href
                                    
                                    article_urls.append(href)
                            
                            markdown_content = '\n'.join([f"[Article]({url})" for url in article_urls])
                        
                        print(f"[MCP] Geli≈ümi≈ü scraper ba≈üarƒ±lƒ±: {len(markdown_content)} karakter (selenium)")
                        
                except Exception as scraper_error:
                    print(f"‚ùå Scraper hatasƒ±: {scraper_error}")
                    continue
                
                # Makale URL'lerini bul
                import re
                
                # Kaynak URL'sinin domain'ini al
                from urllib.parse import urlparse
                source_domain = urlparse(source['url']).netloc
                
                # Bu domain'e ait makale URL'lerini bul
                url_patterns = [
                    rf'https?://{re.escape(source_domain)}/[^\s\)\]]+',
                    rf'https?://www\.{re.escape(source_domain)}/[^\s\)\]]+',
                ]
                
                article_urls = []
                for pattern in url_patterns:
                    found_urls = re.findall(pattern, markdown_content)
                    article_urls.extend(found_urls)
                
                # URL'leri temizle ve filtrele (24 saat kontrol√º ile)
                clean_urls = []
                for url in article_urls:
                    url = url.rstrip(')')
                    
                    # Makale URL'si olup olmadƒ±ƒüƒ±nƒ± kontrol et
                    if (url not in posted_urls and 
                        url not in recent_posted_urls and  # Son 24 saat kontrol√º
                        url not in clean_urls and
                        len(url) > 30 and  # √áok kƒ±sa URL'leri filtrele
                        not any(skip in url.lower() for skip in ['category', 'tag', 'author', 'page', 'search'])):
                        
                        # URL'den makale tarihini √ßƒ±karmaya √ßalƒ±≈ü (TechCrunch, The Verge gibi siteler i√ßin)
                        is_recent_article = check_article_url_date(url, twenty_four_hours_ago)
                        
                        # 24 saat filtresi yerine 48 saat (2 g√ºn) yapalƒ±m - daha fazla makale i√ßin
                        forty_eight_hours_ago = now - timedelta(hours=48)
                        is_recent_48h = check_article_url_date(url, forty_eight_hours_ago)
                        
                        if is_recent_48h:  # 48 saat i√ßindeki makaleleri al
                            clean_urls.append(url)
                        else:
                            print(f"‚è∞ Eski makale atlandƒ± (48h+): {url}")
                
                # Makale sayƒ±sƒ±nƒ± artƒ±r (5 -> 15)
                clean_urls = clean_urls[:15]
                print(f"üîó {source['name']}: {len(clean_urls)} makale URL'si bulundu")
                
                # Her makaleyi MCP ile √ßek
                source_articles = []
                for url in clean_urls:
                    try:
                        article_content = fetch_article_content_with_mcp_only(url)
                        
                        if article_content and len(article_content.get("content", "")) > 100:
                            title = article_content.get("title", "")
                            content = article_content.get("content", "")
                            publish_date = article_content.get("publish_date")
                            
                            # Makale yayƒ±n tarihini kontrol et
                            is_article_recent = True
                            if publish_date:
                                try:
                                    article_pub_date = datetime.fromisoformat(publish_date.replace('Z', ''))
                                    is_article_recent = article_pub_date >= twenty_four_hours_ago
                                    
                                    if not is_article_recent:
                                        print(f"‚è∞ Makale 24 saatten eski: {article_pub_date.strftime('%Y-%m-%d %H:%M')} - {title[:50]}...")
                                        continue
                                except Exception as date_error:
                                    print(f"‚ö†Ô∏è Makale tarih parse hatasƒ±: {date_error}")
                            
                            # Makale hash'i olu≈ütur
                            article_hash = hashlib.md5(title.encode()).hexdigest()
                            
                            # Tekrar kontrol√º (hem genel hem de son 24 saat)
                            if (article_hash not in posted_hashes and 
                                article_hash not in recent_posted_hashes and
                                is_article_recent):
                                
                                source_articles.append({
                                    "title": title,
                                    "url": url,
                                    "content": content,
                                    "hash": article_hash,
                                    "fetch_date": datetime.now().isoformat(),
                                    "is_new": True,
                                    "already_posted": False,
                                    "source": f"MCP - {source['name']}",
                                    "source_id": source["id"],
                                    "article_date": publish_date or datetime.now().isoformat(),
                                    "is_within_24h": True
                                })
                                print(f"üÜï MCP ile yeni makale (24h i√ßinde): {title[:50]}...")
                            else:
                                if article_hash in recent_posted_hashes:
                                    print(f"‚è∞ Son 24 saatte payla≈üƒ±lmƒ±≈ü: {title[:50]}...")
                                elif not is_article_recent:
                                    print(f"üìÖ 24 saatten eski makale: {title[:50]}...")
                                else:
                                    print(f"‚úÖ Makale zaten payla≈üƒ±lmƒ±≈ü: {title[:50]}...")
                        else:
                            print(f"‚ö†Ô∏è ƒ∞√ßerik yetersiz: {url}")
                            
                    except Exception as article_error:
                        print(f"‚ùå Makale √ßekme hatasƒ± ({url}): {article_error}")
                        continue
                
                if source_articles:
                    all_articles.extend(source_articles)
                    source["article_count"] = len(source_articles)
                    source["success_rate"] = min(100, source.get("success_rate", 0) + 10)
                    print(f"‚úÖ {source['name']}: {len(source_articles)} yeni makale bulundu")
                else:
                    source["success_rate"] = max(0, source.get("success_rate", 100) - 10)
                    print(f"‚ö†Ô∏è {source['name']}: Yeni makale bulunamadƒ±")
                
                source["last_checked"] = datetime.now().isoformat()
                
            except Exception as source_error:
                print(f"‚ùå {source['name']} kaynak hatasƒ±: {source_error}")
                source["success_rate"] = max(0, source.get("success_rate", 100) - 30)
                source["last_checked"] = datetime.now().isoformat()
        
        # G√ºncellenmi≈ü config'i kaydet
        try:
            save_json(NEWS_SOURCES_FILE, config)
        except Exception as save_error:
            print(f"‚ö†Ô∏è Haber kaynaklarƒ± kaydetme hatasƒ±: {save_error}")
        
        print(f"üìä MCP ile toplam {len(all_articles)} yeni makale bulundu (Son 24 saat filtreli)")
        
        # Duplikat filtreleme uygula
        if all_articles:
            all_articles = filter_duplicate_articles(all_articles)
            print(f"üîÑ Duplikat filtreleme sonrasƒ±: {len(all_articles)} benzersiz makale")
        
        # 24 saat i√ßindeki makaleleri i≈üaretle
        for article in all_articles:
            article['filtered_by_24h'] = True
            article['filter_applied_at'] = datetime.now().isoformat()
        
        return all_articles
        
    except Exception as e:
        print(f"‚ùå MCP haber √ßekme genel hatasƒ±: {e}")
        return []

def check_article_url_date(url, cutoff_date):
    """URL'den makale tarihini √ßƒ±karƒ±p son 24 saat i√ßinde olup olmadƒ±ƒüƒ±nƒ± kontrol et"""
    try:
        import re
        
        # TechCrunch URL formatƒ±: https://techcrunch.com/2025/01/09/article-name/
        techcrunch_pattern = r'/(\d{4})/(\d{2})/(\d{2})/'
        match = re.search(techcrunch_pattern, url)
        
        if match:
            year, month, day = match.groups()
            try:
                article_date = datetime(int(year), int(month), int(day))
                
                # Makale tarihi son 24 saat i√ßinde mi?
                is_recent = article_date >= cutoff_date.replace(hour=0, minute=0, second=0, microsecond=0)
                
                if is_recent:
                    print(f"üìÖ G√ºncel makale: {article_date.strftime('%Y-%m-%d')} - {url[:60]}...")
                else:
                    print(f"üìÖ Eski makale: {article_date.strftime('%Y-%m-%d')} - {url[:60]}...")
                
                return is_recent
                
            except ValueError as date_error:
                print(f"‚ö†Ô∏è Tarih parse hatasƒ±: {date_error}")
                return True  # Hata durumunda makaleyi dahil et
        
        # The Verge URL formatƒ±: https://www.theverge.com/2025/1/9/article-name
        verge_pattern = r'/(\d{4})/(\d{1,2})/(\d{1,2})/'
        match = re.search(verge_pattern, url)
        
        if match:
            year, month, day = match.groups()
            try:
                article_date = datetime(int(year), int(month), int(day))
                
                # Makale tarihi son 24 saat i√ßinde mi?
                is_recent = article_date >= cutoff_date.replace(hour=0, minute=0, second=0, microsecond=0)
                
                if is_recent:
                    print(f"üìÖ G√ºncel makale: {article_date.strftime('%Y-%m-%d')} - {url[:60]}...")
                else:
                    print(f"üìÖ Eski makale: {article_date.strftime('%Y-%m-%d')} - {url[:60]}...")
                
                return is_recent
                
            except ValueError as date_error:
                print(f"‚ö†Ô∏è Tarih parse hatasƒ±: {date_error}")
                return True  # Hata durumunda makaleyi dahil et
        
        # Diƒüer siteler i√ßin - URL'de tarih bulunamazsa g√ºncel kabul et
        print(f"üìÖ Tarih tespit edilemedi, g√ºncel kabul ediliyor: {url[:60]}...")
        return True
        
    except Exception as e:
        print(f"‚ö†Ô∏è URL tarih kontrol√º hatasƒ±: {e}")
        return True  # Hata durumunda makaleyi dahil et

def extract_article_date_from_content(content):
    """Makale i√ßeriƒüinden yayƒ±n tarihini √ßƒ±karmaya √ßalƒ±≈ü"""
    try:
        import re
        
        # √áe≈üitli tarih formatlarƒ±nƒ± ara
        date_patterns = [
            # ISO format: 2025-01-09T10:30:00
            r'(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2})',
            # Date format: January 9, 2025
            r'(January|February|March|April|May|June|July|August|September|October|November|December)\s+(\d{1,2}),?\s+(\d{4})',
            # Date format: 9 January 2025
            r'(\d{1,2})\s+(January|February|March|April|May|June|July|August|September|October|November|December)\s+(\d{4})',
            # Date format: 2025-01-09
            r'(\d{4}-\d{2}-\d{2})',
            # Date format: 01/09/2025
            r'(\d{2}/\d{2}/\d{4})',
            # Time ago format: "2 hours ago", "1 day ago"
            r'(\d+)\s+(hour|hours|day|days)\s+ago'
        ]
        
        now = datetime.now()
        
        for pattern in date_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            
            for match in matches:
                try:
                    if isinstance(match, tuple):
                        if len(match) == 1:  # ISO format veya basit tarih
                            date_str = match[0]
                            if 'T' in date_str:
                                # ISO format
                                return datetime.fromisoformat(date_str.replace('Z', ''))
                            else:
                                # Basit tarih formatƒ±
                                return datetime.strptime(date_str, '%Y-%m-%d')
                        
                        elif len(match) == 3:  # Month day, year format
                            if match[0].isdigit():  # Day month year
                                day, month_name, year = match
                                month_map = {
                                    'january': 1, 'february': 2, 'march': 3, 'april': 4,
                                    'may': 5, 'june': 6, 'july': 7, 'august': 8,
                                    'september': 9, 'october': 10, 'november': 11, 'december': 12
                                }
                                month = month_map.get(month_name.lower())
                                if month:
                                    return datetime(int(year), month, int(day))
                            else:  # Month day, year
                                month_name, day, year = match
                                month_map = {
                                    'january': 1, 'february': 2, 'march': 3, 'april': 4,
                                    'may': 5, 'june': 6, 'july': 7, 'august': 8,
                                    'september': 9, 'october': 10, 'november': 11, 'december': 12
                                }
                                month = month_map.get(month_name.lower())
                                if month:
                                    return datetime(int(year), month, int(day))
                        
                        elif len(match) == 2:  # Time ago format
                            amount, unit = match
                            amount = int(amount)
                            
                            if 'hour' in unit:
                                return now - timedelta(hours=amount)
                            elif 'day' in unit:
                                return now - timedelta(days=amount)
                    
                    else:  # Single string match
                        if '/' in match:  # MM/DD/YYYY format
                            return datetime.strptime(match, '%m/%d/%Y')
                        elif '-' in match:  # YYYY-MM-DD format
                            return datetime.strptime(match, '%Y-%m-%d')
                
                except (ValueError, TypeError) as parse_error:
                    continue
        
        return None
        
    except Exception as e:
        print(f"‚ö†Ô∏è Tarih √ßƒ±karma hatasƒ±: {e}")
        return None

def fetch_article_content_with_mcp_only(url):
    """Sadece MCP ile makale i√ßeriƒüi √ßekme"""
    try:
        print(f"üîç MCP ile makale √ßekiliyor: {url[:50]}...")
        
        # MCP scrape fonksiyonunu kullan
        scrape_result = mcp_firecrawl_scrape({
            "url": url,
            "formats": ["markdown"],
            "onlyMainContent": True,
            "waitFor": 3000,
            "removeBase64Images": True
        })
        
        if not scrape_result.get("success", False):
            print(f"‚ö†Ô∏è MCP ile √ßekilemedi: {url}")
            return None
        
        # Markdown i√ßeriƒüini al
        markdown_content = scrape_result.get("markdown", "")
        
        if not markdown_content or len(markdown_content) < 100:
            print(f"‚ö†Ô∏è MCP'den yetersiz i√ßerik: {len(markdown_content) if markdown_content else 0} karakter")
            return None
        
        # Ba≈ülƒ±ƒüƒ± √ßƒ±kar (genellikle ilk # ile ba≈ülar)
        lines = markdown_content.split('\n')
        title = ""
        content_lines = []
        
        for line in lines:
            line = line.strip()
            if line.startswith('# ') and not title:
                title = line[2:].strip()
            elif line and not line.startswith('#') and len(line) > 20:
                content_lines.append(line)
        
        # ƒ∞√ßeriƒüi birle≈ütir ve temizle
        content = '\n'.join(content_lines)
        
        # Gereksiz karakterleri temizle
        content = content.replace('*', '').replace('**', '').replace('_', '')
        content = ' '.join(content.split())  # √áoklu bo≈üluklarƒ± tek bo≈üluƒüa √ßevir
        
        # ƒ∞√ßeriƒüi sƒ±nƒ±rla
        content = content[:2500]
        
        # Makale tarihini i√ßerikten √ßƒ±karmaya √ßalƒ±≈ü
        article_publish_date = extract_article_date_from_content(markdown_content)
        
        print(f"‚úÖ MCP ile i√ßerik √ßekildi: {len(content)} karakter")
        if article_publish_date:
            print(f"üìÖ Makale yayƒ±n tarihi: {article_publish_date.strftime('%Y-%m-%d %H:%M')}")
        
        return {
            "title": title or "Ba≈ülƒ±k bulunamadƒ±",
            "content": content,
            "source": "mcp_only",
            "publish_date": article_publish_date.isoformat() if article_publish_date else None
        }
        
    except Exception as e:
        print(f"‚ùå MCP makale i√ßeriƒüi √ßekme hatasƒ± ({url}): {e}")
        return None

# GitHub MCP Mod√ºl√º
def fetch_trending_github_repos(language="python", time_period="daily", limit=10):
    """GitHub'dan trend olan repolarƒ± √ßek"""
    try:
        terminal_log("üîç GitHub trending repolarƒ± √ßekiliyor...", "info")
        
        # GitHub API endpoint'i
        base_url = "https://api.github.com/search/repositories"
        
        # Tarih hesaplama
        if time_period == "daily":
            since_date = (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")
        elif time_period == "weekly":
            since_date = (datetime.now() - timedelta(days=7)).strftime("%Y-%m-%d")
        else:  # monthly
            since_date = (datetime.now() - timedelta(days=30)).strftime("%Y-%m-%d")
        
        # Arama parametreleri
        params = {
            "q": f"language:{language} created:>{since_date}",
            "sort": "stars",
            "order": "desc",
            "per_page": limit
        }
        
        headers = {
            "Accept": "application/vnd.github.v3+json",
            "User-Agent": "AI-Tweet-Bot/1.0"
        }
        
        # GitHub token varsa ekle
        github_token = os.environ.get('GITHUB_TOKEN')
        if github_token:
            headers["Authorization"] = f"token {github_token}"
        
        response = requests.get(base_url, params=params, headers=headers, timeout=30)
        response.raise_for_status()
        
        data = response.json()
        repos = []
        
        for repo in data.get("items", []):
            repo_data = {
                "id": repo["id"],
                "name": repo["name"],
                "full_name": repo["full_name"],
                "description": repo.get("description", ""),
                "url": repo["html_url"],
                "stars": repo["stargazers_count"],
                "forks": repo["forks_count"],
                "language": repo.get("language", ""),
                "created_at": repo["created_at"],
                "updated_at": repo["updated_at"],
                "owner": {
                    "login": repo["owner"]["login"],
                    "avatar_url": repo["owner"]["avatar_url"]
                },
                "topics": repo.get("topics", []),
                "license": repo.get("license", {}).get("name", "") if repo.get("license") else "",
                "open_issues": repo.get("open_issues_count", 0),
                "watchers": repo.get("watchers_count", 0)
            }
            repos.append(repo_data)
        
        terminal_log(f"‚úÖ {len(repos)} GitHub repo √ßekildi", "success")
        return repos
        
    except requests.exceptions.RequestException as e:
        terminal_log(f"‚ùå GitHub API hatasƒ±: {e}", "error")
        return []
    except Exception as e:
        terminal_log(f"‚ùå GitHub repo √ßekme hatasƒ±: {e}", "error")
        return []

def fetch_github_repo_details_with_mcp(repo_url):
    """MCP ile GitHub repo detaylarƒ±nƒ± √ßek"""
    try:
        terminal_log(f"üîç GitHub repo detaylarƒ± √ßekiliyor: {repo_url}", "info")
        
        # MCP ile repo sayfasƒ±nƒ± √ßek
        scrape_result = mcp_firecrawl_scrape({
            "url": repo_url,
            "formats": ["markdown"],
            "onlyMainContent": True,
            "waitFor": 2000
        })
        
        if not scrape_result.get("success", False):
            terminal_log("‚ö†Ô∏è MCP ile repo √ßekilemedi, fallback y√∂nteme ge√ßiliyor...", "warning")
            return fetch_github_repo_details_fallback(repo_url)
        
        content = scrape_result.get("markdown", "")
        
        # README i√ßeriƒüini √ßƒ±kar
        readme_content = ""
        if "README" in content:
            readme_start = content.find("README")
            if readme_start != -1:
                readme_content = content[readme_start:readme_start+2000]  # ƒ∞lk 2000 karakter
        
        terminal_log("‚úÖ GitHub repo detaylarƒ± MCP ile √ßekildi", "success")
        return {
            "success": True,
            "content": content,
            "readme": readme_content,
            "method": "mcp"
        }
        
    except Exception as e:
        terminal_log(f"‚ùå MCP GitHub repo detay hatasƒ±: {e}", "error")
        return fetch_github_repo_details_fallback(repo_url)

def fetch_github_repo_details_fallback(repo_url):
    """Fallback y√∂ntemi ile GitHub repo detaylarƒ±nƒ± √ßek"""
    try:
        terminal_log(f"üîÑ Fallback ile GitHub repo detaylarƒ± √ßekiliyor: {repo_url}", "info")
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
        }
        
        response = requests.get(repo_url, headers=headers, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # README i√ßeriƒüini √ßƒ±kar
        readme_element = soup.find('article', class_='markdown-body')
        readme_content = ""
        if readme_element:
            readme_content = readme_element.get_text(strip=True)[:2000]
        
        # Repo a√ßƒ±klamasƒ±nƒ± √ßƒ±kar
        description_element = soup.find('p', class_='f4')
        description = ""
        if description_element:
            description = description_element.get_text(strip=True)
        
        content = f"Repository: {repo_url}\n"
        if description:
            content += f"Description: {description}\n"
        if readme_content:
            content += f"README: {readme_content}\n"
        
        terminal_log("‚úÖ GitHub repo detaylarƒ± fallback ile √ßekildi", "success")
        return {
            "success": True,
            "content": content,
            "readme": readme_content,
            "description": description,
            "method": "fallback"
        }
        
    except Exception as e:
        terminal_log(f"‚ùå Fallback GitHub repo detay hatasƒ±: {e}", "error")
        return {
            "success": False,
            "error": str(e),
            "method": "fallback"
        }

def generate_github_tweet(repo_data, api_key):
    """GitHub repo i√ßin AI tweet olu≈ütur"""
    try:
        terminal_log(f"ü§ñ GitHub repo i√ßin tweet olu≈üturuluyor: {repo_data['name']}", "info")
        
        # Repo detaylarƒ±nƒ± √ßek
        repo_details = fetch_github_repo_details_with_mcp(repo_data["url"])
        
        # Tweet i√ßin prompt hazƒ±rla
        prompt = f"""
        GitHub Repository Tweet Olu≈ütur:
        
        Repo Bilgileri:
        - ƒ∞sim: {repo_data['name']}
        - A√ßƒ±klama: {repo_data['description']}
        - Dil: {repo_data['language']}
        - Yƒ±ldƒ±z: {repo_data['stars']}
        - Fork: {repo_data['forks']}
        - Konular: {', '.join(repo_data['topics'][:5])}
        - Sahip: {repo_data['owner']['login']}
        - URL: {repo_data['url']}
        
        README ƒ∞√ßeriƒüi:
        {repo_details.get('readme', '')[:1000]}
        
        L√ºtfen bu GitHub repository i√ßin:
        1. T√ºrk√ße bir tweet yazƒ±n (280 karakter sƒ±nƒ±rƒ±)
        2. Repo'nun √∂ne √ßƒ±kan √∂zelliklerini vurgulayƒ±n
        3. Geli≈ütiriciler i√ßin neden ilgin√ß olduƒüunu a√ßƒ±klayƒ±n
        4. Uygun hashtag'ler ekleyin (#GitHub #OpenSource #Programming)
        5. Emoji kullanarak g√∂rsel √ßekicilik katƒ±n
        
        Tweet formatƒ±:
        [Tweet metni]
        
        [Hashtag'ler]
        
        URL: {repo_data['url']}
        """
        
        # Gemini API ile tweet olu≈ütur
        tweet_response = gemini_call(prompt, api_key, max_tokens=200)
        
        if not tweet_response:
            # Fallback tweet
            tweet_text = create_fallback_github_tweet(repo_data)
        else:
            tweet_text = tweet_response.strip()
        
        # Tweet'i temizle ve formatla
        if len(tweet_text) > 280:
            tweet_text = tweet_text[:277] + "..."
        
        terminal_log("‚úÖ GitHub tweet olu≈üturuldu", "success")
        
        return {
            "success": True,
            "tweet": tweet_text,
            "repo_data": repo_data,
            "repo_details": repo_details
        }
        
    except Exception as e:
        terminal_log(f"‚ùå GitHub tweet olu≈üturma hatasƒ±: {e}", "error")
        
        # Fallback tweet
        tweet_text = create_fallback_github_tweet(repo_data)
        
        return {
            "success": False,
            "tweet": tweet_text,
            "repo_data": repo_data,
            "error": str(e)
        }

def create_fallback_github_tweet(repo_data):
    """GitHub repo i√ßin basit fallback tweet"""
    try:
        name = repo_data.get('name', 'Unknown')
        description = repo_data.get('description', '')[:100]
        language = repo_data.get('language', '')
        stars = repo_data.get('stars', 0)
        url = repo_data.get('url', '')
        
        # Basit tweet formatƒ±
        tweet = f"üöÄ {name}\n"
        
        if description:
            tweet += f"{description}\n"
        
        if language:
            tweet += f"üíª {language} "
        
        if stars > 0:
            tweet += f"‚≠ê {stars} stars"
        
        tweet += f"\n\n#GitHub #OpenSource #Programming"
        
        if language:
            tweet += f" #{language}"
        
        tweet += f"\n\n{url}"
        
        # 280 karakter sƒ±nƒ±rƒ±
        if len(tweet) > 280:
            tweet = tweet[:277] + "..."
        
        return tweet
        
    except Exception as e:
        return f"üöÄ Yeni GitHub projesi ke≈üfedildi!\n\n#GitHub #OpenSource #Programming\n\n{repo_data.get('url', '')}"

def fetch_github_articles_for_tweets():
    """Tweet i√ßin GitHub repolarƒ±nƒ± √ßek ve i≈üle"""
    try:
        terminal_log("üîç GitHub repolarƒ± tweet i√ßin √ßekiliyor...", "info")
        
        # Farklƒ± dillerde trend repolarƒ± √ßek
        languages = ["python", "javascript", "typescript", "go", "rust", "java"]
        all_repos = []
        
        for language in languages[:3]:  # ƒ∞lk 3 dil
            repos = fetch_trending_github_repos(language=language, time_period="weekly", limit=5)
            all_repos.extend(repos)
        
        if not all_repos:
            terminal_log("‚ùå GitHub repo bulunamadƒ±", "warning")
            return []
        
        # Mevcut payla≈üƒ±lan repolarƒ± kontrol et
        posted_articles = load_json(HISTORY_FILE)
        posted_urls = [article.get('url', '') for article in posted_articles]
        
        # Yeni repolarƒ± filtrele
        new_repos = []
        for repo in all_repos:
            if repo['url'] not in posted_urls:
                new_repos.append(repo)
        
        terminal_log(f"‚úÖ {len(new_repos)} yeni GitHub repo bulundu", "success")
        
        # Tweet formatƒ±na d√∂n√º≈üt√ºr
        github_articles = []
        for repo in new_repos[:5]:  # ƒ∞lk 5 repo
            article = {
                "title": f"{repo['name']} - {repo['description'][:100]}",
                "url": repo['url'],
                "content": f"GitHub Repository: {repo['full_name']}\n"
                          f"Description: {repo['description']}\n"
                          f"Language: {repo['language']}\n"
                          f"Stars: {repo['stars']}\n"
                          f"Forks: {repo['forks']}\n"
                          f"Topics: {', '.join(repo['topics'][:5])}",
                "source": "GitHub",
                "published_date": repo['created_at'],
                "hash": hashlib.md5(repo['url'].encode()).hexdigest(),
                "repo_data": repo
            }
            github_articles.append(article)
        
        terminal_log(f"‚úÖ {len(github_articles)} GitHub makalesi hazƒ±rlandƒ±", "success")
        return github_articles
        
    except Exception as e:
        terminal_log(f"‚ùå GitHub makalesi √ßekme hatasƒ±: {e}", "error")
        return []

def save_github_repo_history(repo_data, tweet_result):
    """GitHub repo payla≈üƒ±m ge√ßmi≈üini kaydet"""
    try:
        # Mevcut ge√ßmi≈üi y√ºkle
        posted_articles = load_json(HISTORY_FILE)
        
        # Yeni kayƒ±t olu≈ütur
        new_record = {
            "title": f"{repo_data['name']} - GitHub Repository",
            "url": repo_data['url'],
            "content": repo_data.get('description', ''),
            "source": "GitHub",
            "published_date": repo_data.get('created_at', datetime.now().isoformat()),
            "posted_date": datetime.now().isoformat(),
            "hash": hashlib.md5(repo_data['url'].encode()).hexdigest(),
            "tweet_id": tweet_result.get('tweet_id'),
            "tweet_url": tweet_result.get('tweet_url', ''),
            "repo_data": repo_data,
            "type": "github_repo"
        }
        
        # Listeye ekle
        posted_articles.append(new_record)
        
        # Kaydet
        save_json(HISTORY_FILE, posted_articles)
        
        terminal_log(f"‚úÖ GitHub repo ge√ßmi≈üi kaydedildi: {repo_data['name']}", "success")
        
    except Exception as e:
        terminal_log(f"‚ùå GitHub repo ge√ßmi≈üi kaydetme hatasƒ±: {e}", "error")

def get_github_stats():
    """GitHub mod√ºl√º istatistiklerini getir"""
    try:
        posted_articles = load_json(HISTORY_FILE)
        
        # GitHub repolarƒ±nƒ± filtrele
        github_repos = [article for article in posted_articles if article.get('type') == 'github_repo']
        
        # ƒ∞statistikleri hesapla
        total_repos = len(github_repos)
        
        # Dil daƒüƒ±lƒ±mƒ±
        languages = {}
        for repo in github_repos:
            repo_data = repo.get('repo_data', {})
            lang = repo_data.get('language', 'Unknown')
            languages[lang] = languages.get(lang, 0) + 1
        
        # Son 7 g√ºnde payla≈üƒ±lan
        week_ago = datetime.now() - timedelta(days=7)
        recent_repos = [
            repo for repo in github_repos 
            if datetime.fromisoformat(repo.get('posted_date', '2020-01-01')) > week_ago
        ]
        
        return {
            "total_repos": total_repos,
            "recent_repos": len(recent_repos),
            "languages": languages,
            "last_repo": github_repos[-1] if github_repos else None
        }
        
    except Exception as e:
        terminal_log(f"‚ùå GitHub istatistik hatasƒ±: {e}", "error")
        return {
            "total_repos": 0,
            "recent_repos": 0,
            "languages": {},
            "last_repo": None
        }
