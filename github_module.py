import os
import json
import requests
from bs4 import BeautifulSoup
import hashlib
from datetime import datetime, timedelta
from dotenv import load_dotenv

# .env dosyasƒ±nƒ± y√ºkle
load_dotenv()

# Diƒüer mod√ºllerden import
try:
    from utils import (
        terminal_log, load_json, save_json, gemini_call, 
        mcp_firecrawl_scrape
    )
    HISTORY_FILE = "posted_articles.json"
    print("‚úÖ GitHub mod√ºl√º utils import ba≈üarƒ±lƒ±")
except ImportError as e:
    print(f"‚ö†Ô∏è GitHub mod√ºl√º utils import hatasƒ±: {e}")
    # Fallback fonksiyonlar
    def terminal_log(message, level="info"):
        print(f"[{level.upper()}] {message}")
    
    def load_json(filename, default=None):
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"JSON y√ºkleme hatasƒ± ({filename}): {e}")
            return default if default is not None else []
    
    def save_json(filename, data):
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"‚úÖ JSON kaydedildi: {filename}")
        except Exception as e:
            print(f"‚ùå JSON kaydetme hatasƒ± ({filename}): {e}")
    
    def gemini_call(prompt, api_key, max_tokens=200):
        return None
    
    def mcp_firecrawl_scrape(params):
        return {"success": False, "error": "MCP not available"}
    
    HISTORY_FILE = "posted_articles.json"

def load_github_settings():
    """GitHub ayarlarƒ±nƒ± y√ºkle"""
    try:
        default_settings = {
            "default_language": "python",
            "default_time_period": "weekly", 
            "default_limit": 10,
            "search_topics": ["ai", "machine-learning", "deep-learning"],
            "custom_search_queries": [],
            "languages": ["python", "javascript", "typescript", "go", "rust", "java", "cpp", "csharp", "swift", "kotlin"],
            "time_periods": ["daily", "weekly", "monthly"]
        }
        
        try:
            settings = load_json("github_settings.json")
            if not settings:
                settings = default_settings
        except:
            settings = default_settings
        terminal_log(f"‚úÖ GitHub ayarlarƒ± y√ºklendi: {len(settings.get('search_topics', []))} konu", "info")
        return settings
    except Exception as e:
        terminal_log(f"‚ùå GitHub ayarlarƒ± y√ºkleme hatasƒ±: {e}", "error")
        return {
            "default_language": "python",
            "default_time_period": "weekly", 
            "default_limit": 10,
            "search_topics": ["ai", "machine-learning", "deep-learning"],
            "custom_search_queries": [],
            "languages": ["python", "javascript", "typescript", "go", "rust", "java", "cpp", "csharp", "swift", "kotlin"],
            "time_periods": ["daily", "weekly", "monthly"]
        }

def save_github_settings(settings):
    """GitHub ayarlarƒ±nƒ± kaydet"""
    try:
        # Ayarlarƒ± doƒürula
        if not isinstance(settings, dict):
            terminal_log("‚ùå Ge√ßersiz ayar formatƒ±", "error")
            return False
        
        # Zorunlu alanlarƒ± kontrol et
        required_fields = ["default_language", "default_time_period", "default_limit"]
        for field in required_fields:
            if field not in settings:
                terminal_log(f"‚ùå Eksik ayar alanƒ±: {field}", "error")
                return False
        
        # Timestamp ekle
        settings["last_updated"] = datetime.now().isoformat()
        
        # Kaydet
        save_json("github_settings.json", settings)
        terminal_log(f"‚úÖ GitHub ayarlarƒ± kaydedildi - Konular: {settings.get('search_topics', [])}", "success")
        return True
    except Exception as e:
        terminal_log(f"‚ùå GitHub ayarlarƒ± kaydetme hatasƒ±: {e}", "error")
        return False

def search_github_repos_by_topics(topics=None, language="python", time_period="weekly", limit=10):
    """Konulara g√∂re GitHub repolarƒ± ara"""
    try:
        terminal_log(f"üîç GitHub repolarƒ± konulara g√∂re aranƒ±yor: {topics}", "info")
        
        base_url = "https://api.github.com/search/repositories"
        
        # Tarih hesaplama - √ßok daha geni≈ü aralƒ±k kullan
        if time_period == "daily":
            since_date = (datetime.now() - timedelta(days=30)).strftime("%Y-%m-%d")  # Son 30 g√ºn
        elif time_period == "weekly":
            since_date = (datetime.now() - timedelta(days=180)).strftime("%Y-%m-%d")  # Son 6 ay
        else:  # monthly
            since_date = (datetime.now() - timedelta(days=365)).strftime("%Y-%m-%d")  # Son 1 yƒ±l
        
        # Arama sorgusu olu≈ütur
        query_parts = []
        
        # Dil filtresi (any deƒüilse)
        if language and language.lower() != "any":
            query_parts.append(f"language:{language}")
        
        # Tarih filtresi - pushed (son g√ºncelleme) kullan, created yerine
        query_parts.append(f"pushed:>{since_date}")
        
        # Minimum yƒ±ldƒ±z sayƒ±sƒ± ekle (kaliteli repolar i√ßin) - daha d√º≈ü√ºk threshold
        query_parts.append("stars:>1")
        
        if topics and len(topics) > 0:
            # Konularƒ± arama sorgusuna ekle
            if isinstance(topics, str):
                topics = [topics]
            
            # Konularƒ± temizle ve filtrele
            clean_topics = [topic.strip() for topic in topics if topic.strip()]
            terminal_log(f"üè∑Ô∏è Temizlenmi≈ü konular: {clean_topics}", "info")
            
            if clean_topics:
                # Sadece ilk konuyu kullan (GitHub API OR sorgularƒ± ile sorun ya≈üƒ±yor)
                primary_topic = clean_topics[0]
                
                # √ñnce topic aramasƒ± dene
                topic_query = f'topic:{primary_topic}'
                query_parts.append(topic_query)
        else:
            # Konu belirtilmemi≈üse genel AI konusunu ekle
            query_parts.append('topic:ai')
        
        query = " ".join(query_parts)
        terminal_log(f"üîç GitHub arama sorgusu: {query}", "info")
        
        params = {
            "q": query,
            "sort": "stars",
            "order": "desc",
            "per_page": min(limit, 100)  # GitHub API limiti
        }
        
        headers = {
            "Accept": "application/vnd.github.v3+json",
            "User-Agent": "AI-Tweet-Bot/1.0"
        }
        
        # GitHub token varsa ekle
        github_token = os.environ.get('GITHUB_TOKEN')
        if github_token:
            headers["Authorization"] = f"token {github_token}"
            terminal_log("üîë GitHub token kullanƒ±lƒ±yor", "info")
        else:
            terminal_log("‚ö†Ô∏è GitHub token bulunamadƒ±, rate limit d√º≈ü√ºk olacak", "warning")
        
        terminal_log(f"üì° GitHub API'ye istek g√∂nderiliyor...", "info")
        response = requests.get(base_url, params=params, headers=headers, timeout=30)
        
        # Rate limit kontrol√º
        if response.status_code == 403:
            terminal_log("‚ùå GitHub API rate limit a≈üƒ±ldƒ±", "error")
            return []
        
        response.raise_for_status()
        
        data = response.json()
        total_count = data.get("total_count", 0)
        terminal_log(f"üìä GitHub API'den {total_count} toplam sonu√ß bulundu", "info")
        
        # Eƒüer sonu√ß yoksa, daha esnek arama yap
        if total_count == 0:
            terminal_log("üîÑ Sonu√ß bulunamadƒ±, daha esnek arama deneniyor...", "info")
            
            # Daha basit sorgu olu≈ütur
            simple_query_parts = []
            if language and language.lower() != "any":
                simple_query_parts.append(f"language:{language}")
            
            # Sadece konularƒ± ara, tarih ve yƒ±ldƒ±z filtresi olmadan
            if topics and len(topics) > 0:
                clean_topics = [topic.strip() for topic in topics if topic.strip()]
                if clean_topics:
                    # ƒ∞lk konuyu kullan
                    primary_topic = clean_topics[0]
                    simple_query_parts.append(f'{primary_topic} in:name,description')
            
            simple_query = " ".join(simple_query_parts)
            terminal_log(f"üîç Basit arama sorgusu: {simple_query}", "info")
            
            simple_params = {
                "q": simple_query,
                "sort": "stars",
                "order": "desc",
                "per_page": min(limit, 100)
            }
            
            simple_response = requests.get(base_url, params=simple_params, headers=headers, timeout=30)
            if simple_response.status_code == 200:
                simple_data = simple_response.json()
                total_count = simple_data.get("total_count", 0)
                data = simple_data
                terminal_log(f"üìä Basit arama ile {total_count} sonu√ß bulundu", "info")
        
        repos = []
        
        for repo in data.get("items", []):
            try:
                repo_data = {
                    "id": repo["id"],
                    "name": repo["name"],
                    "full_name": repo["full_name"],
                    "description": repo.get("description", ""),
                    "url": repo["html_url"],
                    "stars": repo["stargazers_count"],
                    "forks": repo["forks_count"],
                    "language": repo.get("language", ""),
                    "created_at": repo["created_at"],
                    "updated_at": repo["updated_at"],
                    "owner": {
                        "login": repo["owner"]["login"],
                        "avatar_url": repo["owner"]["avatar_url"]
                    },
                    "topics": repo.get("topics", []),
                    "license": repo.get("license", {}).get("name", "") if repo.get("license") else "",
                    "open_issues": repo.get("open_issues_count", 0),
                    "watchers": repo.get("watchers_count", 0),
                    "search_topics": topics  # Hangi konularla bulunduƒüunu kaydet
                }
                repos.append(repo_data)
            except Exception as repo_error:
                terminal_log(f"‚ö†Ô∏è Repo verisi i≈ülenirken hata: {repo_error}", "warning")
                continue
        
        terminal_log(f"‚úÖ {len(repos)} GitHub repo bulundu (konular: {topics})", "success")
        return repos
        
    except requests.exceptions.RequestException as e:
        terminal_log(f"‚ùå GitHub API hatasƒ±: {e}", "error")
        if hasattr(e, 'response') and e.response is not None:
            terminal_log(f"‚ùå HTTP Status: {e.response.status_code}", "error")
            terminal_log(f"‚ùå Response: {e.response.text[:200]}", "error")
        return []
    except Exception as e:
        terminal_log(f"‚ùå GitHub konu aramasƒ± hatasƒ±: {e}", "error")
        return []

def fetch_trending_github_repos_from_web(time_period="daily", limit=10):
    """GitHub trending sayfasƒ±ndan direkt repo √ßek"""
    try:
        terminal_log("üîç GitHub trending sayfasƒ±ndan repolar √ßekiliyor...", "info")
        
        # GitHub trending URL'i
        base_url = "https://github.com/trending"
        
        # Zaman parametresi
        params = {}
        if time_period == "daily":
            params["since"] = "daily"
        elif time_period == "weekly":
            params["since"] = "weekly"
        else:  # monthly
            params["since"] = "monthly"
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.5",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1"
        }
        
        terminal_log(f"üì° GitHub trending sayfasƒ±na istek g√∂nderiliyor: {base_url}", "info")
        response = requests.get(base_url, params=params, headers=headers, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        repos = []
        
        # Trending repo listesini bul - GitHub'ƒ±n yeni yapƒ±sƒ±na g√∂re
        repo_articles = soup.find_all('article', class_='Box-row') or soup.find_all('div', class_='Box-row')
        
        if not repo_articles:
            # Alternatif selector'lar dene
            repo_articles = soup.select('article') or soup.select('.Box-row')
        
        terminal_log(f"üìä {len(repo_articles)} trending repo elementi bulundu", "info")
        
        for article in repo_articles[:limit]:
            try:
                # Repo adƒ± ve URL - farklƒ± selector'lar dene
                repo_link = None
                
                # Yeni GitHub yapƒ±sƒ±
                h2_elem = article.find('h2') or article.find('h1')
                if h2_elem:
                    repo_link = h2_elem.find('a')
                
                # Alternatif selector
                if not repo_link:
                    repo_link = article.find('a', href=lambda x: x and '/' in x and not x.startswith('#'))
                
                if not repo_link:
                    terminal_log("‚ö†Ô∏è Repo linki bulunamadƒ±, atlanƒ±yor", "warning")
                    continue
                
                # Repo adƒ± ve URL'i temizle
                repo_href = repo_link.get('href', '')
                if not repo_href.startswith('/'):
                    continue
                    
                repo_url = "https://github.com" + repo_href
                repo_name = repo_href.strip('/').replace('/', '/')
                
                # A√ßƒ±klama - farklƒ± yerlerde olabilir
                description = ""
                desc_selectors = [
                    'p.color-fg-muted',
                    'p[class*="col-"]',
                    '.color-fg-muted p',
                    'p'
                ]
                
                for selector in desc_selectors:
                    desc_elem = article.select_one(selector)
                    if desc_elem and desc_elem.get_text().strip():
                        description = desc_elem.get_text().strip()
                        break
                
                # Dil
                language = ""
                lang_selectors = [
                    'span[itemprop="programmingLanguage"]',
                    '.color-fg-muted span',
                    'span.ml-0'
                ]
                
                for selector in lang_selectors:
                    lang_elem = article.select_one(selector)
                    if lang_elem and lang_elem.get_text().strip():
                        lang_text = lang_elem.get_text().strip()
                        if not any(word in lang_text.lower() for word in ['star', 'fork', 'today', 'yesterday']):
                            language = lang_text
                            break
                
                # Yƒ±ldƒ±z ve fork sayƒ±larƒ±
                stars = 0
                forks = 0
                today_stars = 0
                
                # Sayƒ±larƒ± i√ßeren t√ºm linkleri bul
                stat_links = article.find_all('a', href=True)
                
                for link in stat_links:
                    href = link.get('href', '')
                    text = link.get_text().strip().replace(',', '').replace('k', '000')
                    
                    if '/stargazers' in href:
                        try:
                            stars = int(text) if text.isdigit() else 0
                        except:
                            stars = 0
                    elif '/forks' in href:
                        try:
                            forks = int(text) if text.isdigit() else 0
                        except:
                            forks = 0
                
                # Bug√ºnk√º yƒ±ldƒ±z sayƒ±sƒ±
                today_star_patterns = ['stars today', 'star today', 'stars this week', 'star this week']
                all_text = article.get_text().lower()
                
                for pattern in today_star_patterns:
                    if pattern in all_text:
                        # Pattern'den √∂nceki sayƒ±yƒ± bul
                        import re
                        match = re.search(r'(\d+(?:,\d+)*)\s*' + pattern, all_text)
                        if match:
                            try:
                                today_stars = int(match.group(1).replace(',', ''))
                                break
                            except:
                                today_stars = 0
                
                # Owner bilgisi
                owner_name = repo_name.split('/')[0] if '/' in repo_name else ""
                
                repo_data = {
                    "id": hash(repo_url),  # Unique ID olu≈ütur
                    "name": repo_name.split('/')[-1] if '/' in repo_name else repo_name,
                    "full_name": repo_name,
                    "description": description,
                    "url": repo_url,
                    "stars": stars,
                    "forks": forks,
                    "language": language,
                    "created_at": datetime.now().isoformat(),
                    "updated_at": datetime.now().isoformat(),
                    "owner": {
                        "login": owner_name,
                        "avatar_url": f"https://github.com/{owner_name}.png" if owner_name else ""
                    },
                    "topics": [],
                    "license": "",
                    "open_issues": 0,
                    "watchers": stars,
                    "today_stars": today_stars,
                    "trending_period": time_period
                }
                repos.append(repo_data)
                
                terminal_log(f"‚úÖ Repo eklendi: {repo_name} ({stars} ‚≠ê, {today_stars} bug√ºn)", "info")
                
            except Exception as repo_error:
                terminal_log(f"‚ö†Ô∏è Repo verisi i≈ülenirken hata: {repo_error}", "warning")
                continue
        
        terminal_log(f"‚úÖ {len(repos)} GitHub trending repo √ßekildi", "success")
        return repos
        
    except requests.exceptions.RequestException as e:
        terminal_log(f"‚ùå GitHub trending sayfasƒ± hatasƒ±: {e}", "error")
        return []
    except Exception as e:
        terminal_log(f"‚ùå GitHub trending √ßekme hatasƒ±: {e}", "error")
        return []

def fetch_trending_github_repos(language="python", time_period="daily", limit=10):
    """GitHub'dan trend olan repolarƒ± √ßek (API + Web scraping hybrid)"""
    try:
        terminal_log("üîç GitHub trending repolarƒ± √ßekiliyor...", "info")
        
        # √ñnce web scraping ile trending sayfasƒ±ndan √ßek
        trending_repos = fetch_trending_github_repos_from_web(time_period, limit * 2)
        
        if trending_repos:
            # Dil filtresi uygula
            if language and language.lower() != "any":
                filtered_repos = [
                    repo for repo in trending_repos 
                    if repo.get('language', '').lower() == language.lower()
                ]
                if filtered_repos:
                    terminal_log(f"‚úÖ {len(filtered_repos)} {language} repo bulundu (trending)", "success")
                    return filtered_repos[:limit]
                else:
                    terminal_log(f"‚ö†Ô∏è {language} dilinde trending repo bulunamadƒ±, t√ºm diller g√∂steriliyor", "warning")
                    return trending_repos[:limit]
            else:
                terminal_log(f"‚úÖ {len(trending_repos)} trending repo bulundu (t√ºm diller)", "success")
                return trending_repos[:limit]
        
        # Fallback: GitHub API kullan
        terminal_log("üîÑ Fallback: GitHub API kullanƒ±lƒ±yor...", "info")
        
        # GitHub API endpoint'i
        base_url = "https://api.github.com/search/repositories"
        
        # Tarih hesaplama
        if time_period == "daily":
            since_date = (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")
        elif time_period == "weekly":
            since_date = (datetime.now() - timedelta(days=7)).strftime("%Y-%m-%d")
        else:  # monthly
            since_date = (datetime.now() - timedelta(days=30)).strftime("%Y-%m-%d")
        
        # Arama parametreleri
        params = {
            "q": f"language:{language} created:>{since_date}",
            "sort": "stars",
            "order": "desc",
            "per_page": limit
        }
        
        headers = {
            "Accept": "application/vnd.github.v3+json",
            "User-Agent": "AI-Tweet-Bot/1.0"
        }
        
        # GitHub token varsa ekle
        github_token = os.environ.get('GITHUB_TOKEN')
        if github_token:
            headers["Authorization"] = f"token {github_token}"
        
        response = requests.get(base_url, params=params, headers=headers, timeout=30)
        response.raise_for_status()
        
        data = response.json()
        repos = []
        
        for repo in data.get("items", []):
            repo_data = {
                "id": repo["id"],
                "name": repo["name"],
                "full_name": repo["full_name"],
                "description": repo.get("description", ""),
                "url": repo["html_url"],
                "stars": repo["stargazers_count"],
                "forks": repo["forks_count"],
                "language": repo.get("language", ""),
                "created_at": repo["created_at"],
                "updated_at": repo["updated_at"],
                "owner": {
                    "login": repo["owner"]["login"],
                    "avatar_url": repo["owner"]["avatar_url"]
                },
                "topics": repo.get("topics", []),
                "license": repo.get("license", {}).get("name", "") if repo.get("license") else "",
                "open_issues": repo.get("open_issues_count", 0),
                "watchers": repo.get("watchers_count", 0)
            }
            repos.append(repo_data)
        
        terminal_log(f"‚úÖ {len(repos)} GitHub repo √ßekildi (API)", "success")
        return repos
        
    except requests.exceptions.RequestException as e:
        terminal_log(f"‚ùå GitHub API hatasƒ±: {e}", "error")
        return []
    except Exception as e:
        terminal_log(f"‚ùå GitHub repo √ßekme hatasƒ±: {e}", "error")
        return []

def fetch_github_repo_details_with_mcp(repo_url):
    """MCP ile GitHub repo detaylarƒ±nƒ± √ßek"""
    try:
        terminal_log(f"üîç GitHub repo detaylarƒ± √ßekiliyor: {repo_url}", "info")
        
        # MCP ile repo sayfasƒ±nƒ± √ßek
        scrape_result = mcp_firecrawl_scrape({
            "url": repo_url,
            "formats": ["markdown"],
            "onlyMainContent": True,
            "waitFor": 2000
        })
        
        if not scrape_result.get("success", False):
            terminal_log("‚ö†Ô∏è MCP ile repo √ßekilemedi, fallback y√∂nteme ge√ßiliyor...", "warning")
            return fetch_github_repo_details_fallback(repo_url)
        
        content = scrape_result.get("markdown", "")
        
        # README i√ßeriƒüini √ßƒ±kar
        readme_content = ""
        if "README" in content:
            readme_start = content.find("README")
            if readme_start != -1:
                readme_content = content[readme_start:readme_start+2000]  # ƒ∞lk 2000 karakter
        
        terminal_log("‚úÖ GitHub repo detaylarƒ± MCP ile √ßekildi", "success")
        return {
            "success": True,
            "content": content,
            "readme": readme_content,
            "method": "mcp"
        }
        
    except Exception as e:
        terminal_log(f"‚ùå MCP GitHub repo detay hatasƒ±: {e}", "error")
        return fetch_github_repo_details_fallback(repo_url)

def fetch_github_repo_details_fallback(repo_url):
    """Fallback y√∂ntemi ile GitHub repo detaylarƒ±nƒ± √ßek"""
    try:
        terminal_log(f"üîÑ Fallback ile GitHub repo detaylarƒ± √ßekiliyor: {repo_url}", "info")
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
        }
        
        response = requests.get(repo_url, headers=headers, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # README i√ßeriƒüini √ßƒ±kar
        readme_element = soup.find('article', class_='markdown-body')
        readme_content = ""
        if readme_element:
            readme_content = readme_element.get_text(strip=True)[:2000]
        
        # Repo a√ßƒ±klamasƒ±nƒ± √ßƒ±kar
        description_element = soup.find('p', class_='f4')
        description = ""
        if description_element:
            description = description_element.get_text(strip=True)
        
        content = f"Repository: {repo_url}\n"
        if description:
            content += f"Description: {description}\n"
        if readme_content:
            content += f"README: {readme_content}\n"
        
        terminal_log("‚úÖ GitHub repo detaylarƒ± fallback ile √ßekildi", "success")
        return {
            "success": True,
            "content": content,
            "readme": readme_content,
            "description": description,
            "method": "fallback"
        }
        
    except Exception as e:
        terminal_log(f"‚ùå Fallback GitHub repo detay hatasƒ±: {e}", "error")
        return {
            "success": False,
            "error": str(e),
            "method": "fallback"
        }

def generate_github_tweet(repo_data, api_key):
    """GitHub repo i√ßin AI tweet olu≈ütur"""
    try:
        terminal_log(f"ü§ñ GitHub repo i√ßin tweet olu≈üturuluyor: {repo_data['name']}", "info")
        
        # Repo detaylarƒ±nƒ± √ßek
        repo_details = fetch_github_repo_details_with_mcp(repo_data["url"])
        
        # Tweet i√ßin prompt hazƒ±rla
        prompt = f"""
        Create an English Tweet for GitHub Repository:
        
        Repository Information:
        - Name: {repo_data['name']}
        - Description: {repo_data['description']}
        - Language: {repo_data['language']}
        - Stars: {repo_data['stars']}
        - Forks: {repo_data['forks']}
        - Topics: {', '.join(repo_data['topics'][:5])}
        - Owner: {repo_data['owner']['login']}
        - URL: {repo_data['url']}
        
        README Content:
        {repo_details.get('readme', '')[:1000]}
        
        Please create an English tweet for this GitHub repository:
        1. Write in English (280 character limit)
        2. Highlight the key features of the repository
        3. Explain why it's interesting for developers
        4. Add relevant hashtags (#GitHub #OpenSource #Programming)
        5. Use emojis for visual appeal
        6. Keep it engaging and professional
        7. CRITICAL: You MUST include the exact URL at the end: {repo_data['url']}
        8. The URL must be the last line of your tweet
        9. Never modify, shorten, or omit the URL
        
        Required format:
        [Tweet description with emojis]
        
        [Hashtags like #GitHub #OpenSource #Programming]
        
        {repo_data['url']}
        
        EXAMPLE:
        üöÄ Amazing Python project for AI development! ‚≠ê 500 stars
        
        #GitHub #OpenSource #Programming #Python
        
        https://github.com/example/repo
        """
        
        # Gemini API ile tweet olu≈ütur
        tweet_response = gemini_call(prompt, api_key, max_tokens=200)
        
        if not tweet_response:
            # Fallback tweet
            tweet_text = create_fallback_github_tweet(repo_data)
        else:
            tweet_text = tweet_response.strip()
            
            # URL kontrol√º - eƒüer URL tweet'te yoksa fallback kullan
            if repo_data['url'] not in tweet_text:
                terminal_log(f"‚ö†Ô∏è AI tweet'inde URL eksik, fallback kullanƒ±lƒ±yor: {repo_data['name']}", "warning")
                tweet_text = create_fallback_github_tweet(repo_data)
        
        # Tweet'i temizle ve formatla
        if len(tweet_text) > 280:
            # URL'yi koruyarak kƒ±salt
            url = repo_data['url']
            if url in tweet_text:
                # URL'den √∂nceki kƒ±smƒ± kƒ±salt
                url_index = tweet_text.find(url)
                before_url = tweet_text[:url_index]
                after_url = tweet_text[url_index:]
                
                if len(before_url) > 277 - len(after_url):
                    before_url = before_url[:274 - len(after_url)] + "..."
                
                tweet_text = before_url + after_url
            else:
                tweet_text = tweet_text[:277] + "..."
        
        terminal_log("‚úÖ GitHub tweet olu≈üturuldu", "success")
        
        return {
            "success": True,
            "tweet": tweet_text,
            "repo_data": repo_data,
            "repo_details": repo_details
        }
        
    except Exception as e:
        terminal_log(f"‚ùå GitHub tweet olu≈üturma hatasƒ±: {e}", "error")
        
        # Fallback tweet
        tweet_text = create_fallback_github_tweet(repo_data)
        
        return {
            "success": False,
            "tweet": tweet_text,
            "repo_data": repo_data,
            "error": str(e)
        }

def create_fallback_github_tweet(repo_data):
    """Simple fallback tweet for GitHub repo in English"""
    try:
        name = repo_data.get('name', 'Unknown')
        description = repo_data.get('description', '')
        language = repo_data.get('language', '')
        stars = repo_data.get('stars', 0)
        url = repo_data.get('url', '')
        
        # URL ve hashtag'ler i√ßin sabit alan hesapla
        hashtags = f"\n\n#GitHub #OpenSource #Programming"
        if language:
            hashtags += f" #{language}"
        hashtags += f"\n\n{url}"
        
        # Sabit kƒ±sƒ±mlar i√ßin alan hesapla
        fixed_parts = f"üöÄ {name}\n"
        
        # A√ßƒ±klama varsa ekle
        if description:
            fixed_parts += f"{description}\n"
        
        # Dil ve yƒ±ldƒ±z bilgisi
        tech_info = ""
        if language:
            tech_info += f"üíª {language} "
        if stars > 0:
            tech_info += f"‚≠ê {stars} stars"
        
        if tech_info:
            fixed_parts += f"{tech_info}\n"
        
        # A√ßƒ±klama i√ßin kalan alan hesapla ve gerekirse kƒ±rp
        total_length = len(fixed_parts) + len(hashtags)
        
        if total_length > 280:
            # A√ßƒ±klamayƒ± kƒ±salt
            excess = total_length - 280
            if description:
                new_desc_length = len(description) - excess - 3  # "..." i√ßin 3 karakter
                if new_desc_length > 10:
                    description = description[:new_desc_length] + "..."
                    fixed_parts = f"üöÄ {name}\n{description}\n"
                    if tech_info:
                        fixed_parts += f"{tech_info}\n"
                else:
                    # A√ßƒ±klamayƒ± tamamen √ßƒ±kar
                    fixed_parts = f"üöÄ {name}\n"
                    if tech_info:
                        fixed_parts += f"{tech_info}\n"
        
        # Final tweet'i olu≈ütur
        tweet = fixed_parts + hashtags
        
        return tweet
        
    except Exception as e:
        return f"üöÄ New GitHub project discovered!\n\n#GitHub #OpenSource #Programming\n\n{repo_data.get('url', '')}"

def fetch_github_articles_for_tweets():
    """GitHub repolarƒ± i√ßin tweet'leri hazƒ±rla ve pending_tweets.json'a kaydet"""
    try:
        terminal_log("üîç GitHub repolarƒ± tweet'leri hazƒ±rlanƒ±yor...", "info")
        
        # Trend repolarƒ± √ßek
        repos = fetch_trending_github_repos(language="python", time_period="daily", limit=5)
        
        if not repos:
            terminal_log("‚ö†Ô∏è GitHub repolarƒ± bulunamadƒ±", "warning")
            return []
        
        # Mevcut pending tweets'leri y√ºkle
        try:
            pending_tweets = load_json('pending_tweets.json')
        except:
            pending_tweets = []
        
        # Mevcut GitHub repo URL'lerini kontrol et (duplikasyon √∂nleme)
        existing_urls = [tweet.get('url', '') for tweet in pending_tweets if tweet.get('source_type') == 'github']
        
        # Mevcut payla≈üƒ±lan repolarƒ± da kontrol et
        try:
            posted_articles = load_json(HISTORY_FILE)
        except:
            posted_articles = []
        posted_urls = [article.get('url', '') for article in posted_articles]
        
        # Tweet'leri olu≈ütur
        new_tweets = []
        api_key = os.environ.get('GOOGLE_API_KEY')
        
        for repo in repos:
            try:
                # Duplikasyon kontrol√º
                if repo["url"] in existing_urls or repo["url"] in posted_urls:
                    terminal_log(f"‚ö†Ô∏è GitHub repo zaten mevcut: {repo['name']}", "warning")
                    continue
                
                tweet_result = generate_github_tweet(repo, api_key)
                
                if tweet_result.get("success"):
                    # Benzersiz ID olu≈ütur
                    tweet_id = len(pending_tweets) + len(new_tweets) + 1
                    
                    tweet_data = {
                        "id": tweet_id,
                        "title": f"GitHub: {repo['name']}",
                        "content": tweet_result["tweet"],
                        "url": repo["url"],
                        "source": "GitHub",
                        "source_type": "github",
                        "created_at": datetime.now().isoformat(),
                        "repo_data": repo,
                        "is_posted": False,
                        "language": repo.get("language", ""),
                        "stars": repo.get("stars", 0),
                        "forks": repo.get("forks", 0),
                        "owner": repo.get("owner", {}).get("login", ""),
                        "topics": repo.get("topics", [])[:5],
                        "hash": hashlib.md5(repo["url"].encode()).hexdigest()
                    }
                    new_tweets.append(tweet_data)
                    terminal_log(f"‚úÖ GitHub tweet hazƒ±rlandƒ±: {repo['name']}", "success")
                else:
                    terminal_log(f"‚ö†Ô∏è GitHub tweet olu≈üturulamadƒ±: {repo['name']}", "warning")
                    
            except Exception as e:
                terminal_log(f"‚ùå GitHub tweet hatasƒ± ({repo['name']}): {e}", "error")
                continue
        
        if new_tweets:
            # Yeni tweet'leri pending listesine ekle
            pending_tweets.extend(new_tweets)
            save_json('pending_tweets.json', pending_tweets)
            
            terminal_log(f"üìä {len(new_tweets)} GitHub tweet'i pending listesine eklendi", "success")
            
            # GitHub repo ge√ßmi≈üini kaydet
            for tweet in new_tweets:
                save_github_repo_history(tweet["repo_data"], {"success": True, "tweet": tweet["content"]})
        else:
            terminal_log("‚ö†Ô∏è Yeni GitHub tweet'i eklenmedi", "warning")
        
        return new_tweets
        
    except Exception as e:
        terminal_log(f"‚ùå GitHub tweet hazƒ±rlama hatasƒ±: {e}", "error")
        return []

def save_github_repo_history(repo_data, tweet_result):
    """GitHub repo payla≈üƒ±m ge√ßmi≈üini kaydet"""
    try:
        # Mevcut ge√ßmi≈üi y√ºkle
        try:
            posted_articles = load_json(HISTORY_FILE)
        except:
            posted_articles = []
        
        # Yeni kayƒ±t olu≈ütur
        new_record = {
            "title": f"{repo_data['name']} - GitHub Repository",
            "url": repo_data['url'],
            "content": repo_data.get('description', ''),
            "source": "GitHub",
            "published_date": repo_data.get('created_at', datetime.now().isoformat()),
            "posted_date": datetime.now().isoformat(),
            "hash": hashlib.md5(repo_data['url'].encode()).hexdigest(),
            "tweet_id": tweet_result.get('tweet_id'),
            "tweet_url": tweet_result.get('tweet_url', ''),
            "repo_data": repo_data,
            "type": "github_repo"
        }
        
        # Listeye ekle
        posted_articles.append(new_record)
        
        # Kaydet
        save_json(HISTORY_FILE, posted_articles)
        
        terminal_log(f"‚úÖ GitHub repo ge√ßmi≈üi kaydedildi: {repo_data['name']}", "success")
        
    except Exception as e:
        terminal_log(f"‚ùå GitHub repo ge√ßmi≈üi kaydetme hatasƒ±: {e}", "error")

def get_github_stats():
    """GitHub mod√ºl√º istatistiklerini getir"""
    try:
        try:
            posted_articles = load_json(HISTORY_FILE)
        except:
            posted_articles = []
        
        # GitHub repolarƒ±nƒ± filtrele
        github_repos = [article for article in posted_articles if article.get('type') == 'github_repo']
        
        # ƒ∞statistikleri hesapla
        total_repos = len(github_repos)
        
        # Dil daƒüƒ±lƒ±mƒ±
        languages = {}
        for repo in github_repos:
            repo_data = repo.get('repo_data', {})
            lang = repo_data.get('language', 'Unknown')
            languages[lang] = languages.get(lang, 0) + 1
        
        # Son 7 g√ºnde payla≈üƒ±lan
        week_ago = datetime.now() - timedelta(days=7)
        recent_repos = [
            repo for repo in github_repos 
            if datetime.fromisoformat(repo.get('posted_date', '2020-01-01')) > week_ago
        ]
        
        return {
            "total_repos": total_repos,
            "recent_repos": len(recent_repos),
            "languages": languages,
            "last_repo": github_repos[-1] if github_repos else None
        }
        
    except Exception as e:
        terminal_log(f"‚ùå GitHub istatistik hatasƒ±: {e}", "error")
        return {
            "total_repos": 0,
            "recent_repos": 0,
            "languages": {},
            "last_repo": None
        } 